\newcommand{\myTitle}{Enhanced Decision Maps for Exploring Classification Models \xspace} \usepackage{mdframed}			% Create frames \usepackage{soul} %%% for strikeout text {\textsc{abstract:} \slshape\footnotesize }% {} \usepackage{scrhack} % fix warnings when using KOMA with listings package \usepackage{xspace} % to get the spacing after macros right \usepackage{mparhack} % get marginpar right \renewcommand{\bflabel}[1]{{#1}\hfill} % fix the list of acronyms \usepackage{tabularx} % better tables \setlength{\extrarowheight}{3pt} % increase table row height \newcommand{\myfloatalign}{\centering} % to be used with each float for alignment keywordstyle=\color{RoyalBlue},%\bfseries, basicstyle=\small\ttfamily, commentstyle=\color{Green}\ttfamily, stringstyle=\rmfamily, numbers=none,%left,% numberstyle=\scriptsize,%\tiny stepnumber=5, numbersep=8pt, showstringspaces=false, breaklines=true, frameround=ftff, frame=single, belowcaptionskip=.75\baselineskip } {% \usepackage{backref} % to be loaded after hyperref package \renewcommand{\backreftwosep}{ and~} % separate 2 pages \renewcommand{\backreflastsep}{, and~} % separate last of longer list \renewcommand*{\backref}[1]{}  % disable standard }{\relax} colorlinks=true, linktocpage=true, pdfstartpage=3, pdfstartview=FitV,% breaklinks=true, pdfpagemode=UseNone, pageanchor=true, pdfpagemode=UseOutlines,% plainpages=false, bookmarksnumbered, bookmarksopen=true, bookmarksopenlevel=1,% hypertexnames=true, pdfhighlight=/O, urlcolor=black, linkcolor=black, citecolor=black, pdftitle={\myTitle},% pdfauthor={\textcopyright\ \myName},% pdfsubject={},% pdfkeywords={},% pdfcreator={pdfLaTeX},% pdfproducer={LaTeX with hyperref and classicthesis}% } {% }% }% }{\relax} {\topsep}                    % Space above {\topsep}                    % Space below {\itshape}                   % Body font {}                           % Indent amount {\scshape}                   % Theorem head font {.}                          % Punctuation after theorem head {.5em}                       % Space after theorem head {}  % Theorem head spec (can be left empty, meaning ‘normal’) {0}                          % Space above {0}                          % Space below

{} % Indent amount {\scshape} % Theorem head font {.} % Punctuation after theorem head {.5em} % Space after theorem head {} % Theorem head spec (can be left empty, meaning ‘normal’) {0} % Space above {0} % Space below {} % Body font {}                           % Indent amount {\scshape}                   % Theorem head font {.}                          % Punctuation after theorem head {.5em}                       % Space after theorem head {}  % Theorem head spec (can be left empty, meaning ‘normal’) linewidth=1pt, linecolor=gray, skipabove=\topsep, skipbelow=0, innertopmargin=0, innerbottommargin=0]{example} \newcommand{\citeyearp}[1]{(\citeyear{#1})} % Redefine \citet to call \cite \newcommand{\alex}[1]{{ \color{red} [#1]}} %% placeholder for Alex's comments \newcommand{\mib}[1]{{ \color{cyan} [MIB: #1]}} %% placeholder for mib's comments \newcommand{\dupml}[1]{{ \color{orange} [ML]: #1}} \newcommand{\dupform}[1]{{ \color{darkgreen} [Definition:] #1}} \newcommand{\duppinv}[1]{{ \color{blue} [Pinv:] #1}} \newcommand{\dupdm}[1]{{ \color{blue} [DBM:] #1}} \newcommand{\remove}[1]{{ \color{gray} [REMOVE:] #1}} pdftitle={\myTitle}, pdfauthor={\myName}, pdfsubject={}, pdfcreator={}, pdfkeywords={}, colorlinks=true,       		% false: boxed links; true: colored links linkcolor=blue,          	% color of internal links citecolor=blue,        		% color of links to bibliography filecolor=magenta,      	% color of file links urlcolor=blue, bookmarksdepth=4, plainpages = false, linktocpage } } Cover: A decision map visualized in both the data space and the 2D image space. \myTitle\\ \myName\\ PhD Thesis\\

citecolor=blue, % color of links to bibliography filecolor=magenta, % color of file links urlcolor=blue, bookmarksdepth=4, plainpages = false, linktocpage } } Cover: A decision map visualized in both the data space and the 2D image space. \myTitle\\ \myName\\ PhD Thesis\\ The research for this dissertation was conducted at the Visualization and Graphics (VIG) group, part of the Department of Information and Computing Sciences (ICS), Faculty of Science, Utrecht University, the Netherlands. \\ \abbr{DOI:} \href{https://doi.org/10.33540/3023}{https://doi.org/10.33540/3023} \Huge Enhanced Decision Maps for Exploring Classification Models\\[1.5em] \large \textbf{Verbeterde beslissingskaarten voor het verkennen van classificatiemodellen}\\[0.5em] \large(met een samenvatting in het Nederlands)\\[2.5em] \large ter verkrijging van de graad van doctor aan de\\ \large op gezag van de\\ \large rector magnificus, prof. dr. ir. W. Hazeleger,\\ \large ingevolge het besluit van het college voor promoties\\ \large in het openbaar te verdedigen op\\[1em] \large \textcolor{black}{woensdag 16 juli 2025 des ochtends te 10.15 uur}\\[2.5em] door\\[2.5em] geboren op 9 Mei 1997\\ te Hebei, China \Large Prof. dr. A. C. Telea\\[1em] \Large Dr. M. Behrisch\\[1em] \Large Prof. dr. D. Archambault\\ \Large Prof. dr. H. L. Hardman\\ \Large Prof. dr. A. P. J. M. Siebes\\ \Large Prof. dr. R. C. Veltkamp\\ \Large Prof. dr. P. Yolum Birbil\\[2.5em]

Prof. dr. A. C. Telea\\[1em] \Large Dr. M. Behrisch\\[1em] \Large Prof. dr. D. Archambault\\ \Large Prof. dr. H. L. Hardman\\ \Large Prof. dr. A. P. J. M. Siebes\\ \Large Prof. dr. R. C. Veltkamp\\ \Large Prof. dr. P. Yolum Birbil\\[2.5em] High-dimensional data is a key study object for both machine learning (ML) and information visualization. In the field of visualization, dimensionality reduction (DR) methods, also known as projections, are one of the most frequently used classes of techniques for visually exploring large and high-dimensional datasets. In ML, high-dimensional data is generated and processed by classifiers and regressors, which increasingly require visualization for explanation and exploration. This thesis focuses on a recent visualization technique called \emph{decision maps}. A decision map is a 2D image that visualizes the decision boundaries of a classifier in the data space, and can be used to explain and improve the behavior of ML classifiers. Constructing decision maps essentially involves a DR method and its inverse process (inverse projection). As such, ML techniques can help to create decision maps by providing improved (inverse) projections.

be used to explain and improve the behavior of ML classifiers. Constructing decision maps essentially involves a DR method and its inverse process (inverse projection). As such, ML techniques can help to create decision maps by providing improved (inverse) projections. We begin with a case study applying decision maps to explain the classification of mineral deposit genesis. Our findings show that decision maps provide extra insights into the mineral classification model, aiding geologists in interpreting the model. However, we also identified gaps in current decision map techniques that present opportunities for improvement. Following this case study, we conducted a comprehensive evaluation of three notable decision map techniques. Our evaluation shows that each technique has unique advantages and disadvantages. Our results can guide users in selecting the most suitable technique for specific tasks. A particularly salient finding of this evaluation was that all tested decision maps exhibit a surface-like behavior when applied to a 3D dataset.

advantages and disadvantages. Our results can guide users in selecting the most suitable technique for specific tasks. A particularly salient finding of this evaluation was that all tested decision maps exhibit a surface-like behavior when applied to a 3D dataset. We explored the aforementioned surface-like behavior of decision maps across more scenarios. By estimating the intrinsic dimensionality of the maps, we found that existing decision map methods cover only a small portion of the intrinsic dimensionality of high-dimensional data spaces. This finding highlights fundamental limitations in all current approaches to constructing decision maps. To address these limitations, we propose a novel approach for computing inverse projections with the support of ML. Our method allows users to interactively control the location of the inversely projected points, thus also of visualizations like decision maps, within the high-dimensional space. In this way, users can explore larger parts of the data space, bypassing the practical limitations imposed by the aforementioned surface-like behavior of decision maps. We demonstrate the effectiveness of our approach through its application to a style transfer task.

In this way, users can explore larger parts of the data space, bypassing the practical limitations imposed by the aforementioned surface-like behavior of decision maps. We demonstrate the effectiveness of our approach through its application to a style transfer task. Finally, we introduce an accelerated computation method for decision maps. Our method significantly reduces the computation time for both basic decision maps and enhanced variations thereof such as gradient maps. The acceleration facilitates the further deployment of such visualizations in interactive visual analytics workflows for classifier engineering. Hoog-dimensionale data is een belangrijk studieobject voor zowel machine learning (ML) als informatievisualisatie. In het vakgebied van visualisatie zijn dimensionaliteitsreductie (DR) methoden, ook wel projecties genoemd, een van de meest gebruikte klassen van technieken voor het visueel verkennen van grote en hoogdimensionale datasets. In ML wordt hoogdimensionale data gegenereerd en verwerkt door classifiers en regressoren, die steeds vaker visualisatie vereisen voor uitleg en verkenning. Dit proefschrift richt zich op een recente visualisatietechniek genaamd beslissingskaarten.

voor het visueel verkennen van grote en hoogdimensionale datasets. In ML wordt hoogdimensionale data gegenereerd en verwerkt door classifiers en regressoren, die steeds vaker visualisatie vereisen voor uitleg en verkenning. Dit proefschrift richt zich op een recente visualisatietechniek genaamd beslissingskaarten. Een beslissingskaart is een 2D-afbeelding die de beslissingsgrenzen van een classifier in de gegevensruimte visualiseert en kan worden gebruikt om het gedrag van ML-classifiers te verklaren en te verbeteren. Het construeren van beslissingskaarten omvat in essentie een DR-methode en het inverse proces daarvan (inverse projectie). ML-technieken kunnen bijdragen aan het maken van beslissingskaarten door verbeterde (inverse) projecties te leveren. We beginnen met een casestudy waarin beslissingskaarten worden toegepast om de classificatie van mineraalafzettingen te verklaren. Onze bevindingen tonen aan dat beslissingskaarten extra inzichten bieden in het mineralenclassificatiemodel, wat geologen helpt bij de interpretatie van het model. We identificeren ook tekortkomingen van de huidige technieken voor beslissingskaarten -- die mogelijkheden voor verbetering bieden.

mineraalafzettingen te verklaren. Onze bevindingen tonen aan dat beslissingskaarten extra inzichten bieden in het mineralenclassificatiemodel, wat geologen helpt bij de interpretatie van het model. We identificeren ook tekortkomingen van de huidige technieken voor beslissingskaarten -- die mogelijkheden voor verbetering bieden. Na deze casestudy voeren wij een uitgebreide evaluatie uit van drie prominente technieken voor beslissingskaarten. Onze evaluatie toont aan dat elke techniek unieke voor- en nadelen heeft. Onze resultaten kunnen gebruikers helpen bij het kiezen van de meest geschikte techniek voor specifieke taken. Een opvallende bevinding van deze evaluatie was dat alle geteste beslissingskaarten een oppervlakachtig gedrag vertonen wanneer ze worden toegepast op een 3D-dataset. We onderzoeken dit oppervlakachtige gedrag van beslissingskaarten verder in meerdere scenario's. Door de intrinsieke dimensionaliteit van de kaarten te schatten, ontdekten wij dat bestaande methoden slechts een klein deel van de intrinsieke dimensionaliteit van hoogdimensionale ruimtes dekken. Deze bevinding benadrukt fundamentele beperkingen in alle huidige benaderingen voor het construeren van beslissingskaarten.

Door de intrinsieke dimensionaliteit van de kaarten te schatten, ontdekten wij dat bestaande methoden slechts een klein deel van de intrinsieke dimensionaliteit van hoogdimensionale ruimtes dekken. Deze bevinding benadrukt fundamentele beperkingen in alle huidige benaderingen voor het construeren van beslissingskaarten. Om deze beperkingen aan te pakken, stellen we een nieuwe benadering voor om inverse projecties te berekenen met ondersteuning van ML. Onze methode stelt gebruikers in staat om interactief de locatie van de invers geprojecteerde punten te controleren, en daarmee ook visualisaties zoals beslissingskaarten, binnen de hoog-dimensionale ruimte. Op deze manier kunnen gebruikers grotere delen van de dataruimte verkennen zonder de praktische beperkingen die worden opgelegd door het eerder genoemde oppervlakgedrag van beslissingskaarten. We demonstreren de effectiviteit van onze aanpak door de toepassing ervan op een stijltransfertaak. Tot slot introduceren we een versnelde berekeningsmethode voor beslissingskaarten. Onze methode vermindert de rekentijd aanzienlijk voor zowel basisbeslissingskaarten als geavanceerde varianten zoals gradiëntkaarten. Deze versnelling vergemakkelijkt de verdere inzet van dergelijke visualisaties in interactieve visuele analyseworkflows voor classifier-engineering. This thesis is a result of the following publications: \item Y. Wang, A. Machado, and A. C. Telea. Quantitative and qualitative comparison of decision-map techniques for explaining classification models. \emph{Algorithms}, 2023.

van dergelijke visualisaties in interactieve visuele analyseworkflows voor classifier-engineering. This thesis is a result of the following publications: \item Y. Wang, A. Machado, and A. C. Telea. Quantitative and qualitative comparison of decision-map techniques for explaining classification models. \emph{Algorithms}, 2023. \item Y. Wang and A. C. Telea. Fundamental limitations of inverse projections and decision maps. In \emph{Proc. International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (IVAPP)}, 2024. \textbf{(Best Student Paper Award)} \item A. C. Telea, A. Machado, and Y. Wang. Seeing is learning in high dimensions: The synergy between dimensionality reduction and machine learning. \emph{SN Computer Science}, 2024. \item C. Grosu, Y. Wang, and A. C. Telea. Computing fast and accurate decision boundary maps. In \emph{Proc. EuroVis Workshop on Visual Analytics (EuroVA)}, 2024. \item Y. Wang, K. Qiu, A. C. Telea, Z. Hou, T. Zhou, Y. Cai, Z. Ding, H. Yu, and J. Deng. Interpreting mineral deposit genesis classification with decision maps: A case study using pyrite trace elements. \emph{American Mineralogist}, 2024. \item Y. Wang, C. Grosu, and A. C. Telea. Computing fast and accurate maps for explaining classification models. \emph{Computers \& Graphics}, 2025.

Interpreting mineral deposit genesis classification with decision maps: A case study using pyrite trace elements. \emph{American Mineralogist}, 2024. \item Y. Wang, C. Grosu, and A. C. Telea. Computing fast and accurate maps for explaining classification models. \emph{Computers \& Graphics}, 2025. \item Y. Wang and A. C. Telea. Investigating desirable properties of inverse projections and decision maps. \emph{Communications in Computer and Information Science}, 2025. {(In press)} \item Y. Wang, F. Dennig, M. Behrisch, and A. C. Telea. LCIP: Loss-controlled inverse projection of high-dimensional data. 2025. (Submitted to \emph{IEEE Transactions on Visualization and Computer Graphics}) The relation between the abovementioned papers and the chapters of this thesis is explained in a footnote at the beginning of every chapter which is based on these publications. During the development of this thesis, other contributions were also achieved: \item D. Blumberg, Y. Wang, A. C. Telea, D. A. Keim, and F. L. Dennig. Inverting multidimensional scaling projections using data point multilateration. In \emph{Proc. EuroVis Workshop on Visual Analytics (EuroVA)}, 2024. \item D. Blumberg, Y. Wang, A. C. Telea, D. A. Keim, and F. L. Dennig. MultiInv: Inverting multidimensional scaling projections and computing classifier maps by multilateration. \emph{Computers \& Graphics}, 2025.

multilateration. In \emph{Proc. EuroVis Workshop on Visual Analytics (EuroVA)}, 2024. \item D. Blumberg, Y. Wang, A. C. Telea, D. A. Keim, and F. L. Dennig. MultiInv: Inverting multidimensional scaling projections and computing classifier maps by multilateration. \emph{Computers \& Graphics}, 2025. In the last decades, Machine Learning (ML) has exponentially grown to be a key technology that assists a wide, and increasing, range of application domains in science, engineering, technology, and everyday life. Whether serving scientists who aim to design specific chemical molecules for effectively combatting widespread diseases -- like the by now infamous COVID\,\citep{kowalewski2020Predictingnovel}; engineers who work to optimize specific machinery configurations to reduce costs\,\citep{geng2020Physicsguideddeep}; geologists who aim to understand which factors have led to the appearance of specific mineral deposits\,\citep{petrelli2016Solvingpetrological,gregory2019DistinguishingOre}; biologists aiming to automatically classify parasites based on images thereof to increase public health efficiency\,\citep{suwannaphong2023parasitic}; or consumers asking for recommendations on buying specific products\,\citep{ren2020SequentialRecommendation}, ML technologies have become a fundamental part of all such solutions and applications.

specific mineral deposits\,\citep{petrelli2016Solvingpetrological,gregory2019DistinguishingOre}; biologists aiming to automatically classify parasites based on images thereof to increase public health efficiency\,\citep{suwannaphong2023parasitic}; or consumers asking for recommendations on buying specific products\,\citep{ren2020SequentialRecommendation}, ML technologies have become a fundamental part of all such solutions and applications. ML comes in many guises and variants. At a high level, and without loss of generality, we can distinguish so-called \emph{supervised} and \emph{unsupervised} ML approaches. In the unsupervised case, ML algorithms are used to `find structure' in a large amount of data samples (also called observations or data points) where each sample typically has tens up to thousands of so-called dimensions (also called variables or attributes). When successfully identified, structural elements such as clusters of samples (including their spread, density, and shape); distinctions between noisy samples and samples following a given pattern; the fit of samples to a given distribution; and the presence of outlier samples (which are far away from dense sample clusters) can further help practitioners in reasoning about the underlying phenomena that have created the data under study. In the supervised case, ML algorithms aim to construct a so-called \emph{model} that best fits a set of predefined so-called \emph{training set} of samples. The aim hereof is to next extrapolate the constructed model to \emph{predict}, or \emph{infer}, behavior on subsequent, so-called unseen, samples. Supervised learning next takes various forms such as \emph{classification} and \emph{regression}. In classification, the model aims to predict a so-called \emph{class label} based on the values of other available sample dimensions. In regression, the model aims to predict one or several, typically continuous, so-called dependent dimensions based on the available, so-called independent, data dimensions.

model aims to predict a so-called \emph{class label} based on the values of other available sample dimensions. In regression, the model aims to predict one or several, typically continuous, so-called dependent dimensions based on the available, so-called independent, data dimensions. \section{Challenges of ML engineering} Developments in the ML field in the last decade have shown results which were earlier hard to imagine. In particular, the advent of Deep Learning\,\citep{lecun2015Deeplearning}, which proposes models architected as dense neural networks that mimic (up to an extent) the functioning of the human brain, coupled with increasing computational power delivered by Graphics Processing Units (GPUs), have led to practical solutions for training highly complex (and large) models that can perform sophisticated tasks such as face detection and recognition; text and image analysis and synthesis; and steering automotive devices to navigate real-world complex landscapes over land, sea, and air. Names such as DALL-E, ChatGPT, or self-driving cars are well-known proofs of the above advances.

as face detection and recognition; text and image analysis and synthesis; and steering automotive devices to navigate real-world complex landscapes over land, sea, and air. Names such as DALL-E, ChatGPT, or self-driving cars are well-known proofs of the above advances. However, such spectacular developments have also generated additional \emph{challenges}. Apart from those concerning the availability of large amounts of high-quality (and, for classification purposes, labeled) data, and the increasing costs of running huge models that consume large amounts of electrical power, a predominant challenge concerns the \emph{black-box} nature of most modern ML models. Putting it simply, such models are trained by feeding them with data they have to fit -- the hope being that, given sufficient such data, the model will learn the `essence' of the phenomena that the data have been sampled from. After training, a similar hope exists -- namely that the trained models will infer plausible results which actually match the said phenomena. However, during both training and inference, ML engineers (mainly involved during the training phase) and actual model users (mainly involved in the inference phase) have very little \emph{understanding} of how such models operate.

results which actually match the said phenomena. However, during both training and inference, ML engineers (mainly involved during the training phase) and actual model users (mainly involved in the inference phase) have very little \emph{understanding} of how such models operate. The black-box nature of many ML models causes several important challenges to various types of stakeholders. For ML engineers, it is not clear what one can do to increase the performance of a given model or, in some cases, even how to architect and train a model when not sufficient (labeled) data is available\,\citep{benato2021SemiAutomaticData}. When models respond incorrectly -- that is, yield wrong predictions on their so-called \emph{testing sets} -- there is no simple, out-of-the-box, solution to fix the issue. Equally, if not even more importantly, users of trained ML models increasingly want to know \emph{why} a given model emits certain predictions. If this can be shown, users will gain \emph{trust} in the respective solution, which is a key element for its practical acceptance\,\citep{Elzen23}.

more importantly, users of trained ML models increasingly want to know \emph{why} a given model emits certain predictions. If this can be shown, users will gain \emph{trust} in the respective solution, which is a key element for its practical acceptance\,\citep{Elzen23}. Following the above, the last decade has witnessed the growth of a specific research field called Explainable Artificial Intelligence (XAI) or interpretable machine learning\,\citep{ribeiro16,gilpin2018Explainingexplanations,kaur2020InterpretingInterpretability,molnar2020InterpretableMachine,chatzi24}. As its name says, the main goal of XAI is to \emph{explain} the working of ML models throughout their entire lifecycle, that is, from their architecting, following their training and testing, and ending with their deployment in the field. Key questions that XAI aims to address are: \emph{Why} has a model yielded a given prediction; \emph{what} has a model learned from its training data; and \emph{how} has the model arrived at emitting a given prediction for a given input. When such questions can be (partially) answered, one can next fine-tune the design and training of the models in effective ways and also embed the models in complex real-life situations where non-specialist users operate side-by-side with the model to solve actual tasks. \section{Visualization, visual analytics, and XAI}

one can next fine-tune the design and training of the models in effective ways and also embed the models in complex real-life situations where non-specialist users operate side-by-side with the model to solve actual tasks. \section{Visualization, visual analytics, and XAI} Data visualization (VIS) is a separate discipline that has originally developed independent on ML. At its origin, visualization's key goals were to depict large amounts of data, mainly coming from numerical simulations and physical measurements, so that scientists and engineers could get a holistic, high-level, understanding of the measured or simulated phenomena. Following this, the respective (original) branch of visualization has been known under the name \emph{scientific visualization} (scivis)\,\citep{vis_handbook,telea_book_datavis}. Examples of domains targeted by scivis include flow (fluid) simulation, mechanical engineering, material science, geo- and Earth sciences, and medical science (in particular, radiology and surgery).

respective (original) branch of visualization has been known under the name \emph{scientific visualization} (scivis)\,\citep{vis_handbook,telea_book_datavis}. Examples of domains targeted by scivis include flow (fluid) simulation, mechanical engineering, material science, geo- and Earth sciences, and medical science (in particular, radiology and surgery). Following the increase of digital sources of information, and most notably the explosion of the internet, many additional data types and data sources entered the scope of visualization. Examples of such data types include networks (\emph{e.g.}, social, transportation), source code repositories, large text document and image archives, and databases. This expanded the scope of visualization to consider contexts where data would not come from a physical phenomenon embedded in (2D or 3D) Euclidean space and governed by (well known) physical laws. Such data was addressed by a new subfield of visualization called \emph{information visualization} (infovis)\,\citep{tufte01,munzner_book}.

to consider contexts where data would not come from a physical phenomenon embedded in (2D or 3D) Euclidean space and governed by (well known) physical laws. Such data was addressed by a new subfield of visualization called \emph{information visualization} (infovis)\,\citep{tufte01,munzner_book}. In parallel with the development of infovis, the goals addressed by visualization users would shift from the mere \emph{depiction} of data to the \emph{exploration} of data and, even further, to obtaining so-called `actionable insights' from it. In other words, the key added value associated with visualization would increasingly correlate with the ability of its users to solve complex, often ill-posed, problems pertaining to a multitude of phenomena that underlie the data at hand. To address this, a new field, called Visual Analytics (VA) emerged\,\citep{andrienko_va_book,cook05,keim08}. Since its inception, around the turn of the millennium, VA proposed a wealth of interaction, navigation, and visual depiction techniques which, when applied jointly and iteratively, enable the so-called `sensemaking loop', \emph{i.e.}, empower users to incrementally extract higher-level knowledge from raw data -- knowledge which is next directly usable for problem solving.

wealth of interaction, navigation, and visual depiction techniques which, when applied jointly and iteratively, enable the so-called `sensemaking loop', \emph{i.e.}, empower users to incrementally extract higher-level knowledge from raw data -- knowledge which is next directly usable for problem solving. Given the above, it is not surprising that VA and ML have been increasingly getting closer to each other. We summarize their interaction as two `flows' wherein one domain helps the other one, as follows: \textbf{ML4VIS:} In this context, ML algorithms, \emph{e.g.}, classifiers and regressors, are used to improve visualization algorithms and techniques. Examples hereof include the acceleration of algorithms due to the highly-parallelizable nature of ML, in particular DL, models\,\citep{espadoto2020Deeplearning}; simplified operation of visualization techniques by using trained ML models to execute their tasks\,\citep{espadoto2019NNinv}; and the design of novel visualization metaphors using the generative power of modern ML models\,\citep{data2vis}.

due to the highly-parallelizable nature of ML, in particular DL, models\,\citep{espadoto2020Deeplearning}; simplified operation of visualization techniques by using trained ML models to execute their tasks\,\citep{espadoto2019NNinv}; and the design of novel visualization metaphors using the generative power of modern ML models\,\citep{data2vis}. \textbf{VIS4ML:} In this context, the roles are reversed: Visualization (mainly infovis and VA) techniques and tools are used to support the entire workflow involved in ML pipelines\,\citep{sacha2019VIS4MLOntology,yuan2021surveyvisual}. This directly maps to the earlier-introduced XAI goals: Indeed, since visualization can effectively depict large and complex data (and underlying phenomena), it is a prime candidate for depicting how ML models operate during their training and/or inference. Moreover, the interactive exploration proposed by VA directly supports `what if' exploration that enable users to probe complex models to enrich their understanding of the model operation. As very suitably put, visualization is a very effective instrument for `opening the black box' of ML models\,\citep{tzeng05,alicioglu22,chatzi20,chatzi24}. %This is precisely the context in which the work in this thesis can be placed. This thesis contributes to both ML4VIS and VIS4ML, as we elaborate next. \section{Decision maps for classifier engineering}

for `opening the black box' of ML models\,\citep{tzeng05,alicioglu22,chatzi20,chatzi24}. %This is precisely the context in which the work in this thesis can be placed. This thesis contributes to both ML4VIS and VIS4ML, as we elaborate next. \section{Decision maps for classifier engineering} We further choose to focus our work on VIS4ML and ML4VIS on one of the two main applications of ML indicated in Sec.~\ref{sec:ch1:ML}, namely \emph{classification}. We motivate our choice as follows. Classification is a relatively simpler problem than general-purpose regression. Indeed, the output of a classifier is relatively simple: For a given input sample, a class label (defined over a usually small set of categorical values) is to be output, optionally accompanied by a continuous confidence value. In contrast, regressors output one or several continuous values for every given input. As such, designing VIS4ML solutions for general-purpose regressors is a significantly more complex task. Given this, and since we believe that VIS4ML has still unexplored potential for the classification context, we choose to focus our work on classification models. \caption{Decision map for a simple neural network model for a 2-class problem. Two-dimensional samples enter a single hidden layer (8 neurons) followed by a 2-neuron classification layer. The

the classification context, we choose to focus our work on classification models. \caption{Decision map for a simple neural network model for a 2-class problem. Two-dimensional samples enter a single hidden layer (8 neurons) followed by a 2-neuron classification layer. The sample space can be directly mapped to an image (since two-dimensional) yielding the decision map shown in the right image. Visualization constructed with Playground Tensorflow\,\citep{playground}.}

samples enter a single hidden layer (8 neurons) followed by a 2-neuron classification layer. The sample space can be directly mapped to an image (since two-dimensional) yielding the decision map shown in the right image. Visualization constructed with Playground Tensorflow\,\citep{playground}.} Further on, we focus on a specific visualization technique for classifier explanation, namely \emph{decision maps}, also called by some authors decision boundary maps (DBMs). In a nutshell, decision maps are 2D images that aim to capture the behavior of a trained ML model in a \emph{dense} way. To explain their operation, consider Fig.~\ref{fig:ch1:dbm_naive} (constructed using Playground Tensorflow\,\citep{playground}). A simple 3-layer neural network is created to classify two-dimensional samples in two classes. The network's input has two units since the data is two-dimensional. The output layer has two units, one responsible for each class. Since the data space is two-dimensional, we can map it one-to-one to a 2D image (see Fig.~\ref{fig:ch1:dbm_naive} right). If we color each image pixel by the label assigned to a sample having the respective $x$ and $y$ coordinates, we effectively obtain a decision map for our classifier. For this example, we see how, after training, the decision map consists of two disconnected blue decision zones and one, larger, orange decision zone. We also see how the training samples (white-outlined dots overlaid atop of the map) of the two classes perfectly fit the two decision zones.

the decision map consists of two disconnected blue decision zones and one, larger, orange decision zone. We also see how the training samples (white-outlined dots overlaid atop of the map) of the two classes perfectly fit the two decision zones. Visualizing decision boundaries or decision maps is a powerful tool for understanding how a classifier operates. For example, Fig.~\ref{fig:ch1:sklearn_fig}, from the well-known machine learning library scikit-learn\,\citep{pedregosa2011ScikitlearnMachine}, shows decision boundaries for four Support Vector Machine (SVM) classifiers with different kernels on the Iris dataset\,\citep{fisher1988IrisPlants}. Since our target visual space is two-dimensional, only two features of the dataset's four original features are used in this simple example. The decision maps of these classifiers help get an intuitive understanding of their respective expressive power. In this toy example, one can see how a linear classifier (top row images) can only separate the data with a straight line; in contrast, using more complex kernels for the SVM classifier can separate the data with a more complex curve (bottom row images). \caption{Plotting the decision boundaries for four SVM classifiers with different kernels on the Iris dataset. Only the first 2 features are used\,\citep{sklearn_svm_boudary_fig}.}

complex kernels for the SVM classifier can separate the data with a more complex curve (bottom row images). \caption{Plotting the decision boundaries for four SVM classifiers with different kernels on the Iris dataset. Only the first 2 features are used\,\citep{sklearn_svm_boudary_fig}.} However, classification models have to handle data whose dimensionality is \emph{far} larger than two. One way to construct decision maps for such cases is to first use \emph{projections} to reduce such data dimensionality to two. Projection algorithms, also known as dimensionality reduction (DR) methods, well known in infovis, aim to place samples whose dimensions are similar close to each other in the (2D) projection space\,\citep{nonato18,espadoto19}. Figure~\ref{fig:ch1:dbm_intro} (left) shows a projection of a 2310-sample, 19-dimensional, 7-class dataset, with labels mapped to categorical colors. We see clusters of same-color (same-label) points appearing in the projection since, indicating that many similar-class samples also have similar data values. Conversely, projection areas where such point clusters mix are a potential indication of classification difficulties, since some similar-valued samples have different classes. However, this visualization only shows what the trained model does for the \emph{fixed} set of 2310 samples the projection was given to depict. How the model \emph{generalizes} for other samples is left unexplained.

since some similar-valued samples have different classes. However, this visualization only shows what the trained model does for the \emph{fixed} set of 2310 samples the projection was given to depict. How the model \emph{generalizes} for other samples is left unexplained. \caption{How decision boundary maps (DBMs) work. The left image shows a projection of a 19-dimensional, 2310-sample, 7-class dataset. Points map data samples and their class labels. While one can see how similar points (close in the projection) tend to have similar labels, this image does not tell us what happens in the white areas between projected points. The right image shows a DBM for the same dataset and model which fills such gaps. All image points are now colored to show the label assigned by the model to a sample which would project at that location. Compact same-colored areas show the model's \emph{decision zones}; pixels on the boundaries of these areas show the model's \emph{decision boundaries}. Images taken from\,\citep{rodrigues2018Imagebasedvisualization}.}

to show the label assigned by the model to a sample which would project at that location. Compact same-colored areas show the model's \emph{decision zones}; pixels on the boundaries of these areas show the model's \emph{decision boundaries}. Images taken from\,\citep{rodrigues2018Imagebasedvisualization}.} Decision maps aim to precisely fill the above-mentioned gap which is left unexplained by projection visualizations. More specifically, they aim to literally show how a classifier operates in the empty areas in a projection -- that is, for samples which would project in the white space between projection points. To illustrate this, consider the dataset shown in Fig.~\ref{fig:ch1:dbm_intro} (left) on which we train a simple logistic regression model to infer the classes. Figure~\ref{fig:ch1:dbm_intro} (right image) shows a decision map for this classifier, constructed by an early technique\,\citep{rodrigues2018Imagebasedvisualization}. All image pixels are now assigned a class label, that of a data sample which would project at that location. Finding such samples is done by using a so-called \emph{inverse projection} technique which aims to reverse the effects of a user-chosen projection technique. Such visualizations extend the insights provided by simple projections in several directions, as follows:

at that location. Finding such samples is done by using a so-called \emph{inverse projection} technique which aims to reverse the effects of a user-chosen projection technique. Such visualizations extend the insights provided by simple projections in several directions, as follows: \item They show a \emph{dense} image where every pixel conveys information on the explored classification model. The amount of depicted information does not depend on the size of the training (or test) set fed to the projection but only on the resolution of this image. Thus, increasing this resolution shows increasingly more insights into the classifier's behavior. Conversely, one can use (in theory at least) as many or as few samples as one wants to construct the projection. \item Decision maps show how the classifier behaves \emph{between} the training (or test) set samples. The available 2D space is effectively partitioned into compact single-color areas which indicate the classifier's \emph{decision zones}. Borders of these areas, where two or more different colors meet, indicate the classifier's \emph{decision boundaries}.

\emph{between} the training (or test) set samples. The available 2D space is effectively partitioned into compact single-color areas which indicate the classifier's \emph{decision zones}. Borders of these areas, where two or more different colors meet, indicate the classifier's \emph{decision boundaries}. \item Decision zones and boundaries help get insights into a model's \emph{behavior}. Analyzing how decision boundaries `go through' a set of training samples effectively tells how a model \emph{generalizes} to unseen samples. The jaggedness (or, conversely, smoothness) of decision boundaries gives insights into how well the model can fit its given training data. Additionally, if one marks misclassified samples on a decision map, one can see how close these are to decision boundaries and eventually perform training fine-tuning to alleviate such problems.

into how well the model can fit its given training data. Additionally, if one marks misclassified samples on a decision map, one can see how close these are to decision boundaries and eventually perform training fine-tuning to alleviate such problems. Given all above, we argue -- in line with previous authors -- that decision maps are useful tools for classifier engineering tasks such as assessing the actual fit of a model with what one expects from its known theoretical behavior\,\citep{DBM2019rodrigues}; getting insight on where, in data space, the classifier performs well or not\,\citep{rodrigues2020VisualAnalytics,schulz2015Usingdiscriminative}; finding types of data samples for which the classifier is brittle\,\citep{differentiableDBM}; and guiding semi-supervised annotation to enrich labeled datasets for classifier training\,\citep{benato2024HumanloopUsing}. In the same time, just as for all other XAI techniques (based on visual analytics or not), decision maps are only one of the tools in a wider arsenal offered to AI specialists to complete their various tasks related to model design, improvement, deployment, and usage. In particular, decision maps do not directly handle regressors but only classifiers; they show where, in data space, the decision boundaries are drawn, but do not offer direct mechanisms to `tune' these boundaries, apart from suggesting the addition of extra (labeled) samples to certain zones; and they cannot directly link specific sample properties to the formation of decision boundaries in a model. Nevertheless, our conclusion is that, within these more general limitations, decision maps are a useful tool in the abovementioned arsenal and, as such, deserve being further refined to better accomplish the goals they have been originally introduced for.

a model. Nevertheless, our conclusion is that, within these more general limitations, decision maps are a useful tool in the abovementioned arsenal and, as such, deserve being further refined to better accomplish the goals they have been originally introduced for. In the last decade, several decision map techniques have been proposed\,\citep{schulz2015Usingdiscriminative, rodrigues2018Imagebasedvisualization, espadoto2019NNinv, schulz2020DeepViewVisualizing, rodrigues2020VisualAnalytics, DBM2019rodrigues, oliveiraSDBM2022, oliveira23}. Globally put, these techniques can handle generically any classification method, data of any dimensionality, and are relatively simple to set up and use in practice. Deep-learning-based variants of such methods additionally bring in extra computational performance\,\citep{oliveiraSDBM2022} and are also robust to small-scale noise present in the input data\,\citep{oliveira2023StabilityAnalysis}. Recent extensions allow enriching a plain DBM image with various additional information depicting \emph{e.g.} the model's confidence, support of its decisions, and non-linear deformations performed during the data-to-image mapping\,\citep{differentiableDBM}. However, many aspects of decision maps remain unsolved to date and lead to corresponding research questions:

plain DBM image with various additional information depicting \emph{e.g.} the model's confidence, support of its decisions, and non-linear deformations performed during the data-to-image mapping\,\citep{differentiableDBM}. However, many aspects of decision maps remain unsolved to date and lead to corresponding research questions: \textbf{Quality (RQ1):} While several decision map construction methods have been proposed, as outlined above, there have been formally no comparisons of their quality apart from the side-by-side display of decision maps produced by them for a few hand-picked classification models and datasets. This is not surprising: To perform such comparisons, we would need in the first place to have formal quality \emph{metrics} that quantify desirable aspects of such maps. Two questions follow here: How can we define such quality metrics; and how do current decision map methods fare with respect to each other from the perspective of such metrics?

formal quality \emph{metrics} that quantify desirable aspects of such maps. Two questions follow here: How can we define such quality metrics; and how do current decision map methods fare with respect to each other from the perspective of such metrics? \textbf{Coverage (RQ2):} A decision map aims to depict the behavior of a trained model by mapping its high-dimensional decision zones to a partition of a 2D image into corresponding label-colored zones. Formally speaking, for such a map to capture the \emph{full} behavior of the model, it should be able to map \emph{all} the high-dimensional points that the model can work on to 2D points. However, it is clear that densely sampling a high-dimensional space creates a sample count having an exponential size with the spatial dimension, so not all such samples can be realistically mapped to 2D. Hence, existing decision map methods do internally make some (non-transparent) choices as to which areas of the data space they sample to depict. Key questions related to this coverage are: How do decision map methods differ in their sampling of the data space? Does this sampling depend on the data dimensionality or depicted classification model?

of the data space they sample to depict. Key questions related to this coverage are: How do decision map methods differ in their sampling of the data space? Does this sampling depend on the data dimensionality or depicted classification model? \textbf{Control (RQ3):} If, as stated above, current decision map methods do not densely cover their entire data space when creating decision map images, and arguably complete coverage is impossible in practice due to the aforementioned dimensionality problem, it follows that users would benefit from being able to specify how a decision map method samples this space. Such functionality would allow one to \emph{e.g.} explore, in turn, different parts of the space, to compose a final picture (or insight) on how the model actually works -- much like radiologists compose their understanding of a 3D CT or MRI volume by examining individual 2D slices thereof. The ensuing question is: How can we enable users to control the parts of the data space depicted by a decision map in simple, interactive, ways?

understanding of a 3D CT or MRI volume by examining individual 2D slices thereof. The ensuing question is: How can we enable users to control the parts of the data space depicted by a decision map in simple, interactive, ways? \textbf{Interactivity (RQ4):} One of the key added-value claims of decision maps resided on their ability to examine the behavior of a classifier and, upon seeing problems such as misclassifications, tune its hyperparameters and/or augment its training data to obtain an improved model\,\citep{rodrigues2020VisualAnalytics}. For this to effectively work in a VA setting, the loop consisting of model training, decision map construction, and user-driven parameter changing should execute as fast as possible -- ideally, at near-interactive rates. If this were possible, users could literally `shape' the model's decision zones by interacting with data depicted in a decision map visualization. However, none of the current decision map methods offers this performance -- the fastest such methods we are aware of still require several seconds to generate a decision map image of roughly $500^2$ pixels\,\citep{oliveiraSDBM2022}. Can we significantly improve this computation time for creating decision maps for any classification model and for any type and dimensionality of datasets?

are aware of still require several seconds to generate a decision map image of roughly $500^2$ pixels\,\citep{oliveiraSDBM2022}. Can we significantly improve this computation time for creating decision maps for any classification model and for any type and dimensionality of datasets? Summarizing the above, we can next state our key research question: \emph{How can we improve decision map methods to understand and control which parts of the data space they sample, leading to desirable quality values, with high computational performance?} This thesis aims to answer the above-stated central research question by covering all sub-questions \textbf{RQ1} -- \textbf{RQ4} that are subsumed therein. We next outline how we approach this goal and also thereby detail the structure of the thesis. Chapter~\ref{ch2:related} presents related work in machine learning (ML) and visual analytics (VA) and, more specifically, the way in which ML can help VA and conversely. We introduce here basic notations used throughout the thesis and also explain the key methods and techniques that form the basis of ML4VIS and VIS4ML. While VIS4ML is the area that the use of decision maps fall into, ML4VIS provides several important techniques for building decision maps.

the thesis and also explain the key methods and techniques that form the basis of ML4VIS and VIS4ML. While VIS4ML is the area that the use of decision maps fall into, ML4VIS provides several important techniques for building decision maps. We focus here chiefly on methods and quality metrics related to high-dimensional data and, more specifically, projection (dimensionality reduction) and inverse projection, since these are the key mechanisms underlying the computation of decision maps. Apart from presenting related work, this chapter outlines an important claim of this thesis, namely that ML4VIS and VIS4ML can be, and should be, approached \emph{jointly} for most effective results in both fields. At a higher level, this chapter details the context in which we are going to improve decision maps to further answer \textbf{RQ1} -- \textbf{RQ4}.

and VIS4ML can be, and should be, approached \emph{jointly} for most effective results in both fields. At a higher level, this chapter details the context in which we are going to improve decision maps to further answer \textbf{RQ1} -- \textbf{RQ4}. Chapter~\ref{ch3:geo_application} presents an application of decision maps, as we encountered them at the onset of our work, for understanding classification models constructed for a geoscience application, namely the understanding of the genesis of mineral deposits. The main added value of this application is to expose both the advantages, but also the limitations, of current decision map methods in a \emph{practical} setting, when used by scientists who are not ML specialists. Our work outlines that decision maps can indeed help to answer concrete questions in our chosen application domain -- an insight which, to our knowledge, was not presented before in the DBM literature. This insight justifies the added value of decision maps and, thus, implicitly, our goal of further analyzing \textbf{RQ1} -- \textbf{RQ4}. In the same time, our work exposed several limitations of existing DBM methods, thus provides starting points to further answer \textbf{RQ1} -- \textbf{RQ4} in the following chapters.

decision maps and, thus, implicitly, our goal of further analyzing \textbf{RQ1} -- \textbf{RQ4}. In the same time, our work exposed several limitations of existing DBM methods, thus provides starting points to further answer \textbf{RQ1} -- \textbf{RQ4} in the following chapters. Chapter~\ref{ch4:metrics_decision_map} addresses \textbf{RQ1} by proposing a set of novel metrics for the comparison of decision maps -- more precisely, the inverse projection techniques which are fundamental to any decision map construction. We use these metrics to evaluate all the decision map techniques (and inverse projections) we are aware of on a variety of datasets and classification models. Our findings show to-date unknown differences between the evaluated DBM techniques and, also, propose a practical workflow for choosing suitable techniques in practice based on the aspects quantified by our metrics. To our knowledge, this is the first -- and, to writing date, only -- study that quantitatively compares decision maps and inverse projections. This chapter also outlines a surprising observation we made during our study, namely that decision maps seem to cover only a small, manifold-like, surface embedded in the high-dimensional data space they aim to depict.

compares decision maps and inverse projections. This chapter also outlines a surprising observation we made during our study, namely that decision maps seem to cover only a small, manifold-like, surface embedded in the high-dimensional data space they aim to depict. Chapter~\ref{ch5:surface_like_behavior} further explores the above-mentioned observation on the coverage of decision maps. We study this issue of coverage further using a larger set of decision maps (and inverse projections), datasets, and classification models. We also propose both visual (qualitative) and computational (quantitative) ways to assess the dimensionality of this coverage -- thereby addressing \textbf{RQ2}. Our findings confirm our earlier observation, namely that all decision map methods cover only a manifold of intrinsic dimensionality two embedded in the data space, regardless of any other parameters. Our findings -- to our knowledge, observed for the first time by our work -- significantly affect the way in which decision maps can be next used in practice.

two embedded in the data space, regardless of any other parameters. Our findings -- to our knowledge, observed for the first time by our work -- significantly affect the way in which decision maps can be next used in practice. Chapter~\ref{ch6:controllable_Pinv} builds on the findings explored in Chapter~\ref{ch5:surface_like_behavior} by proposing an interactive mechanism that allows users to control which parts of the data space an inverse projection -- and thus a decision map -- cover. We propose a simple, generic, and computationally efficient mechanism that allows users to `pull' the aforementioned manifold surface closely towards selected data samples, thereby addressing \textbf{RQ3}. We illustrate the added value of this mechanism by presenting an application in style transfer in image datasets. To our knowledge, our work is the first technique that allows users to explicitly control inverse projections. In contrast to the previous chapters, which focus on VIS4ML application, chapter falls under the topic ML4VIS, as it uses deep learning based methods to construct the inverse projections.

the first technique that allows users to explicitly control inverse projections. In contrast to the previous chapters, which focus on VIS4ML application, chapter falls under the topic ML4VIS, as it uses deep learning based methods to construct the inverse projections. Chapter~\ref{ch7:fastDBM} attacks our last research question, \textbf{RQ4}, namely the accelerated computation of decision maps. We propose FastDBM, a simple heuristic that reduces the number of times that an inverse projection needs to be called to compute a decision map by exploiting smoothness and continuity properties which should be present in the underlying mappings. Our method can be generically applied to any inverse projection and classification method, thus, any decision map method. Our results show that we can compute decision maps at about one order of magnitude faster than existing methods and with minimal quality loss. We also generalize this algorithm to accelerate the computation of other classifier maps such as gradient maps and distance-to-boundary maps (see Sec.~\ref{sec:dbm_enhancements}). This enables one, for the first time, to practically compute maps that explain classifiers at high resolutions. Our work next opens the possibility for using decision maps in near-interactive visual analytics loops where users iteratively refine a classification model based on insights drawn from the visualized decision maps.

practically compute maps that explain classifiers at high resolutions. Our work next opens the possibility for using decision maps in near-interactive visual analytics loops where users iteratively refine a classification model based on insights drawn from the visualized decision maps. Finally, Chapter~\ref{ch8:conclusions} concludes this thesis by revisiting our research questions, summarizing our results, and outlining potential directions of future work for the computation and application of decision map techniques. This chapter introduces basic notations and concepts used throughout the thesis and explain how both dimensionality reduction (DR) and machine learning (ML) can help each other in achieving their respective aims, \emph{i.e.}, ML4VIS and VIS4ML. By presenting an overview of the state-of-the-art in both fields, we aim to provide readers with a better understanding of the existing synergies between ML and DR. More importantly, we highlight the role of decision maps in this context, \emph{i.e.}, their importance in VIS4ML, and how they can benefit from ML4VIS\footnote{This chapter is based on the paper ``Seeing is Learning in High Dimensions: The Synergy Between Dimensionality Reduction and Machine Learning''\,\citep{telea2024SeeingLearning}.}. ML has become one of the indispensable instruments in data-driven science and virtually any data-intensive application domain in our society.

chapter is based on the paper ``Seeing is Learning in High Dimensions: The Synergy Between Dimensionality Reduction and Machine Learning''\,\citep{telea2024SeeingLearning}.}. ML has become one of the indispensable instruments in data-driven science and virtually any data-intensive application domain in our society. At a high level, and without loss of generality, ML models can be described as engines which process \emph{high-dimensional data} -- that is, collections of observations (samples) consisting of tens up to millions of individual measurements (dimensions) of a given phenomenon. Such data occurs throughout the ML pipeline -- it is present in the input of the models (\emph{e.g.}, images consisting of millions of pixels); in the internal working of such models (\emph{e.g.}, the so-called activations of neural units in the many intermediate layers of a DL model), and also in the models' output (\emph{e.g.}, the image created by generative AI techniques from given inputs). As such, it is not surprising that understanding high-dimensional data, and how it is transformed by ML models, is a key goal and challenge in ML.

the models' output (\emph{e.g.}, the image created by generative AI techniques from given inputs). As such, it is not surprising that understanding high-dimensional data, and how it is transformed by ML models, is a key goal and challenge in ML. In a separate field, exploring and understanding high-dimensional data is one of the top goals of information visualization (infovis)\,\citep{munzner_book,telea_book_datavis,maljovec15}. During the last decades, many techniques have been proposed to this end, including scatterplots and scatterplot matrices\,\citep{yates14,lehmann2012selecting}, parallel coordinate plots\,\citep{inselberg90}, table lenses\,\citep{rao94,tablelens}, and glyphs\,\citep{borgo13}. However, most such techniques are fundamentally limited in the size of the datasets they can depict: They can show either datasets having many samples with few dimensions, few samples having many dimensions, but not both.

plots\,\citep{inselberg90}, table lenses\,\citep{rao94,tablelens}, and glyphs\,\citep{borgo13}. However, most such techniques are fundamentally limited in the size of the datasets they can depict: They can show either datasets having many samples with few dimensions, few samples having many dimensions, but not both. Dimensionality reduction (DR) techniques, also called multidimensional projections (MPs), are a family of visualization techniques that aim to solve the aforementioned visualization scalability issue in both the sample and dimension count. Simply put, given a high-dimensional dataset consisting of several samples, DR techniques create a low-dimensional (typically 2D or 3D) scatterplot in which close points correspond to similar samples in the input dataset. This allows users of DR visualizations to identify salient \emph{patterns} in the dataset in the form of clusters of closely-packed scatterplot points, clusters of points with different shapes or densities, or outlier points\,\citep{lespinats11,sorzano2014surveydimensionality,nonato18}.

to similar samples in the input dataset. This allows users of DR visualizations to identify salient \emph{patterns} in the dataset in the form of clusters of closely-packed scatterplot points, clusters of points with different shapes or densities, or outlier points\,\citep{lespinats11,sorzano2014surveydimensionality,nonato18}. Tens, if not hundreds, of DR techniques have been designed to cater for the various functional and non-functional requirements inherent to the DR process, such as computational scalability, ability of treating data of various types and dimensionality, handling time-dependent data or data with missing values, ability of depicting new samples along existing ones (out-of-sample property), stability in the presence of noise, ability of capturing specific aspects present in the input dataset, and ease of use\,\citep{cunningham15_survey,espadoto19,nonato18}.

and dimensionality, handling time-dependent data or data with missing values, ability of depicting new samples along existing ones (out-of-sample property), stability in the presence of noise, ability of capturing specific aspects present in the input dataset, and ease of use\,\citep{cunningham15_survey,espadoto19,nonato18}. At a first glance, ML and DR are separate fields with different goals. ML is chiefly concerned with \emph{learning} a model to predict the behavior of some phenomenon from existing samples thereof. In a more general setting, this has been extended to additional tasks such as data representation (autoencoders) or generative AI. For the purpose of our discussion next, we will mainly focus on prediction tasks, either in a classification or regression setting. Separately, DR aims at depicting, or \emph{seeing}, the samples of such a phenomenon. We argue that these two goals -- learning and seeing -- are, however, strongly related, and advances in one field directly support requirements of the other field in both directions. Simply put, we argue that \emph{seeing is learning}, in both directions of the implication, as outlined below:

learning and seeing -- are, however, strongly related, and advances in one field directly support requirements of the other field in both directions. Simply put, we argue that \emph{seeing is learning}, in both directions of the implication, as outlined below: \item{\textbf{Learning needs seeing (VIS4ML):}} The ML field generates complex models, whose `black-box' behavior is increasingly hard-to-understand by both their developers and users. Understanding such models is increasingly important for fine-tuning their behavior but also gaining trust in their deployment. Such understanding can be massively aided by seeing (visualizing) their structure and operation. Since ML models revolve around high-dimensional data, and DR techniques are ideally suited for depicting such data, DR techniques are a candidate of choice for visualizing them; \item{\textbf{Seeing needs learning (ML4VIS):}} Existing DR techniques are increasingly challenged by the already-mentioned sum of requirements they have to cope with. Few, if any, of such existing techniques can cope with all these requirements. In contrast, many ML techniques are designed upfront to handle such requirements, especially computational scalability, accuracy, stability, and out-of-sample ability. Given these, it makes sense to use ML techniques to learn the high-to-low dimensional mapping and thereby assist the DR task.

contrast, many ML techniques are designed upfront to handle such requirements, especially computational scalability, accuracy, stability, and out-of-sample ability. Given these, it makes sense to use ML techniques to learn the high-to-low dimensional mapping and thereby assist the DR task. In this chapter, we explore the commonalities of ML and DR techniques and bring evidence of existing, emerging, and potentially new interactions between these two fields. We proceed by introducing our two fields of interest -- ML and DR -- with an emphasis on their commonalities (Sec.~\ref{sec:background_ch2}). We next explore in Sec.~\ref{sec:ml_survey} how learning (ML) is supported by seeing (DR), especially in the creation of visual analytics (VA) solutions for explainable artificial intelligence (XAI), in particular decision maps. Subsequently, we study in Sec.~\ref{sec:dr_survey} the converse connection, that is, how seeing (DR) is supported by leaning (ML). We further outline in Sec.~\ref{sec:forward} new, emerging, connections between the two fields that point to promising future research directions in which the DR and ML fields can benefit from each other. Finally, Section~\ref{sec:conclusion_ch2} concludes the chapter.

supported by leaning (ML). We further outline in Sec.~\ref{sec:forward} new, emerging, connections between the two fields that point to promising future research directions in which the DR and ML fields can benefit from each other. Finally, Section~\ref{sec:conclusion_ch2} concludes the chapter. In this section, we provide a general introduction to ML and DR concepts, notations, and principles, with a focus on highlighting the commonalities between the two fields, which will be further explored in the remainder of the chapter. \textbf{Notations:} Let $D = \{ \mathbf{x}_i \}$ be a dataset of $n$-dimensional samples, also called observations or data points $\mathbf{x}_i$, $1 \leq i \leq N$. A sample $\mathbf{x}_i = (x_i^1, \ldots, x_i^n)$ is a tuple of $n$ components $x_i^j$, also called features, variables, attributes, or dimension values. For exposition simplicity, we next consider that  $x_i^j \in \mathbb{R}$; other data domains are treated similarly for the purpose of our discussion.

(x_i^1, \ldots, x_i^n)$ is a tuple of $n$ components $x_i^j$, also called features, variables, attributes, or dimension values. For exposition simplicity, we next consider that $x_i^j \in \mathbb{R}$; other data domains are treated similarly for the purpose of our discussion. We denote by $Z \subset \mathbb{R}^n$ the spatial subset where samples of a given phenomenon are found. For instance, considering image data, only positive values (possibly bound by a maximum) can denote pixel intensities. Following this notation, $D$ can be depicted as a table with $N$ rows (one per sample) and $n$ columns (one per dimension). Typically, these dimensions are considered to be \emph{independent} variables, \emph{i.e.}, whose values are measured from the behavior of a given phenomenon over $Z$. Atop of these, $D$ can have one or more dimensions (columns) of so-called \emph{dependent} variables, also called labels or annotations. We next consider a single such dependent variable $y_i \in C$, where $C$ is the domain of definition of the labels, unless specified otherwise. We denote the annotated dataset $D$ by $D_a = [D | \mathbf{y}]$. Given a so-called test set $D_T \subset D_a$, machine learning (ML) techniques aim to create a function f : Z \rightarrow C

of the labels, unless specified otherwise. We denote the annotated dataset $D$ by $D_a = [D | \mathbf{y}]$. Given a so-called test set $D_T \subset D_a$, machine learning (ML) techniques aim to create a function f : Z \rightarrow C which predicts data values for most (ideally, all) samples in $Z$. Models $f$ are built by using a so-called training set $D_t \subset D_a$, $D_t \cap D_T = \emptyset$ so as to maximize the number of test set points $\mathbf{x}_i \in D_T$ for which $f(\mathbf{x}_i) = y_i$. ML models can be further split into \emph{classifiers}, for which $C$ is typically a categorical, or label, dataset; and \emph{regressors}, for which $C$ is typically a subset of $\mathbb{R}$. For regressors, $f$ typically strives that $f( \mathbf{x}_i )$ is as close as possible to $y_i$, whereas for classifiers exact equality is aimed at.

a categorical, or label, dataset; and \emph{regressors}, for which $C$ is typically a subset of $\mathbb{R}$. For regressors, $f$ typically strives that $f( \mathbf{x}_i )$ is as close as possible to $y_i$, whereas for classifiers exact equality is aimed at. Many methods exist to measure the performance of ML models. The most widespread such methods measure several so-called quality metrics on the training set (training performance) and, separately, on the unseen test set (testing performance). Common metrics include accuracy, precision, recall, F-score, and Cohen's kappa score. More advanced methods take into account hyperparameters that allow optimizing between precision and recall, \emph{e.g.} the Receiver Operator Characteristic (ROC) curve and area underneath\,\citep{botchkarev19,jiang20,mlbench22}. A dimensionality reduction technique, or multidimensional projection $P$, is a function that maps every $\mathbf{x}_i \in D$ to a point $P(\mathbf{x}_i) \in \mathbb{R}^q$. For convenience, we next denote by P(D) = \{P(\mathbf{x}_i) | \mathbf{x}_i \in D \} the projection of an entire dataset $D$. For visualization purposes, $q \in \{2,3\}$, \emph{i.e.}, $P(D)$ is a 2D, respectively 3D, scatterplot. Since 2D projections are by far the most commonly used in VIS4ML, we will focus on them for the remainder of this thesis.

entire dataset $D$. For visualization purposes, $q \in \{2,3\}$, \emph{i.e.}, $P(D)$ is a 2D, respectively 3D, scatterplot. Since 2D projections are by far the most commonly used in VIS4ML, we will focus on them for the remainder of this thesis. At a high level, all projection techniques $P$ aim to preserve the so-called \emph{structure} of the dataset $D$, so that users can infer this structure by visualizing $P(D)$, following a well-known inverse mapping principle in data visualization\,\citep{telea_book_datavis}. Forms of such structure include, but are not limited to, clusters of similar samples; clusters having different sample densities; similarities between different samples; and outlier samples. Structure-preserving projections map (some) of these data properties to the corresponding properties of their generated scatterplots. Usually, projections do not use data annotations (even when these are available), but only independent variables -- more on this aspect to be discussed further in Sec.~\ref{sec:nnp}. Since data structure preservation entails several aspects, as outlined above, different so-called \emph{quality metrics} have been devised to capture the abilities of a given $P$. A quality metric is a function M(D, P(D)) \in \mathbb{R}^{+}

be discussed further in Sec.~\ref{sec:nnp}. Since data structure preservation entails several aspects, as outlined above, different so-called \emph{quality metrics} have been devised to capture the abilities of a given $P$. A quality metric is a function M(D, P(D)) \in \mathbb{R}^{+} that tells how well the scatterplot $P(D)$, or a part thereof, captures a given aspect present in the dataset $D$. At a high level, such metrics can be grouped into (1) measuring \emph{distance} preservation between pairs of samples, respectively pairs of projection points, in $\mathbb{R}^n$ and $\mathbb{R}^q$ respectively, such as normalized stress and the Shepard diagram correlation\,\citep{lamp}; and (2) measuring if neighborhoods (groups of close points) in $D$ are mapped to neighborhoods in $P(D)$, such as trustworthiness and continuity\,\citep{venna2006visualizing}, false and missing neighbors\,\citep{martins14}, and the Kullback-Leibler divergence\,\citep{tSNEMaaten2008}. The latter class is extended for projections of labeled data $P(D_a)$ by metrics such as neighborhood hit and class consistency\,\citep{paulovich2008least,sips09}. Detailed surveys of projection quality metrics are given in\,\citep{aupetit07,lespinats11,nonato18,espadoto19}.

trustworthiness and continuity\,\citep{venna2006visualizing}, false and missing neighbors\,\citep{martins14}, and the Kullback-Leibler divergence\,\citep{tSNEMaaten2008}. The latter class is extended for projections of labeled data $P(D_a)$ by metrics such as neighborhood hit and class consistency\,\citep{paulovich2008least,sips09}. Detailed surveys of projection quality metrics are given in\,\citep{aupetit07,lespinats11,nonato18,espadoto19}. Projection quality is implicitly linked to the measuring of the quality of inverse projections (discussed further in Sec.~\ref{sec:inv_quality}). Indeed, if a direct projection $P$ has poor quality, then it will not preserve some aspects of its input data $D$. As such, and since inverse projections $P^{-1}$ are constructed based on the projection $P(D)$ of the data $D$, it follows that also $P^{-1}$ will inherently have low quality. In turn, poor direct and/or inverse projection methods will affect the quality of visualizations constructed using them such as the decision maps we discuss further in Sec.~\ref{sec:dbm_ch2}. Importantly, we note here that, in general, no projection technique $P$ can preserve \emph{all} structure in $D$ perfectly in $P(D)$ -- that is, at least some quality metrics will have relatively low values. This is inherent to the operation of mapping datasets having high \emph{intrinsic} dimensionality to a (very) low-dimensional target space.

preserve \emph{all} structure in $D$ perfectly in $P(D)$ -- that is, at least some quality metrics will have relatively low values. This is inherent to the operation of mapping datasets having high \emph{intrinsic} dimensionality to a (very) low-dimensional target space. To disentangle such quality assessments, a simple but effective rule of thumb is to first measure the quality of $P(D)$ for some given dataset $D$. If this quality is deemed by the users to be too low for their application at hand, then $P(D)$ should be discarded and one should attempt to create better results by using \emph{e.g.} different projection techniques or their hyperparameter settings\,\citep{martins14}. Conversely, if the quality of $P(D)$ is deemed good enough, then one can further measure the quality of $P^{-1}$ to decide if this latter technique is good enough for the application context. We will use this approach throughout all our work in this thesis. \subsection{Interaction between ML and DR} As mentioned in Sec.~\ref{sec:introduction_ch2}, our central statement is that \emph{learning} (accomplished using ML) and \emph{seeing} (visualization accomplished using DR) are intimately related to each other. This assertion, illustrated by Fig.~\ref{fig:framework_ch2}, is explored next in detail. \subsubsection{How DR helps ML and conversely}

mentioned in Sec.~\ref{sec:introduction_ch2}, our central statement is that \emph{learning} (accomplished using ML) and \emph{seeing} (visualization accomplished using DR) are intimately related to each other. This assertion, illustrated by Fig.~\ref{fig:framework_ch2}, is explored next in detail. \subsubsection{How DR helps ML and conversely} \textbf{Machine learning pipeline:} The central box (Fig.~\ref{fig:framework_ch2} blue) shows a technical view on the typical ML pipeline which maps an input real-valued dataset $D$ into class labels or another real-valued signal by means of a classifier, respectively regressor. Such ML pipelines can be next deployed to assist a wide variety of tasks. In our work here, we do not further detail these, but rather focus on how DR techniques can be used to assist the technical aspects of a typical, task-generic, ML pipeline; and conversely, how ML techniques can generically assist constructing better DR methods. As explained earlier in Sec.~\ref{sec:introduction_ch2}, ML models operate on high-dimensional data. The green arrows atop this pipeline point to various visualization methods that use DR to depict such data. By using such visualizations, one can literally `see' how the model learns. We further exemplify the use of such visualizations for ML tasks such as semi-automatic labeling (Sec.~\ref{sec:pseudolabeling}), assessing classification difficulty (Sec.~\ref{sec:assessing}), and assessing training of DL models (Sec.~\ref{sec:understanding_dl}).

data. By using such visualizations, one can literally `see' how the model learns. We further exemplify the use of such visualizations for ML tasks such as semi-automatic labeling (Sec.~\ref{sec:pseudolabeling}), assessing classification difficulty (Sec.~\ref{sec:assessing}), and assessing training of DL models (Sec.~\ref{sec:understanding_dl}). \textbf{Dimensionality reduction pipeline:} The bottom box (Fig.~\ref{fig:framework_ch2} yellow) shows how ML regressors can be used to create better DR projections of any high-dimensional data. Examples of this process include \mbox{(self-)}supervised projections and sensitivity analyses (Sec.~\ref{sec:nnp}), inverse projections (Sec.~\ref{sec:nninv}), and quality analysis for inverse projections (Sec.~\ref{sec:ml_for_dr}). Such better DR methods can be next used for assisting ML engineering tasks, as shown by the red arrow in Fig.~\ref{fig:framework_ch2}. \caption{Two-way interaction between machine learning (ML) and dimensionality reduction (DR) workflows. ML algorithms can be used to construct DR techniques. In turn, these can be used to construct explanatory visualizations for ML. See Sec.~\ref{sec:ml_dr_common}.} \subsection{Common aspects of DR and ML} Section~\ref{sec:ml_dl_help} and Fig.~\ref{fig:framework_ch2} have outlined how DR can help ML and conversely. As such, it is not surprising that DL and ML share many common aspects. We detail next such commonalities, grouped in functional and non-functional ones, following a systems engineering perspective\,\citep{sommerville15}.

Fig.~\ref{fig:framework_ch2} have outlined how DR can help ML and conversely. As such, it is not surprising that DL and ML share many common aspects. We detail next such commonalities, grouped in functional and non-functional ones, following a systems engineering perspective\,\citep{sommerville15}. Functional aspects describe how a system should operate. As already outlined, both ML models $f$ and DR projection methods $P$ are specialized cases of \emph{inference} involving high-dimensional data. More specifically, $P$ can be seen as a particular type of regressor from $\mathbb{R}^n$ to $\mathbb{R}^2$. Given this, we next use the notation $\mathcal{M}$ to jointly denote an ML model or DR algorithm, when distinguishing between the two is not important. Non-functional aspects describe how a system should behave in practice. Without claiming full coverage, we identify the following key aspects that both ML and DR techniques $\mathcal{M}$ strive to achieve in their operation. We also outline cases where these two classes of techniques achieve the respective requirements up to different degrees, thereby pointing to potential synergies where one technique family can be used to assist the other.

strive to achieve in their operation. We also outline cases where these two classes of techniques achieve the respective requirements up to different degrees, thereby pointing to potential synergies where one technique family can be used to assist the other. \textbf{Genericity:} In the ideal case, $\mathcal{M}$ should be readily applicable to any dataset $D$ -- that is, of any dimensionality, attribute types, and provenance application domain. \textbf{Accuracy:} $\mathcal{M}$ should deliver highly accurate results (inferences for ML; projection scatterplots for DR) as gauged by specific quality metrics in the two fields. \textbf{Scalability:} $\mathcal{M}$ should scale well computationally with the number of samples $N$ and dimensions $n$ -- ideally, $\mathcal{M}$ should be linear in both $N$ and $n$. In practice, $\mathcal{M}$ should be able to handle datasets with millions of samples and hundreds of dimensions on commodity hardware at interactive rates. This further enables the use of $\mathcal{M}$ in visual analytics (VA) scenarios where the iterative and interactive exploration of complex hypotheses via data visualization is essential. We discuss this aspect further in Secs.~\ref{sec:ml_survey} and~\ref{sec:dr_survey}.

on commodity hardware at interactive rates. This further enables the use of $\mathcal{M}$ in visual analytics (VA) scenarios where the iterative and interactive exploration of complex hypotheses via data visualization is essential. We discuss this aspect further in Secs.~\ref{sec:ml_survey} and~\ref{sec:dr_survey}. \textbf{Understandability:} For a technique to be useful and usable in practice, its operation should be easily understandable by its intended users. This requirement takes different forms for ML and DR techniques. In general, ML techniques have an easy-to-understand \emph{output} -- they are designed to infer features having a clear meaning, \emph{e.g.}, the classes present in a dataset. However, due to their often black-box nature, the way in which they \emph{operate} to do this is far less understandable, leading to challenges for their design, deployment, and acceptance (see next Sec.~\ref{sec:understanding_dl}). In contrast, most DR methods have a relatively clear way of operation -- the projection aims to minimize a cost function that preserves certain aspects of the data $D$ in the projection scatterplot $P(D)$\,\citep{sorzano2014surveydimensionality,nonato18}. However, their output -- a raw scatterplot, in the minimal case -- is hard to interpret and requires additional explanatory mechanisms\,\citep{martins14,dasilva15,coimbra2016Explainingthreedimensional,tvisne,marcilio21,tian2021Usingmultiple,thijssen23}.

aims to minimize a cost function that preserves certain aspects of the data $D$ in the projection scatterplot $P(D)$\,\citep{sorzano2014surveydimensionality,nonato18}. However, their output -- a raw scatterplot, in the minimal case -- is hard to interpret and requires additional explanatory mechanisms\,\citep{martins14,dasilva15,coimbra2016Explainingthreedimensional,tvisne,marcilio21,tian2021Usingmultiple,thijssen23}. Understandability is subtly related, but not identical, to the concept of \emph{interpretability}. As mentioned above, we refer to understandability as the `low level' ability of the intended users of a technique or tool to grasp how the tool works, at a basic level, so they are able to deploy it in practice. Interpretability operates at a higher conceptual level and refers to the ability of the users to reason about how the tool operates \emph{internally} when executing its work. For ML models, for instance, linear regression is arguably more interpretable than deep neural networks due to its inherent linear model. Similarly, PCA's operation based on a global and linear data transformation is easier to understand than local and/or non-linear DR techniques such as t-SNE. In our further discussion, we mainly focus on the lower level of understandability.

inherent linear model. Similarly, PCA's operation based on a global and linear data transformation is easier to understand than local and/or non-linear DR techniques such as t-SNE. In our further discussion, we mainly focus on the lower level of understandability. \textbf{Out of sample (OOS):} An operator $\mathcal{M}$ is said to be OOS if it can extrapolate its behavior beyond the data from which it was constructed. In ML, this usually means that the model $f$ extrapolates from a training set $D_t$ to an unseen test set $D_T$ and beyond. By analogy, a projection $P$ is OOS if, when extending some dataset $D$ with additional samples $D' \nsubseteq D$, the projection $P(D \cup D')$ ideally keeps the samples of $D$ at the locations they had in $P(D)$, \emph{i.e.}, $P(D \cup D') = P(D) \cup P(D')$. If $P$ is OOS, this helps users to maintain their `mental map' obtained by studying $P(D)$ when they further study $P(D \cup D')$. As most ML methods are OOS by design, they can be potentially used to design OOS projections (see next Sec.~\ref{sec:dr_survey}).

OOS, this helps users to maintain their `mental map' obtained by studying $P(D)$ when they further study $P(D \cup D')$. As most ML methods are OOS by design, they can be potentially used to design OOS projections (see next Sec.~\ref{sec:dr_survey}). \textbf{Stability:} Small changes in the input dataset $D$ should only lead to small changes in the output dataset $\mathcal{M}(D)$. If not, spurious perturbations in $D$ can massively affect the resulting inference $\mathcal{M}(D)$ thereby rendering such results potentially unusable and/or misleading. Similarly, large-scale changes in $D$ should arguably lead to correspondingly large changes in $\mathcal{M}(D)$. Stability is related but not the same as OOS: An OOS algorithm is stable by definition but not all stable algorithms are OOS\,\citep{vernier21,espadoto19,oliveira2023StabilityAnalysis}. Most ML methods are OOS by design, a property which is not shared by many projection techniques -- therefore opening up an interesting case for using ML for DR. We discuss stability and OOS in more detail in Secs.~\ref{sec:nnp} and~\ref{sec:ml_for_dr}.

Most ML methods are OOS by design, a property which is not shared by many projection techniques -- therefore opening up an interesting case for using ML for DR. We discuss stability and OOS in more detail in Secs.~\ref{sec:nnp} and~\ref{sec:ml_for_dr}. \textbf{Ease of use:} Visualization methods aim, by construction, to be easily usable by a wide range of users and with minimal or no programming effort. In contrast, building -- and especially debugging and fine-tuning -- an ML pipeline can be challenging for practitioners with limited training in ML. As such, this offers opportunities for using visualization (and DR in particular) to ease the task of ML practitioners. \textbf{Availability:} $\mathcal{M}$  should be readily available to practitioners in terms of documented open-source code. While sometimes neglected, this is a key requirement for ML and DR algorithms to become adopted and impactful in practice.

ease the task of ML practitioners. \textbf{Availability:} $\mathcal{M}$ should be readily available to practitioners in terms of documented open-source code. While sometimes neglected, this is a key requirement for ML and DR algorithms to become adopted and impactful in practice. Table~\ref{tab:reqs} summarizes the above observations at a high level by comparing how ML and DL techniques satisfy in general the above requirements. Scores are given on a 5-point Likert scale (++: best; -\,-: worst), according to our own experience. Besides genericity, where both ML and DR algorithms score equally well, all other requirements are met complementarity by the two algorithm families. This supports our earlier point that the two technique classes can support each other, if combined properly. \caption{Comparison of how ML and DR methods satisfy desirable requirements (\textbf{Gen}ericity, \textbf{Acc}uracy, \textbf{Scal}ability, \textbf{Out} (understandability of output), \textbf{Alg} (understandability of algorithm), \textbf{OOS}, \textbf{Stab}ility, \textbf{Ease} of use, \textbf{Avail}ability).} \begin{tabular}{c | c  c  c  c  c  c  c  c  c} \textbf{Methods} & \textbf{Gen} &  \textbf{Acc} & \textbf{Scal} & \textbf{Out}  & \textbf{Alg} & \textbf{OOS} & \textbf{Stab} & \textbf{Ease} & \textbf{Avail} \\ ML & ++ & ++  & ++ & ++ & -\,- & ++  & ++ & - & ++ \\

\textbf{Methods} & \textbf{Gen} & \textbf{Acc} & \textbf{Scal} & \textbf{Out} & \textbf{Alg} & \textbf{OOS} & \textbf{Stab} & \textbf{Ease} & \textbf{Avail} \\ ML & ++ & ++ & ++ & ++ & -\,- & ++ & ++ & - & ++ \\ DR & ++ & -/+ & - & -\,- & + & -\,- & - & ++ & ++ \\ We next explore these commonalities and contrasts by first discussing how DR is used to help ML (Sec.~\ref{sec:ml_survey}) and next how ML is used to create better DR algorithms (Sec.~\ref{sec:dr_survey}). \section{Seeing for learning: DR assists ML} Many examples of visualization applications that assist ML workflows exist, most often coming in the form of complex multiple-view visual analytics systems\,\citep{garcia18,hohman19,yuan2021surveyvisual,alicioglu22,chatzimparmpas2023DeforestVisBehavior}. An exhaustive presentation thereof is out of the scope of this chapter. Rather, we focus in the following on selected use-cases where DR techniques have been used, with minimal additions, to assist ML workflows: assessing and improving classifiers (Sec.~\ref{sec:assessing}), pseudolabeling for enriching training sets (Sec.~\ref{sec:pseudolabeling}), exploring deep learning models (Sec.~\ref{sec:understanding_dl}), and exploring classifier outputs via decision boundary maps (Sec.~\ref{sec:dbm_ch2}). We also highlight connection points between the discussed techniques and the focus of our research, namely decision maps. \subsection{Assessing and improving classifiers}

enriching training sets (Sec.~\ref{sec:pseudolabeling}), exploring deep learning models (Sec.~\ref{sec:understanding_dl}), and exploring classifier outputs via decision boundary maps (Sec.~\ref{sec:dbm_ch2}). We also highlight connection points between the discussed techniques and the focus of our research, namely decision maps. \subsection{Assessing and improving classifiers} One of the simplest, and still most frequently used, application of DR in ML is to project a labeled training or test set $D_a$ with points $\mathbf{x}_i$ colored by their ground-truth labels $y_i$ or labels $f(\mathbf{x}_i)$ inferred by some classifier $f$. The rationale for this use-case is straightforward: A projection places similar samples close to each other; a classifier labels similar samples similarly; hence, the visual structure of the projection helps several tasks: \item see how (and where) are \emph{misclassified} samples distributed over the extent of a test set $D_T$ (to next elicit what makes them hard to classify); \item see how well a training set $D_t$ \emph{covers} the data space (to \emph{e.g.} determine where extra training samples are needed); \item see how well a training set $D_t$ is \emph{separated} into different same-label sample groups (to next predict the classification ease).

how well a training set $D_t$ \emph{covers} the data space (to \emph{e.g.} determine where extra training samples are needed); \item see how well a training set $D_t$ is \emph{separated} into different same-label sample groups (to next predict the classification ease). The two first tasks are quite straightforward. In contrast, the last task is particularly interesting. The intuition that a projection $P(D)$ which is well separated into compact same-label groups indicates that $D$ is easy to classify is quite old. Yet, a formal study of this correlation was only relatively recently presented\,\citep{rauber17}. In the respective work, the authors show that, given a range of classifiers, a dataset $D$ whose projection $P(D)$ has well-separated classes (as measured by the neighborhood hit metric\,\citep{paulovich2008least}) is far easier classifiable than a dataset whose projection shows intermixed points of different labels (low neighborhood hit). The projection $P(D)$ becomes a `predictor' for the ease of classifying $D$, helping one to assess classification difficulty \emph{before} actually embarking on the expensive cycle of classifier design-train-test. \caption{Classification difficulty assessment via projections\,\citep{rauber17}.}

points of different labels (low neighborhood hit). The projection $P(D)$ becomes a `predictor' for the ease of classifying $D$, helping one to assess classification difficulty \emph{before} actually embarking on the expensive cycle of classifier design-train-test. \caption{Classification difficulty assessment via projections\,\citep{rauber17}.} Figure~\ref{fig:classif_assessment} illustrates the above usage of projections. Images (a) and (b) show the two-class Madelon dataset\,\citep{madelon} ($n=500$ dimensions, $|C|=2$ classes) classified by KNN and Random Forests (RFC) respectively, with samples projected by t-SNE\,\citep{tSNEMaaten2008} and colored by class labels. The two projections show a very poor separation of the two classes, in line with the obtained low accuracies $AC=54\%$ and $AC=66\%$ (also visible by the misclassified samples, marked as triangles). Images (c) and (d) show the same dataset where extremely randomized trees\,\citep{geurts06} was used to select $n=20$ dimensions. The projections show a much higher visual separation of the two classes, in line with the higher accuracies $AC=88\%$ and $AC=89\%$ obtained. Many other examples in \citep{rauber17} show that projections can predict classification accuracy quite well. \textbf{Connection to DBMs:} Decision maps, discussed next in this chapter, keep the predictive power of projections for assessing classification models and also enhance this simple scatterplot-only view by filling in the gaps between projected data points.

classification accuracy quite well. \textbf{Connection to DBMs:} Decision maps, discussed next in this chapter, keep the predictive power of projections for assessing classification models and also enhance this simple scatterplot-only view by filling in the gaps between projected data points. \subsection{Pseudolabeling for ML training}

\textbf{Connection to DBMs:} Decision maps, discussed next in this chapter, keep the predictive power of projections for assessing classification models and also enhance this simple scatterplot-only view by filling in the gaps between projected data points. \subsection{Pseudolabeling for ML training} If projections are good predictors of classification accuracy, it means that their low-dimensional (2D) space captures well the similarity of the high-dimensional samples. This leads to the idea of using projections to create, rather than just explain, ML models. A first attempt was shown by \citet{bernard2017Comparingvisualinteractive} in the context of an user evaluation that compared classical active learning with a user-supported procedure they dubbed Visual Interactive Labeling (VIL). Next after that, \citet{benato2018SemiSupervisedLearning} proposed a very similar approach to VIL, called visual pseudolabeling, aimed to assist building a classifier from a training set having only very few labeled points: The entire training set, including unlabeled points, is projected and the user explores the projection to find unlabeled points tightly packed around labeled ones. Next, the user employs a tooltip to study the attributes of these points to confirm that they have the same class as the surrounded labeled one. If so, the user simply assigns that label to the unlabeled points. This workflow minimizes the user's labeling effort to quickly lead to sufficiently-large labeled sets for training the desired model. Interestingly, \emph{automatic} label propagation in the embedded space using state-of-the-art methods\,\citep{belkin06,amorim16} leads to \emph{poorer} results than user-driven labeling, which confirms the added value of the human-in-the-loop, and thus of the projections.

to sufficiently-large labeled sets for training the desired model. Interestingly, \emph{automatic} label propagation in the embedded space using state-of-the-art methods\,\citep{belkin06,amorim16} leads to \emph{poorer} results than user-driven labeling, which confirms the added value of the human-in-the-loop, and thus of the projections. However, optimal results are obtained when humans and machine \emph{cooperate}, rather than aim to replace, each other. \citet{benato2021SemiAutomaticData} refined the above workflow to (a) use automatic label propagation\,\citep{belkin06,amorim16} for the projection points where the propagation confidence is high; and (b) expose only the remaining unlabeled points to manual labeling (see Fig.~\ref{fig:label_prop}). This way, many `easy to label' points are handled automatically and the user's effort is channeled towards the hard cases, further reducing the manual labeling effort. This strategy also leads to increasing model accuracy and, again, surpassed confidence-based label propagation into the high-dimensional space. \caption{Semi-automatic label propagation for constructing training sets. An algorithm propagates ground-truth labels from a small set of supervised samples towards unlabeled neighbor samples in the projection. When this algorithm is uncertain, samples are left for manual labeling\,\citep{benato2021SemiAutomaticData}. See Sec.~\ref{sec:pseudolabeling}.}

space. \caption{Semi-automatic label propagation for constructing training sets. An algorithm propagates ground-truth labels from a small set of supervised samples towards unlabeled neighbor samples in the projection. When this algorithm is uncertain, samples are left for manual labeling\,\citep{benato2021SemiAutomaticData}. See Sec.~\ref{sec:pseudolabeling}.} \textbf{Connection to DBMs:} Decision boundaries are a crucial concept in the so-called \emph{uncertainty sampling} strategy in active learning \citep{monarch2021HumanLoopMachine}. As such, decision maps can be used to visualize the decision boundaries of a classifier, and therefore provide a visual cue to the user to select the most informative samples for annotating. This strategy was recently explored showing an increase of the efficiency of users in constructing annotated training sets as compared to using only raw projections\,\citep{benato2024HumanloopUsing}.

provide a visual cue to the user to select the most informative samples for annotating. This strategy was recently explored showing an increase of the efficiency of users in constructing annotated training sets as compared to using only raw projections\,\citep{benato2024HumanloopUsing}. Deep learned (DL) models, with their millions of parameters, are among the hardest artifacts in ML to understand\,\citep{shwartz17,azodi20}. Visualization has been listed early on as the technique of choice for explainable AI (XAI) for DL models\,\citep{tzeng05}. A recent survey\,\citep{garcia18} outlines a wide spectrum of visual analytics techniques and tools used for DL engineering, grouped along how they support the tasks of training analysis (TA), architecture understanding (AU), and feature understanding (FU). Given the diversity of these tasks, the variety of the proposed visualization solutions -- \emph{e.g.} matrix plots\,\citep{pezzotti-2017-deepeyes}, icicle plots\,\citep{aksallah}, parallel coordinate plots\,\citep{lstmvis}, stacked barcharts, annotated networks\,\citep{liu-2016-towards}, activation maps\,\citep{gradcam_pp} -- is not surprising.

(TA), architecture understanding (AU), and feature understanding (FU). Given the diversity of these tasks, the variety of the proposed visualization solutions -- \emph{e.g.} matrix plots\,\citep{pezzotti-2017-deepeyes}, icicle plots\,\citep{aksallah}, parallel coordinate plots\,\citep{lstmvis}, stacked barcharts, annotated networks\,\citep{liu-2016-towards}, activation maps\,\citep{gradcam_pp} -- is not surprising. Projections occupy a particular role among such visualizations due to their ability to compactly capture high-dimensional data -- in the limit, a projection needs a single pixel to represent an $n$-dimensional point, for any value of $n$. As such, they are very suitable instruments to depict several aspects of a DL model. For example, in Fig.~\ref{fig:vis_hidden}a, every point denotes a high-dimensional sample in $D$, in this case a digit image from the SVHN dataset\,\citep{rauber17}. The points, colored by their ground-truth classes, have as dimensions all activations of the last hidden layer -- also called \emph{learned features} -- of a DL model trained to classify this dataset. We notice a good separation of same-class images (the projection contains compact same-color groups), which tells that the model's training went well. We also see, for each color (class), two distinct such groups. This tells that the model has learned to split images of the \emph{same} digit into two subclasses. Upon inspection, illustrated by the tooltips in the figure, we see that the model has learned \emph{by itself} to separate dark-on-bright-background digits from bright-on-dark background ones. Such findings would be hard to get without the projection-based visual exploration tool. Moreover, such findings can help fine-tuning the model to increase performance -- in this case, eliminate the learning of the background-\emph{vs}-foreground artificial separation for same-class digits.

ones. Such findings would be hard to get without the projection-based visual exploration tool. Moreover, such findings can help fine-tuning the model to increase performance -- in this case, eliminate the learning of the background-\emph{vs}-foreground artificial separation for same-class digits. Figure~\ref{fig:vis_hidden}b explores a different DL aspect, namely how the model learns. For every epoch, a projection of all training-set samples is made, using as dimensions the samples' last hidden layer activations, similar to image (a). To maintain temporal coherence, \emph{i.e.}, have similar-value samples from the same or different epochs project to close locations, a \emph{dynamic} projection algorithm, in this case dt-SNE\,\citep{rauber_dtsne}, was used (see further Sec.~\ref{sec:ml_for_dr}). Next, same-sample points from all epochs are connected by a trail. As the last step, trails are bundled in 2D\,\citep{cubu} to reduce visual clutter. The resulting image (b) shows how the projection literally `fans out' from a dark clump (in the middle of the image), which represents the similar activations of all samples in the first epoch, to separate clusters of same-label points (in the final epoch). This effectively summarizes the training success -- the model has increasingly learned to separate the classes throughout its training. We also see some challenges of this model: The purple bundle (digit 4) is less well separated from the others, which indicates difficulties in classifying this digit.

-- the model has increasingly learned to separate the classes throughout its training. We also see some challenges of this model: The purple bundle (digit 4) is less well separated from the others, which indicates difficulties in classifying this digit. Figure~\ref{fig:vis_hidden}c shows a similarly-constructed visualization but where the trails connect projections of test-set image activations through all network's hidden \emph{layers}. Bundles start fanned out but apart from each other, indicating that the trained model successfully separates the classes even after its first layer. Same-color trails in a bundle progressively fan in and also stay separated from trails in other bundles, indicating that, as we go down the model towards its further layers, class separation only becomes better, \emph{i.e.}, that the chosen network architecture is indeed good for the classification task at hand. \caption{Projections for understanding DL models. Exploring (a) activations of similar instances, (b) evolution of activations over training epochs, and (c) evolution of activations over network layers\,\citep{rauber17_vishidden}. See Sec.~\ref{sec:understanding_dl}.}

chosen network architecture is indeed good for the classification task at hand. \caption{Projections for understanding DL models. Exploring (a) activations of similar instances, (b) evolution of activations over training epochs, and (c) evolution of activations over network layers\,\citep{rauber17_vishidden}. See Sec.~\ref{sec:understanding_dl}.} \textbf{Connection to DBMs:} If visual structures in projections help understanding the operation of a DL model, then the richer visual structures shown by decision maps -- such as the shapes and sizes of decision zones and the smoothness (or lack thereof) of decision boundaries -- will be able to give richer insights on such operation. A key aspect of ML classification models are points in their input data space $Z \subset \mathbb{R}^n$ where $f$ changes output, \emph{i.e..}, the inferred class. Given the continuity assumption behind most ML models, such points are located on hypersurfaces (manifolds) embedded in $Z$, also called \emph{decision surfaces} (see the light blue surfaces in Fig.~\ref{fig:dbm_idea}a). These partition the $Z$ space into compact regions where the classifier has the same output, also called \emph{decision zones}.

such points are located on hypersurfaces (manifolds) embedded in $Z$, also called \emph{decision surfaces} (see the light blue surfaces in Fig.~\ref{fig:dbm_idea}a). These partition the $Z$ space into compact regions where the classifier has the same output, also called \emph{decision zones}. \caption{Decision boundary maps. (a) A high-dimensional dataset with its decision boundary hypersurfaces. (b) Projecting the samples (green) and decision boundaries (light blue) of this dataset yields the red 2D points, respectively light blue 2D curves. (c) Example of such a 2D projection with samples colored by the class inferred by a ML model. (d) The decision zones for this classifier are depicted in the 2D projection space as same-color areas. See Sec.~\ref{sec:dbm_ch2}.}

blue 2D curves. (c) Example of such a 2D projection with samples colored by the class inferred by a ML model. (d) The decision zones for this classifier are depicted in the 2D projection space as same-color areas. See Sec.~\ref{sec:dbm_ch2}.} As described so far, projections $P(D)$ depict a \emph{discrete} set of samples $D$, optionally color-coded to show the behavior of a ML model $f$. For the dataset $D$ represented by the green points in Fig.~\ref{fig:dbm_idea}a, this would yield the red-points scatterplot in Fig.~\ref{fig:dbm_idea}b. Such images, however, do not explicitly show where decision boundaries are -- we know that they occur somewhere between the red dots, but not where precisely. Depicting such boundaries, along the color-coded training and/or test sets of $f$, significantly improves the understanding of how $f$ actually behaves. This can help ML engineers to find where in the input space more training samples are needed to improve a classifier or, conversely, assess in which such areas would samples be misclassified. \subsubsection{Basic idea of decision boundary maps}

actually behaves. This can help ML engineers to find where in the input space more training samples are needed to improve a classifier or, conversely, assess in which such areas would samples be misclassified. \subsubsection{Basic idea of decision boundary maps} Decision boundary maps (DBMs, sometimes called simply decision maps in various papers and also this thesis) are a visual representation for both decision zones and boundaries for any classifier\,\citep{schulz2015Usingdiscriminative,rodrigues2018Imagebasedvisualization}. Intuitively put, DBMs aim to map the entire space $Z$ (as classified by $f$) to 2D rather than the discrete sample set $D$, as follows. Given a training and/or test set $D$, a direct projection $P$ is used to create a 2D embedding thereof. After that, given an image space $I \subset \mathbb{R}^2$, a mapping $P^{-1} : I \rightarrow \mathbb{R}^n$ is constructed to reverse the effects of $P$. We describe $P^{-1}$ in detail further in Sec.~\ref{sec:nninv}.

is used to create a 2D embedding thereof. After that, given an image space $I \subset \mathbb{R}^2$, a mapping $P^{-1} : I \rightarrow \mathbb{R}^n$ is constructed to reverse the effects of $P$. We describe $P^{-1}$ in detail further in Sec.~\ref{sec:nninv}. The mapping $P^{-1}$ is then used to `backproject' each pixel $\mathbf{p} \in I$ to a high-dimensional point $\mathbf{x}=P^{-1}(\mathbf{p})$, $\mathbf{x} \in Z$. Finally, each pixel $\mathbf{p}$ is colored by the label $f(\mathbf{x})$ assigned to it by the trained model to be explored. Same-color areas emerging in $I$ indicate $f$'s decision zones; pixels on the frontiers of these areas show $f$'s decision boundaries. Figure~\ref{fig:dbm_idea}d shows this for a KNN classifier trained to produce the test set depicted by the projection in Fig.~\ref{fig:dbm_idea} for a six-class problem.

in $I$ indicate $f$'s decision zones; pixels on the frontiers of these areas show $f$'s decision boundaries. Figure~\ref{fig:dbm_idea}d shows this for a KNN classifier trained to produce the test set depicted by the projection in Fig.~\ref{fig:dbm_idea} for a six-class problem. The key to DBM construction is creating the mapping $P^{-1}$ for a given direct projection $P$. In principle, any combination of $P$ and $P^{-1}$ can be used to construct a DBM for any given classifier by directly following the per-pixel procedure outlined above. However, earlier studies have shown that, for certain classification problems where one has ground-truth information about the expected outcomes -- for example, in the sense of the shapes, sizes, and smoothness of the decision zones that a given classifier should create for that dataset -- certain direct projections $P$ and $P^{-1}$ combinations work better\,\citep{DBM2019rodrigues,oliveiraSDBM2022}. We discuss these aspects separately in Sec.~\ref{sec:nninv}. \subsubsection{Enhancements of basic DBMs}

of the shapes, sizes, and smoothness of the decision zones that a given classifier should create for that dataset -- certain direct projections $P$ and $P^{-1}$ combinations work better\,\citep{DBM2019rodrigues,oliveiraSDBM2022}. We discuss these aspects separately in Sec.~\ref{sec:nninv}. \subsubsection{Enhancements of basic DBMs} DBMs can be further enhanced to encode, via brightness, the model's confidence at every 2D pixel (Figs.~\ref{fig:dbm}a,c) or the actual distance, in $Z$, to the closest decision boundary (Fig.~\ref{fig:dbm}b). The appearing brightness gradients tell which areas in the projection space are more prone to misclassifications. Importantly, this does not require actual training or test samples to exist in these areas -- rather, such samples are synthesized by $P^{-1}$. \caption{Decision boundary maps for classifier analysis with luminance encoding classifier confidence (a,c)\,\citep{schulz2015Usingdiscriminative,oliveiraSDBM2022}, respectively distance-to-decision-boundary (b)\,\citep{DBM2019rodrigues}. See Sec.~\ref{sec:dbm_ch2}.}

Importantly, this does not require actual training or test samples to exist in these areas -- rather, such samples are synthesized by $P^{-1}$. \caption{Decision boundary maps for classifier analysis with luminance encoding classifier confidence (a,c)\,\citep{schulz2015Usingdiscriminative,oliveiraSDBM2022}, respectively distance-to-decision-boundary (b)\,\citep{DBM2019rodrigues}. See Sec.~\ref{sec:dbm_ch2}.} Interpreting confidence or distance-enhanced DBMs is, however, not trivial, as illustrated next by the example in Fig.~\ref{fig:dbm_interp}. Image (a) shows the MNIST digit dataset\,\citep{lecun2010MNISThandwritten} ($n=782$ dimensions, $|C|=10$ classes) projected to 2D by using t-SNE and classified by a neural network. Image (b) shows the DBMs for this problem. Image (c) enhances this by encoding the classifier confidence encoded into brightness (dark=lower confidence). For clarity, image (d) shows the confidence information separately (green=low confidence; yellow=high confidence). The images (c) and (d) convey the impression that the visualized classifier is highly, and equally, confident in all areas except very close to its decision boundaries.

For clarity, image (d) shows the confidence information separately (green=low confidence; yellow=high confidence). The images (c) and (d) convey the impression that the visualized classifier is highly, and equally, confident in all areas except very close to its decision boundaries. Combining this information with the distance-to-closest-decision boundary reveals a different story. Image (e) shows this distance. In contrast to earlier techniques\,\citep{DBM2019rodrigues} (Fig.~\ref{fig:dbm}b) which use expensive iterative-search in the high-dimensional space to locate, for each pixel $\mathbf{p}$, the distance from $\mathbf{x} = P^{-1}(\mathbf{p})$ to its closest decision boundary, \citet{differentiableDBM} proposed recently to create such images (e) by a simpler, and much faster approach. For each such point $\mathbf{x}$, they synthesize its closest adversarial example $\mathbf{a} \in Z$ and approximate the sought distance as $\|\mathbf{x} - \mathbf{a}\|$ using DeepFool\,\citep{moosavi-dezfooli2016DeepFoolsimple}.

\citet{differentiableDBM} proposed recently to create such images (e) by a simpler, and much faster approach. For each such point $\mathbf{x}$, they synthesize its closest adversarial example $\mathbf{a} \in Z$ and approximate the sought distance as $\|\mathbf{x} - \mathbf{a}\|$ using DeepFool\,\citep{moosavi-dezfooli2016DeepFoolsimple}. This is orders of magnitude faster than iterative search and allows generating the desired distances in subsecond time on a commodity PC. Examining image (e), we see that the distance-to-boundary evolves very differently for the different decision zones and has complex patterns even in a single such zone, indicating that certain points are far closer to decision boundaries than others. For example, the red decision zone, although appearing very close to its neighbors in the raw projection (Fig.~\ref{fig:dbm_interp}a, is quite bright, telling that it is farther away from its surrounding decision boundaries, than the other, darker, zones. Image (f) shows the same information, but with inverse brightness mapping than in (e). This highlights zones close to decision boundaries, \emph{i.e.}, where the classifier may have trouble. We see, for example, a small yellow decision zone (marked by a white triangle). This zone, which is also disconnected from the other, larger, yellow decision zone (thus, for the same class), is very bright in image (e), indicating that it is very close to decision boundaries. This likely indicates potential model instabilities in this area.

which is also disconnected from the other, larger, yellow decision zone (thus, for the same class), is very bright in image (e), indicating that it is very close to decision boundaries. This likely indicates potential model instabilities in this area. \caption{Explanatory visualizations for interpreting DBMs created by the technique of \citet{differentiableDBM}. See Sec.~\ref{sec:dbm_ch2}.}

the same class), is very bright in image (e), indicating that it is very close to decision boundaries. This likely indicates potential model instabilities in this area. \caption{Explanatory visualizations for interpreting DBMs created by the technique of \citet{differentiableDBM}. See Sec.~\ref{sec:dbm_ch2}.} To further explore this hypothesis, \citet{differentiableDBM} performed a simple experiment, as follows. They select ten pixels in the above-mentioned small yellow region, synthesize their corresponding data samples by $P^{-1}$, and add to them a wrong label -- corresponding to the cyan color instead of the correct, yellow, label (see Fig.~\ref{fig:dbm_interp}g, with the selected pixels marked in red). They next add these mislabeled points to the training set, re-train the model, and visualize its DBM. The result (Fig.~\ref{fig:dbm_interp}h), shows how the small yellow region has become cyan, which is potentially not surprising given the newly added labels. More interestingly, however, we see large changes in decision zones of \emph{different} classes adjacent to the yellow region: The dark-blue zone grows significantly to cut away a portion of the brown zone. This shows that few data changes in a small decision zone, potentially flagged by the DBM visualization as unstable, change indeed the overall behavior of the classifier even outside this zone.

to cut away a portion of the brown zone. This shows that few data changes in a small decision zone, potentially flagged by the DBM visualization as unstable, change indeed the overall behavior of the classifier even outside this zone. Annotating a DBM with the classifier confidence and/or distance to closest decision boundary does not, however, reveal all information that characterizes different decision zones. Indeed, one additional such information involves how close the DBM points are to the actual training points that the classifier was constructed from. We illustrate the added-value of this information next. Images (c-f) in Fig.~\ref{fig:dbm_interp} show several large decision zones, \emph{e.g.} the green and orange ones, which look quite similar from the perspective of confidence and distance to boundary -- their inner pixels appear to be quite confident and far away from the surrounding decision boundaries. To gain more insight, \citet{differentiableDBM} visualize, for each pixel $\mathbf{p}$, the distance of its corresponding data sample to the closest training-set point, \emph{i.e.} d_D(\mathbf{p}) = \min_{\mathbf{x} \in D_t} \| P^{-1}(\mathbf{p}) - \mathbf{x} \|,

far away from the surrounding decision boundaries. To gain more insight, \citet{differentiableDBM} visualize, for each pixel $\mathbf{p}$, the distance of its corresponding data sample to the closest training-set point, \emph{i.e.} d_D(\mathbf{p}) = \min_{\mathbf{x} \in D_t} \| P^{-1}(\mathbf{p}) - \mathbf{x} \|, which we will also use in Chapters 4 and 7. Figure~\ref{fig:dbm_interp}i shows the distance $d_D$ for the MNIST classifier, with dark blue indicating small distances and yellow large ones, respectively. We immediately see that all pixels of the orange decision zone are very close to the training-set, whereas pixels in all other zones appear much farther away. This indicates non-linear behavior of the DBM construction algorithm -- the visible sizes of the decision zones in the DBM do not indicate actual sizes in the data space. Differently put, the orange decision zone is much closer `wrapped around' training-set points than the other zones. This indicates that, all other aspects being equal, one should have more trust in the classifier behavior in the orange zone, as its depicted points are much closer to the training set that the classifier was built from.

other zones. This indicates that, all other aspects being equal, one should have more trust in the classifier behavior in the orange zone, as its depicted points are much closer to the training set that the classifier was built from. Additionally, we see in image (i) a bright yellow band at the bottom of the corresponding pink decision zone. This tells that points around this decision boundary (between the pink and green zone) are quite far away from \emph{any} training-set point. As such, even if the confidence of the classifier appears quite high in this area, apart from points very close to the decision boundary (see image (d)), the classifier extrapolates much farther away from its training data here, so, it is more prone to errors. Note that one would expect the confidence to drop as the data points become further apart from the training set (intuitively, what the classifier learned from that training set is now `stretched' to account for very different data), but this is not the case for this classifier. The visualizations show that purely relying on classifier confidence is not sufficient for users to gain enough understanding of what the classifier does in specific situations and, hence, whether they trust (or not) the classifier in those situations.

this classifier. The visualizations show that purely relying on classifier confidence is not sufficient for users to gain enough understanding of what the classifier does in specific situations and, hence, whether they trust (or not) the classifier in those situations. Besides the above, we see, within each decision zone, a varying color pattern consisting of dark `cells' separated by slightly brighter bands. These indicate how the respective sub-areas in a decision zone have been created by samples in the training-set -- much like the visualization of a Voronoi diagram whose sites are the training-set samples. Figure~\ref{fig:dbm_interp}j shows a final variation of the explanatory visualizations for DBMs. Here, instead of depicting the distance of a map pixel to the closest training-set point, \citet{differentiableDBM} show the distance to the closest training-set point of the same class as the pixel itself. d_D^{sameclass} (\mathbf{p}) = \min_{\mathbf{x} \in D_t | f(\mathbf{x}) = f(P^{-1}(\mathbf{p}))}  \| P^{-1}(\mathbf{p}) - \mathbf{x} \|.

a map pixel to the closest training-set point, \citet{differentiableDBM} show the distance to the closest training-set point of the same class as the pixel itself. d_D^{sameclass} (\mathbf{p}) = \min_{\mathbf{x} \in D_t | f(\mathbf{x}) = f(P^{-1}(\mathbf{p}))} \| P^{-1}(\mathbf{p}) - \mathbf{x} \|. The distance $d_D^{sameclass}$ shows how far away samples that map to a decision zone are from training-set samples that led to the creation of that zone in the model $f$. We see that image (j) is quite similar to image (i). This is a positive finding, as it tells that pixels in a decision zone correspond to data points which are close to the training samples for that zone, which is indeed what a good DBM should show. In the same time, we see that the contrast between the orange and green zones, visible as dark blue, respectively bright green in image (j), has increased. This tells that the decision boundary between the orange and green zones is far \emph{closer} to the orange training samples than to the green ones -- an insight which the basic confidence or distance-to-boundary maps do not reveal. \subsection{Putting it all together: Visual analytics workflow}

boundary between the orange and green zones is far \emph{closer} to the orange training samples than to the green ones -- an insight which the basic confidence or distance-to-boundary maps do not reveal. \subsection{Putting it all together: Visual analytics workflow} At this point, we can further detail the general visual-analytics workflow of ML assistance by DR techniques introduced in Fig.~\ref{fig:framework_ch2} (green arrows). Figure~\ref{fig:workflow_final} does this by refining the workflow for using DR to assist ML proposed earlier by Rauber \emph{et al.} (Fig.~1 in\,\citet{rauber17}) to include the DR-based techniques discussed above in this section -- all which are novel in comparison with\,\citet{rauber17}), apart from the classification ease analysis (see next). This gives a practical guideline on how to use the presented visualization techniques in practical ML engineering. In the following, numbers in the text indicate steps in the workflow Fig.~\ref{fig:workflow_final}. \caption{Workflow of using DR techniques to assist ML (see Fig.~\ref{fig:framework_ch2}, green arrows). Visual analytics (VA) operations enabled by DR are marked by red-outlined boxes.} \item The process starts by acquiring a dataset that one wants to further analyze, \emph{e.g.} classify, using ML.

using DR techniques to assist ML (see Fig.~\ref{fig:framework_ch2}, green arrows). Visual analytics (VA) operations enabled by DR are marked by red-outlined boxes.} \item The process starts by acquiring a dataset that one wants to further analyze, \emph{e.g.} classify, using ML. \item If not enough labeled samples are available (1), pseudolabeling (Sec.~\ref{sec:pseudolabeling}) can be used to create additional ones (2), else the process continues with the available data (3). Projections and decision maps can guide users during the pseudolabeling process. \item From these data, features are typically extracted by various processing operations (4). \item Next, DR is used to construct a projection (5) from the data. The projection is visually studied to assess whether the data form sufficiently separated classes to suggest a feasible classification problem (Sec.~\ref{sec:assessing}). \item If so (7), a ML architecture is chosen to design and train a model. Else, the workflow reverts to extracting better features (6). \item DR-enabled techniques are next used to assess (8) whether the training performed well (Sec.~\ref{sec:understanding_dl}). \item If training is found unsatisfactory (9), the model is further inspected (Sec.~\ref{sec:understanding_dl}) to find whether it has a poor design (10) or was fed by poor features (11).

next used to assess (8) whether the training performed well (Sec.~\ref{sec:understanding_dl}). \item If training is found unsatisfactory (9), the model is further inspected (Sec.~\ref{sec:understanding_dl}) to find whether it has a poor design (10) or was fed by poor features (11). \item In the former case, the model goes to redesign stage; in the latter, different features are extracted. \item If the model's training was positively assessed (12), the flow continues with standard ML evaluation (testing). \item Upon measuring satisfactory performance (13), the workflow ends with an operational model ready for use. \item If testing performance is found too low (14), decision-map techniques (Sec.~\ref{sec:dbm_ch2}) can be used to find out whether different features (15) or if more (or different) training data is needed (16). In both such cases, the workflow continues from the respective earlier steps, as indicated in the figure. \section{Learning for seeing: ML assists DR}

to find out whether different features (15) or if more (or different) training data is needed (16). In both such cases, the workflow continues from the respective earlier steps, as indicated in the figure. \section{Learning for seeing: ML assists DR} Section~\ref{sec:ml_survey} has shown several examples of how DR visualizations help in several use-cases of ML engineering. In this section, we outline the opposite path, \emph{i.e.}, how ML techniques can be used to create DR visualizations so as to surpass limitations of existing DR algorithms. We discuss two classes of such methods for creating projections (Sec.~\ref{sec:nnp}), respectively inverse projections (Sec.~\ref{sec:nninv}). Tens of DR techniques have been developed in the visualization community. However, choosing such a technique to apply in practice, for instance for the ML use-cases outlined in Sec.~\ref{sec:ml_survey}, is challenging, as few comparisons of such techniques exist following all desirable requirements listed in Sec.~\ref{sec:commonalities}.

been developed in the visualization community. However, choosing such a technique to apply in practice, for instance for the ML use-cases outlined in Sec.~\ref{sec:ml_survey}, is challenging, as few comparisons of such techniques exist following all desirable requirements listed in Sec.~\ref{sec:commonalities}. A recent survey\,\citep{espadoto19} addressed this question at scale for the first time by comparing 44 projection techniques $P$ over 19 datasets $D$ from the perspective of 6 quality metrics $M$, using grid-search to explore the hyperparameter spaces of the projection techniques. Equally important, all its results -- datasets, projection techniques, quality metric implementations, study protocol -- are automated and freely available, much like similar endeavors in the ML arena. Following the survey's results, four projection methods consistently scored high on quality for all datasets, namely UMAP\,\citep{UMAP2018}, t-SNE\,\citep{tSNEMaaten2008}, IDMAP\,\citep{minghim2006content}, and PBC\,\citep{paulovich2006text}, with several others close to them. However, none of the top-ranked surveyed techniques also met the OOS, computational scalability, and stability criteria. As such, the survey concluded that better DR techniques are needed.

datasets, namely UMAP\,\citep{UMAP2018}, t-SNE\,\citep{tSNEMaaten2008}, IDMAP\,\citep{minghim2006content}, and PBC\,\citep{paulovich2006text}, with several others close to them. However, none of the top-ranked surveyed techniques also met the OOS, computational scalability, and stability criteria. As such, the survey concluded that better DR techniques are needed. Following the analogy with ML regressors and given that such regressors meet the OOS, scalability, and stability criteria (Sec.~\ref{sec:ml_dr_common}), it becomes interesting to consider ML for building better projection algorithms. Autoencoders\,\citep{hinton2006reducing} do precisely that and meet all requirements in Sec.~\ref{sec:ml_dr_common} except quality -- the resulting projections have in general poorer trustworthiness and continuity than state-of-the-art methods like UMAP and t-SNE. Figure~\ref{fig:nnp} illustrates this: The well-known MNIST dataset, which is well separable into its 10 classes by many ML techniques, appears, wrongly, poorly separated when projected by autoencoders. Following \citep{rauber17} (see also Sec.~\ref{sec:assessing}), we can conclude that autoencoders are a poor solution for DR. \subsubsection{Basic idea of learning projections}

is well separable into its 10 classes by many ML techniques, appears, wrongly, poorly separated when projected by autoencoders. Following \citep{rauber17} (see also Sec.~\ref{sec:assessing}), we can conclude that autoencoders are a poor solution for DR. \subsubsection{Basic idea of learning projections} The idea of using deep learning to create an OOS projection is quite old. \citet{pekalska1999new} proposed to do this to approximate Sammon's mapping in a way that could be extended to approximate also other DR techniques. Autoencoders, mentioned earlier, are another early approach for the same task. More recently, \citet{espadoto2020Deeplearning} proposed Neural Network Projections (NNP), a supervised approach to learning DR: Given any dataset $D$ and its projection $P(D)$ computed by the user's technique of choice $P$, a simple three-layer fully-connected network is trained to learn to regress $P(D)$ when given $D$. Despite its simplicity, NNP can learn to imitate any projection technique $P$ for any dataset $D$ surprisingly well. While NNP's quality is typically slightly lower than state-of-the-art projections like t-SNE and UMAP, it is a \emph{parametric} method, stable as proven by sensitivity analysis studies\,\citep{bredius2022VisualExploration}, OOS, linear in the sample count $N$ and dimensionality $n$ (in practice, thousands of times faster than t-SNE), and very simple to implement.

projections like t-SNE and UMAP, it is a \emph{parametric} method, stable as proven by sensitivity analysis studies\,\citep{bredius2022VisualExploration}, OOS, linear in the sample count $N$ and dimensionality $n$ (in practice, thousands of times faster than t-SNE), and very simple to implement. \caption{Projection of MNIST dataset with (a) t-SNE\,\citep{tSNEMaaten2008} and with deep learning methods: (b) NNP\,\citep{espadoto2020Deeplearning}, (c) kNNP\,\citep{modrakowski2022ImprovingDeep}, (d) autoencoders\,\citep{hinton2006reducing}, (e) SSNP\,\citep{espadoto2021SSNP}, (f,g) SHaRP\,\citep{ShaRP2023} with elliptic, respectively rectangular, cluster shapes, and (h-j) HyperNP\,\citep{appleby2021HyperNPInteractive} imitating t-SNE for three different perplexity values $p$. See Sec.~\ref{sec:dr_survey}.} \subsubsection{OOS and sensitivity analysis} As explained earlier, the OOS and sensitivity (stability to small changes of the input) are related, but not identical, desirable properties. We illustrate both properties for NNP next, noting also that all other similar deep-learned projection algorithms (kNNP, SNNP, SHaRP, autoencoders) share by construction the same properties, since they use very similar neural network architectures.

related, but not identical, desirable properties. We illustrate both properties for NNP next, noting also that all other similar deep-learned projection algorithms (kNNP, SNNP, SHaRP, autoencoders) share by construction the same properties, since they use very similar neural network architectures. Figure~\ref{fig:sensitivity} (top two rows) illustrates NNP's out-of-sample ability. The top row shows t-SNE projections of the MNIST dataset for an increasing number of samples (from 2K to 100K). As visible, the projection continuously changes, making it hard for users to maintain their mental map of the studied data. The row below shows NNP trained to mimic t-SNE. We see that the shape of the projection and location of its ten clusters (one per class) stays the same as more samples are added. A drawback of this is that the cluster separation is lower than for the t-SNE projection as more samples are added. This is expected since NNP was trained only on the initial set of 2K samples, \emph{i.e.}, it did not have a chance to see the additional ones.

separation is lower than for the t-SNE projection as more samples are added. This is expected since NNP was trained only on the initial set of 2K samples, \emph{i.e.}, it did not have a chance to see the additional ones. Figure~\ref{fig:sensitivity} (bottom row) illustrates NNP's stability. An NNP model is trained to project the MNIST dataset, after which is asked to project MNIST images where an increasingly larger number of dimensions (pixel values) have been cancelled, \emph{i.e.}, set to zero. Surprisingly, NNP can capture the cluster structure of the data (10 classes for the 10 digits) up to 40\% cancelled dimensions. The aggregated image shows the `movement' of the points in the NNP projection as increasingly more dimensions get dropped. Similar insights are obtained for other input-dataset perturbations such as sample jitter, translation, and scaling. At a higher level, we see sensitivity analysis as a very powerful, yet under-explored, technique -- well known in the ML repertoire -- to assess the quality of DR projections.

other input-dataset perturbations such as sample jitter, translation, and scaling. At a higher level, we see sensitivity analysis as a very powerful, yet under-explored, technique -- well known in the ML repertoire -- to assess the quality of DR projections. Subsequent refinements of NNP aim to keep the attractive aspects of the method (speed, OOS, genericity, stability, simplicity) while increasing its \emph{quality} and controlling the visual \emph{appearance} of the resulting projections (see further Fig.~\ref{fig:nnp}). k-NNP\,\citep{modrakowski2022ImprovingDeep} enhances the projection quality, measured following the metrics introduced in Sec.~\ref{sec:background_ch2}, by learning to project sets of neighbor samples rather than individual samples. SSNP\,\citep{espadoto2021SSNP} works in a self-supervised way, similar to autoencoders, thus dispenses of the need of a training projection. The self-supervised information comes either in the form of ground-truth labels (when available) or pseudolabels computed by clustering the input data. Since based on an autoencoder structure, SSNP can also create inverse projections (see next Sec.~\ref{sec:nninv}). SDR-NNP\,\citep{sdrnnp} increases NNP's ability to separate clusters of different observations by pre-sharpening the input training set $D_t$ via mean shift\,\citep{comaniciu02}. SHaRP\,\citep{ShaRP2023} refines SSNP to allow one to control the shapes of clusters of similar observations to match desired templates such as ellipses, rectangles, or triangles. Finally, HyperNP\,\citep{appleby2021HyperNPInteractive} extends NNP by learning the behavior of a projection technique $P$ for all its hyperparameter values, thereby allowing users to explore this parameter space at interactive rates. All the above results prove that DL is a serious contender for generating projections that comply with all requirements set forth by practice.

$P$ for all its hyperparameter values, thereby allowing users to explore this parameter space at interactive rates. All the above results prove that DL is a serious contender for generating projections that comply with all requirements set forth by practice. We will use deep-learned projections at several points in our work such as when quantitatively comparing decision maps created using such techniques (Chapter~\ref{ch4:metrics_decision_map}). \caption{NNP out-of-sample (OOS) analysis. Image taken from \citep{bredius2022VisualExploration}. The top row shows projections of the MNIST dataset using t-SNE for increasing numbers of samples in the test set $D_T$. The projection does not maintain stability. The second row shows how NNP maintains stability as new samples are added to the test set. Bottom row: NNP sensitivity analysis when removing between 10\% and 90\% of the dataset's dimensions. NNP can robustly depict the data structure even when a large part of the input information is missing. See Sec.~\ref{sec:nnp}.}

added to the test set. Bottom row: NNP sensitivity analysis when removing between 10\% and 90\% of the dataset's dimensions. NNP can robustly depict the data structure even when a large part of the input information is missing. See Sec.~\ref{sec:nnp}.} Following the success of DL for constructing projections $P$ outlined above, it becomes immediately interesting to consider their use for the complementary problem of computing \emph{inverse} projections $P^{-1}$. Introduced in Sec.~\ref{sec:dbm_ch2} for constructing DBMs, inverse projections have additional uses, \emph{e.g.}, generating synthetic samples for data augmentation scenarios\,\citep{rodrigues2018Imagebasedvisualization} and hypothesizing the unexplored regions of a sampled data space for \emph{e.g.} shape or image morphing applications\,\citep{Amorim2012ilamp}. \textbf{Definition:} Formally put, given a direct projection function $P : \mathbb{R}^n \rightarrow \mathbb{R}^q$, with $q \ll n$ typically, the inverse projection is just the inverse of that function, \emph{i.e.}, a function P^{-1} : \mathbb{R}^q \rightarrow \mathbb{R}^n so that $P^{-1}(P(\mathbf{x})) = \mathbf{x}$ for any $\mathbf{x} \in \mathbb{R}^n$. Unfortunately, computing such an exact inverse function is not possible for most of the existing projection algorithms $P$ for one or several of the following reasons:

: \mathbb{R}^q \rightarrow \mathbb{R}^n so that $P^{-1}(P(\mathbf{x})) = \mathbf{x}$ for any $\mathbf{x} \in \mathbb{R}^n$. Unfortunately, computing such an exact inverse function is not possible for most of the existing projection algorithms $P$ for one or several of the following reasons: \item \emph{non-injectivity}: Typical projection functions $P$ are not injective -- that is, they can map different points in $\mathbb{R}^n$ to the same location in $\mathbb{R}^q$. This is a direct, and likely unavoidable in general, consequence of the fact that $q \ll n$;

Typical projection functions $P$ are not injective -- that is, they can map different points in $\mathbb{R}^n$ to the same location in $\mathbb{R}^q$. This is a direct, and likely unavoidable in general, consequence of the fact that $q \ll n$; \item \emph{non-parametric nature:} While we are talking about $P$ as being a \emph{function} between two spaces ($\mathbb{R}^n$ and $\mathbb{R}^q$), many projection algorithms do not work in this fashion. Rather, they map a given \emph{sampling}, or dataset, $D \subset \mathbb{R}^n$ to another dataset $P(D) \subset \mathbb{R}^q$ (see Eqn.~\ref{eqn:projdef}). When the dataset $D$ changes, the mapping can change as well -- that is, the same point $\mathbf{x} \in \mathbb{R}^n$ can be mapped to different locations in $\mathbb{R}^q$, depending on which other points it comes along with in the new $D$. This is precisely the lack of OOS ability discussed in Sec.~\ref{sec:nnp} and illustrated, for t-SNE, in Fig.~\ref{fig:sensitivity}. Only a (small) subset of projection techniques are \emph{parametric}, \emph{i.e.}, comply with the functional definition given above\,\citep{ptsne,jolliffe02};

along with in the new $D$. This is precisely the lack of OOS ability discussed in Sec.~\ref{sec:nnp} and illustrated, for t-SNE, in Fig.~\ref{fig:sensitivity}. Only a (small) subset of projection techniques are \emph{parametric}, \emph{i.e.}, comply with the functional definition given above\,\citep{ptsne,jolliffe02}; \item \emph{inverse problem:} Even for cases when an algorithmic functional definition of $P$ is available and $P$ is injective, computing its inverse can be quite challenging since inverse problems do not always have guaranteed unique and/or stable solutions. To address the above three problems, the practical approach to computing $P^{-1}$ is by various forms of approximation, which share one or several of the following characteristics: \item \emph{non-injectivity:} This aspect is usually neglected by practical algorithms that compute $P^{-1}$. That is, if two (or more) points $\mathbf{x}_i \in \mathbb{R}^n$ map to the same location $\mathbf{p} \in \mathbb{R}^q$, $P^{-1}(\mathbf{p})$ will yield a single value in $\mathbb{R}^n$;

characteristics: \item \emph{non-injectivity:} This aspect is usually neglected by practical algorithms that compute $P^{-1}$. That is, if two (or more) points $\mathbf{x}_i \in \mathbb{R}^n$ map to the same location $\mathbf{p} \in \mathbb{R}^q$, $P^{-1}(\mathbf{p})$ will yield a single value in $\mathbb{R}^n$; \item \emph{non-parametric nature:} Since most projections $P$ are of this nature, inverse projections are usually defined in terms of a given dataset $D \subset \mathbb{R}^n$. For such a dataset, an inverse projection is a function that aims to yield $P^{-1}(P(\mathbf{x})) \approx \mathbf{x}$ for all $\mathbf{x} \in D$. However, inverse projections \emph{need} to be parametric themselves, \emph{i.e.}, applicable to any other values $\mathbf{p} \in \mathbb{R}^q$ apart from $P(D)$, otherwise they would not be useful for the tasks mentioned earlier in this chapter -- in particular, for the construction of decision maps. For such `unseen' points $\mathbf{p}$, $P^{-1}$ is usually defined to work in a \emph{smooth} manner -- that is, the closest $\mathbf{p}$ is to a known $P(\mathbf{x})$ value, the closest should $P^{-1}(\mathbf{p})$ be to $\mathbf{x}$.

for the construction of decision maps. For such `unseen' points $\mathbf{p}$, $P^{-1}$ is usually defined to work in a \emph{smooth} manner -- that is, the closest $\mathbf{p}$ is to a known $P(\mathbf{x})$ value, the closest should $P^{-1}(\mathbf{p})$ be to $\mathbf{x}$. \item \emph{inverse problem:} Computing $P^{-1}$ is usually done by minimizing suitably-chosen (convex) error functions that model the goal $P^{-1}(P(\mathbf{x})) \approx \mathbf{x}$ for all $\mathbf{x} \in D$ mentioned above. This simplifies and accelerates the computation problem by using existing numerical optimization methods. \subsubsection{Early methods for computing inverse projections} Likely one of the earliest methods for computing inverse projections is by training an autoencoder to jointly perform $P$ and $P^{-1}$\,\citep{hinton2006reducing}. While this method is simple to implement and fast to compute, it does not allow inverting any user-chosen direct projection $P$. Moreover, projections created by autoencoders are of lower quality than other state-of-the-art techniques (see Fig.~\ref{fig:nnp} and related text and also the evaluation in\,\citep{espadoto19}).

simple to implement and fast to compute, it does not allow inverting any user-chosen direct projection $P$. Moreover, projections created by autoencoders are of lower quality than other state-of-the-art techniques (see Fig.~\ref{fig:nnp} and related text and also the evaluation in\,\citep{espadoto19}). \citet{mamani13} presented a method that uses inverse projections to transform the high-dimensional data space based on manipulations of the 2D projection space. This allows users to execute several potentially complex operations that affect the invisible high-dimensional data based on a simple interface that allows direct manipulation of 2D projection points. However, the proposed method does not directly allow inversely projecting new points -- that is, 2D points which are not the direct projection of existing data points. iLAMP\,\citep{Amorim2012ilamp} uses local information in $P(D)$ to build affine transformations that map $\mathbb{R}^2$ to $\mathbb{R}^n$. Although iLAMP was proposed to reverse the LAMP projection technique\,\citep{lamp}, it can be used to reverse other projections $P$ with reasonable results\,\citep{rodrigues2018Imagebasedvisualization,espadoto2019NNinv}. The same authors next proposed an inverse projection method using Radial Basis Functions (RBFs) to gain continuity and global behavior.

was proposed to reverse the LAMP projection technique\,\citep{lamp}, it can be used to reverse other projections $P$ with reasonable results\,\citep{rodrigues2018Imagebasedvisualization,espadoto2019NNinv}. The same authors next proposed an inverse projection method using Radial Basis Functions (RBFs) to gain continuity and global behavior. \citet{schulz2015Usingdiscriminative,schulz2020DeepViewVisualizing} proposed DeepView to compute inverse projections using UMAP\,\citep{UMAP2018} as direct projection $P$. In contrast to UMAP, however, similarities of points are computed by combining their high-dimensional attributes with the outcome of a classifier $f$ (similar to SSNP). The inverse projection is then computed also by UMAP from the projection of the given dataset $P(D)$ and then extrapolated to the entire 2D space by minimizing the Kullback-Leibler divergence (similar to t-SNE). DeepView yields quite smooth results (see its application for DBM construction in Fig.~\ref{fig:dbm}a) but is over an order of magnitude slower than all other $P^{-1}$ techniques described here.

the entire 2D space by minimizing the Kullback-Leibler divergence (similar to t-SNE). DeepView yields quite smooth results (see its application for DBM construction in Fig.~\ref{fig:dbm}a) but is over an order of magnitude slower than all other $P^{-1}$ techniques described here. More recently, \citet{invertingMDS2024EuroVA} proposed iMDS to invert MDS projections using multilateration with randomly selected samples to estimate the inverse projection. While this method is simple to implement, it only gives good results for datasets having a quite low intrinsic dimensionality (under 10) and can only invert MDS. Although this method is quite recent, we include it in this section which discusses early inverse projections since it is, fundamentally, sharing the same characteristics as all other methods which do not use the more recent deep-learning approach which are discussed in the next section. \subsubsection{Deep learning inverse projections}

in this section which discusses early inverse projections since it is, fundamentally, sharing the same characteristics as all other methods which do not use the more recent deep-learning approach which are discussed in the next section. \subsubsection{Deep learning inverse projections} Following the success of NNP for direct projections (Sec.~\ref{sec:nnp}), \citet{espadoto2019NNinv} computed inverse projections by simply `switching' the input and output of NNP, \emph{i.e.}, given a dataset $D$ that projects to a 2D scatterplot by some technique $P$, train a regressor to output $D$ when given $P(D)$. This technique, called NNInv inherits all the desirable properties of NNP (Sec.~\ref{sec:nnp}) and also produces higher-quality inverse projections than autoencoders and iLAMP. The usage of NNInv is illustrated in the DBM construction in Fig.~\ref{fig:dbm}b. Further variations of this design include SSNP\,\citep{espadoto2021SSNP} (used to construct the DBM in Fig.~\ref{fig:dbm}c and SHaRP\,\citep{ShaRP2023}). Both techniques use an autoencoder basis so produce both a direct and inverse projection (see also Sec.~\ref{sec:nnp}). However, their quality is higher than plain autoencoders and also than plain NNInv given their (self-)supervised operation based on class (pseudo)labels.

and SHaRP\,\citep{ShaRP2023}). Both techniques use an autoencoder basis so produce both a direct and inverse projection (see also Sec.~\ref{sec:nnp}). However, their quality is higher than plain autoencoders and also than plain NNInv given their (self-)supervised operation based on class (pseudo)labels. Figure~\ref{fig:sharp_dbm} illustrates this by comparing NNinv (first row) with SHaRP (second and third rows) for the construction of a decision map for a simple k-nearest neighbors (KNN) classifier ($k=21$) for four datasets of varying dimensionalities $n$. This is a novel insight as SHaRP has, so far, not been gauged on its performance for computing decision maps. We see that, similarly to SSNP (shown in Fig.~\ref{fig:dbm}c), SHaRP produces decision zones with smoother boundaries than NNInv, which are closer to the known ground-truth smooth boundaries (hyperplanes) that a KNN classifier should have. \caption{Comparison of decision maps constructed by NNinv\,\citep{espadoto2019NNinv} (top row) and SHaRP\,\citep{ShaRP2023} (middle row: plain map; bottom row: map with classifier confidence encoded into color saturation) inverse-projection techniques, for a KNN classifier and four datasets ($n$ indicates the dataset dimensionalities). See Sec.~\ref{sec:nninv}.} \subsubsection{Applications of inverse projections}

by NNinv\,\citep{espadoto2019NNinv} (top row) and SHaRP\,\citep{ShaRP2023} (middle row: plain map; bottom row: map with classifier confidence encoded into color saturation) inverse-projection techniques, for a KNN classifier and four datasets ($n$ indicates the dataset dimensionalities). See Sec.~\ref{sec:nninv}.} \subsubsection{Applications of inverse projections} NNInv was further explored in detail for visual analytics scenarios involving dynamic imputation and exploring ensemble classifiers\,\citep{espadoto2021unprojection}. Figure~\ref{fig:class_aggr} shows the latter use-case: In the image, each pixel is backprojected and ran through a set of nine classifiers, trained to separate classes 1 and 7 from the MNIST dataset. The pixel is then colored to indicate the classifiers' agreement. Deep blue, respectively red, zones show areas where all classifiers agree with class 1, respectively 7. Brighter areas indicate regions of high classifier disagreement -- which are thus highly difficult to decide upon and are prime candidates for ML engineering, regardless of the used classifier.

show areas where all classifiers agree with class 1, respectively 7. Brighter areas indicate regions of high classifier disagreement -- which are thus highly difficult to decide upon and are prime candidates for ML engineering, regardless of the used classifier. \caption{Classifier agreement map for 9 classifiers, two-class problem (MNIST datasets digits 1 and 7). Dark colors indicate more of the 9 classifiers agreeing, at a pixel in the map, with their decisions (red=1, blue=7). Brighter, desaturated, colors indicate fewer classifiers in agreement (white=four classifiers output 1, the other five output 7, or conversely). Image taken from\,\citep{espadoto2021unprojection}. See Sec.~\ref{sec:dbm_ch2}.} \section{Future exploitations of the ML-DR connection} Reflecting upon the current achievements of using ML for DR and conversely, we see a bright future ahead for research where the two directions assist each other. We illustrate this with a few selected, non-exhaustive, examples of such potential ML-DR synergies. Moreover, we connect these examples to concrete cases where we leverage this synergy in our own work on decision maps in the next chapters. \subsection{Prospects of DR assisting ML: Seeing to learn better} \yu{This gray part is elaborated in Chapter 5. where we work on intrinsic dimensionality estimation. We can remove it here.}

in our own work on decision maps in the next chapters. \subsection{Prospects of DR assisting ML: Seeing to learn better} \yu{This gray part is elaborated in Chapter 5. where we work on intrinsic dimensionality estimation. We can remove it here.} \textbf{Better DBMs:} One recent, and unexpected, result of our analysis is that all DBM methods only visualize a surface-like subset of the high-dimensional data space (see the coverage discussion in Sec.~\ref{sec:dbm_ch2}). This tells us that not only a small, surface-like, subset of a classifier's behavior is thus visualized, but also that how this subset is selected is not under the user's control, but automatically determined by the inverse projection method being used by the DBM algorithm. As such, the insights users will get from DBM methods applied to various classifiers and datasets will be highly dependent on the DBM technique in use. Even more critically, imagine a given, trained, classifier $f$ whose input space is sampled by different test datasets $D$, each being used next to construct a decision map image. Such images will likely be (very) different since they depend on inverse projections constructed, in turn, from different datasets. This can be highly misleading given that we are visualizing the same, fixed, classifier $f$.

to construct a decision map image. Such images will likely be (very) different since they depend on inverse projections constructed, in turn, from different datasets. This can be highly misleading given that we are visualizing the same, fixed, classifier $f$. It is likely impossible to devise a DBM method that densely samples an \emph{entire} high-dimensional space to construct a classifier image. As such, we see two ways forward to improving DBMs: \item construct an inverse projection with predictable, guaranteed, \emph{behavior} for a given sampling $D$ of the data space; \item allow users to \emph{control} the surface that is backprojected by the inverse projection to sample the classifier; Option 1 relates to how the aforementioned backprojected surfaces are constructed by the function $P^{-1}$. All designs of inverse projections essentially minimize a cost which tells that points of a given projection $P(\mathbf{x})$ have to bakproject, via $P^{-1}$, one to one, to those of a given dataset $D = \{\mathbf{x}\}$\,\citep{Amorim2012ilamp,amorim2015Facinghighdimensions,espadoto2019NNinv,espadoto2021SSNP,ShaRP2023,schulz2015Usingdiscriminative,schulz2020DeepViewVisualizing}. Hence, such backprojected surfaces can be thought of as level sets, or isosurfaces, of low values of the cost function. Analyzing them from this perspective can lead to important theoretical insights in their behavior.

a given dataset $D = \{\mathbf{x}\}$\,\citep{Amorim2012ilamp,amorim2015Facinghighdimensions,espadoto2019NNinv,espadoto2021SSNP,ShaRP2023,schulz2015Usingdiscriminative,schulz2020DeepViewVisualizing}. Hence, such backprojected surfaces can be thought of as level sets, or isosurfaces, of low values of the cost function. Analyzing them from this perspective can lead to important theoretical insights in their behavior. Option 2 offers a more practical way forward. Simple ways can be devised to allow users to \emph{e.g.} shift this surface in given directions and/or by given amounts in the data space by means of interactive controls applied to the 2D image space. Similar ideas have been since long used, albeit in a different context, for the visualization of scalar functions of many variables\,\citep{hyperslice}. The difference, in our case, is that we would start with a more complex surface, and would have to design intuitive ways to shift this surface in meaningful directions in the data space. This idea could be further extended by allowing for sampling a thick `band' close to this surface. The challenge, in this case, would be to map this band to the 2D image space of a decision map. }

data space. This idea could be further extended by allowing for sampling a thick `band' close to this surface. The challenge, in this case, would be to map this band to the 2D image space of a decision map. } For researchers in various fields, DBMs can provide insights into the behavior of classifiers in their respective applications, thereby helping them to understand and interpret the results of their ML models. For machine learning engineers, they offer a visual and informative way to refine models. Apart from the scenarios depicted in \citep{espadoto2021unprojection,Amorim2012ilamp,amorim2015Facinghighdimensions}, DBMs could be readily used in a visual analytics explorative scenario to drive a classifier's training. If computable in real-time, users could visualize the DBMs, find problematic areas with respect to how the decision boundaries wrap around samples, and next modify the training set by \emph{e.g.} adding or deleting labels, adding new augmented samples, or even moving samples. We envisage a tool in which users could effectively `sculpt' the shape of decision boundaries by sample manipulation much as one edits 2D shapes by manipulating spline control points. This would offer unprecedented freedom and a wholly new way of fine-tuning classifiers to extend the approaches pioneered in\,\citet{benato2018SemiSupervisedLearning,benato2021SemiAutomaticData}.

effectively `sculpt' the shape of decision boundaries by sample manipulation much as one edits 2D shapes by manipulating spline control points. This would offer unprecedented freedom and a wholly new way of fine-tuning classifiers to extend the approaches pioneered in\,\citet{benato2018SemiSupervisedLearning,benato2021SemiAutomaticData}. This thesis contributes to this topic from two aspects: First, \Cref{ch3:geo_application} presents an application of decision maps to understand classification models constructed for geoscience research, which demonstrates how decision maps can be used to help research in other fields in practice. Secondly, \Cref{ch7:fastDBM} presents a novel method to accelerate the construction of decision maps (and other maps related to inverse projections). Our method can compute such maps interactive rates, which is a prerequisite for the interactive exploration mentioned above. An early version of our acceleration technique has been used in a practical application, namely for pseudo-labeling high dimensional datasets to improve classification performance\,\citep{benato2024HumanloopUsing}. All visualizations examples shown in this chapter have covered only the depiction of classifiers that output a single categorical value.

our acceleration technique has been used in a practical application, namely for pseudo-labeling high dimensional datasets to improve classification performance\,\citep{benato2024HumanloopUsing}. All visualizations examples shown in this chapter have covered only the depiction of classifiers that output a single categorical value. However, as Sec.~\ref{sec:background_ch2} mentions, ML also studies multi-valued classifiers and, further, single-valued and multi-valued regressors. Concerning decision maps, we are not aware of their extension to multi-valued classifiers. This could be achieved by using multiple-view maps, one per classifier output, or categorical color-coding of all multi-valued class combinations in a single decision map. Concerning regressors, recent results have shown how to extend the decision map metaphor to visualize single-valued regressors\,\citep{espadoto2021OptMapUsing,regsurf}. However, this research only used a relatively low-quality projection (PCA), so it could be readily explored how better direct and inverse projections, like the ones described in Secs.~\ref{sec:nnp} and~\ref{sec:nninv} could improve its results. Visualizing multi-valued regressors is a harder problem as several continuous values would need to be displayed at each pixel. To assist this, techniques developed earlier in scientific visualization, such as tensor visualization\,\citep{tensorvis}, could offer an outcome. We leave this challenge of visualizing general regressors open in our work in this thesis.

would need to be displayed at each pixel. To assist this, techniques developed earlier in scientific visualization, such as tensor visualization\,\citep{tensorvis}, could offer an outcome. We leave this challenge of visualizing general regressors open in our work in this thesis. \subsection{Prospects of ML assisting DR: Learning to see better} While many metrics exist to gauge the quality of direct projections (Sec.~\ref{sec:background_ch2}), there are no established ways to measure the quality of an inverse projection, apart from the simple mean-square-error (MSE) $\sum_{\mathbf{x} \in D} \| \mathbf{x} - P(P^{-1}(\mathbf{x})) \|$\,\citep{espadoto2019NNinv}. This is not surprising since, as explained in Sec.~\ref{sec:nninv}, inverse projections are mainly used to infer, or hypothesize, what the data would be in locations where no ground-truth is present. As such, defining what a good inverse projection should return in such areas is conceptually hard. Yet, possibilities exist. One can \emph{e.g.} use a ML approach where an unseen test set is kept apart from the construction of the inverse projection and is used to assess the quality of such a trained model using the aforementioned MSE.

possibilities exist. One can \emph{e.g.} use a ML approach where an unseen test set is kept apart from the construction of the inverse projection and is used to assess the quality of such a trained model using the aforementioned MSE. An equally interesting question is how to design a scale, or hierarchy, of errors. It is likely that differently inversely-projected points $\mathbf{x}' = P^{-1}(P(\mathbf{x}))$ that deviate from its ground-truth location $\mathbf{x}$ by the same distance $\|\mathbf{x}' - \mathbf{x}\|$ are not equally good, or equally bad, depending on the application perspective. As such, inverse projection quality metrics may need to be designed in an application-specific way. Similarly to direct projections\,\citep{espadoto19}, the quality of inverse projections can be measured not only globally (by a single aggregate metric) but also locally, at every pixel. The explanatory visualizations in Fig.~\ref{fig:dbm_interp} can be thought as being such per-pixel quality maps (for classifiers).

direct projections\,\citep{espadoto19}, the quality of inverse projections can be measured not only globally (by a single aggregate metric) but also locally, at every pixel. The explanatory visualizations in Fig.~\ref{fig:dbm_interp} can be thought as being such per-pixel quality maps (for classifiers). For inverse projections, we are aware of a single such per-pixel quality visualization -- gradient maps\,\citep{espadoto2021unprojection}. Figure~\ref{fig:gradmap}a shows this gradient map, which depicts the gradient magnitude of the $P^{-1}$ function (in this case constructed with NNInv) at every pixel. Hot, respectively dark, regions in the map indicate nearby 2D points which backproject far away from, respectively close to, each other. Points in the hot regions thus indicate areas where the inverse projection may be unstable, and as such, potentially create misleading data.

regions in the map indicate nearby 2D points which backproject far away from, respectively close to, each other. Points in the hot regions thus indicate areas where the inverse projection may be unstable, and as such, potentially create misleading data. However, one cannot directly say that this is an error of the inverse projection $P^{-1}$. Such regions may correspond to areas where the \emph{direct} projection $P$ squeezed faraway data points to fit them in the 2D space -- thus areas of low continuity\,\citep{venna2006visualizing}. Hence, analyzing inverse projection errors should go hand-in-hand with analyzing the errors of the direct projection it was computed for. For the latter, many per-pixel techniques are readily usable\,\citep{aupetit07,lespinats11,martins14}. We will further use gradient maps in evaluating our results in Chapters 4 and 6.

errors should go hand-in-hand with analyzing the errors of the direct projection it was computed for. For the latter, many per-pixel techniques are readily usable\,\citep{aupetit07,lespinats11,martins14}. We will further use gradient maps in evaluating our results in Chapters 4 and 6. \caption{(a) Gradient map of NNInv inverse projection constructed from a t-SNE projection of an uniformly sampled sphere. Hot, respectively dark, regions indicate nearby 2D points that inversely project to far-apart, respectively close, $n$D points (green line, top sphere; orange line, bottom sphere, respectively). (b) Gradient map of NNInv inverse projection used to construct the decision maps for the MNIST classification in Fig.~\ref{fig:dbm_interp}, with distance-to-closest-boundary map at the top (grayscale). (c) Two regions of large, respectively low, gradients are sampled by the red, respectively green, points. The corresponding images generated by NNInv are shown and confirm the large, respectively low, variations of the inverse projection in these areas. See Sec.~\ref{sec:ml_for_dr}. }

Two regions of large, respectively low, gradients are sampled by the red, respectively green, points. The corresponding images generated by NNInv are shown and confirm the large, respectively low, variations of the inverse projection in these areas. See Sec.~\ref{sec:ml_for_dr}. } Figure~\ref{fig:gradmap}b shows an additional use-case for gradient maps. The image depicts the gradient map of the NNInv inverse-projection method used to construct the decision map visualizations for the MNIST classifier explored in Fig.~\ref{fig:dbm_interp}. Atop of this gradient map, we overlaid the classifier confidence (Fig.~\ref{fig:dbm_interp}d), so the dark bands in the image correspond to the classifier's decision boundaries. For clarity of exposition, we show atop image (b) the distance-to-closest-boundary, \emph{i.e.}., the same information as encoded in the luminance in Fig.~\ref{fig:dbm_interp}e. Image (b) gives us several insights. We see that large inverse-projection gradients occur both along decision boundaries but also deep inside the decision zones. Also, these large gradients are not correlated with areas of low, or high, distance-to-closest boundary. Hence, the gradient map tells additional information not present in earlier visualizations. This information helps seeing where a classifier will be exposed to high data variability, thus, meet more challenges. We show this by taking five points ($A \ldots E$) in a low-gradient, and five others ($F \ldots J$) in a high-gradient area, respectively. Fig.~\ref{fig:gradmap}c shows the MNIST images corresponding to these points. Indeed, we see how the respective digits vary significantly more in high-gradient areas than in low-gradient ones.

in a low-gradient, and five others ($F \ldots J$) in a high-gradient area, respectively. Fig.~\ref{fig:gradmap}c shows the MNIST images corresponding to these points. Indeed, we see how the respective digits vary significantly more in high-gradient areas than in low-gradient ones. Another aspect related to inverse projection quality is the \emph{coverage} of an inverse projection function. Simply put: Consider an image space $I = \{ \mathbf{p} \}$, $I \subset \mathbb{R}^2$ as a set of pixels $\mathbf{p}$, and an inverse projection function $P^{-1} : \mathbb{R}^2 \rightarrow \mathbb{R}^n$. Let $I^{-1} = \{ P^{-1}(\mathbf{p}) | \mathbf{p} \in I \}$ be the mapping of the entire pixel image $I$ to the data space via $P^{-1}$. Current decision maps only `sample' the data space by examining $I^{-1}$. The question is: How well does $I^{-1}$ cover, or represent, the entire data space $Z$ on which some machine-learning model is supposed to work? We will describe a comprehensive quantitative and qualitative evaluation of the decision maps in \Cref{ch4:metrics_decision_map} which addresses the inverse projection quality aspect discussed in this section. Separately, we will further investigate the coverage of inverse projections in \Cref{ch5:surface_like_behavior}.

to work? We will describe a comprehensive quantitative and qualitative evaluation of the decision maps in \Cref{ch4:metrics_decision_map} which addresses the inverse projection quality aspect discussed in this section. Separately, we will further investigate the coverage of inverse projections in \Cref{ch5:surface_like_behavior}. Clearly, both direct and inverse projection operations can be defined, and applied, by only having an input present in the form of a high-dimensional dataset, respectively a 2D scatterplot. However, in many cases, this lack of controlling how the direct and/or inverse projection actually work can lead to suboptimal results. Typical ways to address this are of offer various hyperparameters to control the direct and/or inverse projection\,\citep{appleby2021HyperNPInteractive}. However, controlling such hyperparameters is not always easy or intuitive for actual users\,\citep{wattenberg2016use}. Several other mechanisms for control exist and could be further exploited, as follows.

address this are of offer various hyperparameters to control the direct and/or inverse projection\,\citep{appleby2021HyperNPInteractive}. However, controlling such hyperparameters is not always easy or intuitive for actual users\,\citep{wattenberg2016use}. Several other mechanisms for control exist and could be further exploited, as follows. All projection methods aim to encode the relative distance between data points in their resulting scatterplot. Atop of this, parametric projections aim to encode the actual data values. SHaRP extends this to force data clusters to specific shapes (Sec.~\ref{sec:nnp}). Such strategies could be extended to map other data attributes, such as sample density or specific value ranges, to the size, shape, and/or position of point clusters in a projection. For DL methods, this could be done by refining their loss function. Additionally, SHaRP could be extended to create a hierarchy-aware projection algorithm that would combine the advantages of treemaps and classical projections, extending earlier ideas in this class\,\citep{nmap}. All in all, projection techniques can be extended to take more properties of the input data into account when computing the output 2D scatterplot than pairwise point distances. We will not explore this direction in our work.

in this class\,\citep{nmap}. All in all, projection techniques can be extended to take more properties of the input data into account when computing the output 2D scatterplot than pairwise point distances. We will not explore this direction in our work. A second extension would be to design \emph{local} cost functions that attempt to construct the projection by combining different criteria for different subsets of the input data -- for example, to achieve a globally-better projection that locally behaves like t-SNE in some areas and like UMAP in others. ML techniques can help here by \emph{e.g.} extending the HyperNP idea\,\citep{appleby2021HyperNPInteractive} to train from a set of projection techniques run on the same input dataset. Further inspiration can be gotten from recent ways in which DL is used for image synthesis and style transfer, \emph{e.g.}, \citet{deeptransfer}. We will also not explore this direction further.

a set of projection techniques run on the same input dataset. Further inspiration can be gotten from recent ways in which DL is used for image synthesis and style transfer, \emph{e.g.}, \citet{deeptransfer}. We will also not explore this direction further. Finally, one can control the way that inverse projections work. Indeed, as mentioned in Sec.~\ref{sec:inv_quality}, current inverse projections simply map a given 2D image, discretized as a set of pixels, to the high-dimensional data space, without any user control. It is very likely that this simple construction will not be able to cover all the high-dimensional data space equally well. As such, it is interesting to consider ways to control how the inverse projection samples this data space so that regions of higher interest, for any given application, are offered a higher chance to show up in the 2D representation -- for example, in a decision map. We will present such a mechanism that allows users to control the inverse projection intuitively and interactively in \Cref{ch6:controllable_Pinv}, with the help of deep learning.

chance to show up in the 2D representation -- for example, in a decision map. We will present such a mechanism that allows users to control the inverse projection intuitively and interactively in \Cref{ch6:controllable_Pinv}, with the help of deep learning. Section~\ref{sec:understanding_dl} has briefly introduced dynamic projections. These are extensions of the standard, static, projection techniques which aim to handle a dataset consisting of high-dimensional points which maintain their identity while changing their attribute values through time. Dynamic projections have a wealth of applications -- simply put, anywhere one wants to study high-dimensional data which changes over time. However, only a handful of dynamic projection techniques exist\,\citep{rauber_dtsne,vernier21,vernier20,xtreaming}, and their quality -- as gauged by established quality metrics -- is good in data structure preservation \emph{or} data dynamics preservation but not both aspects. Designing a dynamic projection technique that accurately maps both data structure and dynamics is a grand challenge for the infovis community. Following the good

metrics -- is good in data structure preservation \emph{or} data dynamics preservation but not both aspects. Designing a dynamic projection technique that accurately maps both data structure and dynamics is a grand challenge for the infovis community. Following the good results of using ML for DR (Sec.~\ref{sec:dr_survey}), it looks highly interesting to explore ML (and in particular DL) to create dynamic projections. An issue here is that, since good ground-truth dynamic projections are relatively hard to construct, the supervised way (NNP-class methods) may be less preferable than the self-supervised (SSNP-like) direction. We leave this challenge open in our work in this thesis.

An issue here is that, since good ground-truth dynamic projections are relatively hard to construct, the supervised way (NNP-class methods) may be less preferable than the self-supervised (SSNP-like) direction. We leave this challenge open in our work in this thesis. In this chapter, we have presented an overview of recent connections between the two fields, with a focus on techniques and methods in one field which assist tasks and use-cases in the other, and also satisfy overal desirable criteria as genericity, computational scalability, stability, and ease of use. We have made the case that the two fields are complementary, with key features being offered by methods in one field being required by methods in the other, therefore the potential for cross-fertilization. The first part of our overview (Sec.~\ref{sec:ml_survey}) showed how DR can assist ML tasks by examples in assessing the behavior of general-purpose classifiers, pseudolabeling for creating large training-sets, exploring the training and inference of deep learning models, and depicting the high-dimensional decision zones and boundaries of classifiers. The second part (Sec.~\ref{sec:dr_survey}) showed how ML can assist DR by examples covering the deep learning of projections and inverse projections.

exploring the training and inference of deep learning models, and depicting the high-dimensional decision zones and boundaries of classifiers. The second part (Sec.~\ref{sec:dr_survey}) showed how ML can assist DR by examples covering the deep learning of projections and inverse projections. In the third part (Sec.~\ref{sec:forward}), we have outlined several high-potential research directions at the crossroads of ML and DR based on the techniques discussed in this chapter, and we have shown which of these directions we will further explore in the context of this thesis. Decision maps are an important tool of the use-case where DR can assist ML as they help exploring the behavior of a trained classifier. Conversely, decision maps rely on direct and inverse projections which can be improved in several ways by using ML. In the remainder of this thesis, we will explore both these directions and propose improvements for both use-cases.

a trained classifier. Conversely, decision maps rely on direct and inverse projections which can be improved in several ways by using ML. In the remainder of this thesis, we will explore both these directions and propose improvements for both use-cases. To date, decision maps have not been adapted or used to actually assist in solving scientific problems. As \citet{oliveiraSDBM2022} remark, studies are needed to show how users actually interpret such maps to extract information on the visualized classification problems. In this chapter, we fill this gap by presenting a case study where we apply decision maps to a geological problem -- the classification of mineral deposit genetic environments. We show how decision maps can be used to interpret the machine learning classification results and how they can be used to guide the exploration of the data in a \emph{practical} setting. This chapter contributes indirectly to our research questions \textbf{RQ1} -- \textbf{RQ4} listed in Chapter~\ref{ch1:intro} by (1) making the case that decision maps are indeed useful instruments in classifier engineering in a real-world setting; and (2) outlining several limitations of decision maps which we discovered in this setting, which further justify our choices for exploring \textbf{RQ1} -- \textbf{RQ4} in the next chapters\,\footnote{This chapter is based on the paper: ``Interpreting mineral deposit genesis classification with decision maps: a case study using pyrite trace elements'' \citep{wang2024AMpyriteSDBM}.}.

discovered in this setting, which further justify our choices for exploring \textbf{RQ1} -- \textbf{RQ4} in the next chapters\,\footnote{This chapter is based on the paper: ``Interpreting mineral deposit genesis classification with decision maps: a case study using pyrite trace elements'' \citep{wang2024AMpyriteSDBM}.}. The accelerating pace of data generation and computational power, coupled with the burgeoning interest of geoscientists in machine learning, is leading to significant breakthroughs in the applications and discoveries in geosciences\,\citep{petrelli2016Solvingpetrological,bergenMachineLearningDatadriven2019,karpatne2019MachineLearning,petrelli2021IntroductionPython}. The data-driven study in geosciences essentially aims at digging deep information from complex/huge data sets, rather than merely and simply producing classification or prediction models. The `black box' nature of machine models, however, hinders our understanding of decision-making processes during machine learning\,\citep{lipton2018MythosModel,carvalho2019Machinelearning,molnar2020InterpretableMachine}. Although pioneering explorations on the transparency of the working pathway of machine learning have emphasized the significance of the interpretability machine learning model\,\citep{lipton2018MythosModel,carvalho2019Machinelearning,molnar2020InterpretableMachine,yuan2021surveyvisual}, such work is lacking in the classification of mineral deposit genetic environments. Understanding the mineral deposit genetic environments is important to explore the physio-chemical conditions that are responsible for the ore formation\,\citep{rusk2012CathodoluminescentTextures}. To improve the precision of the ore deposit classification environment, with a transparent and interpretable machine learning approach, we apply decision maps.

mineral deposit genetic environments is important to explore the physio-chemical conditions that are responsible for the ore formation\,\citep{rusk2012CathodoluminescentTextures}. To improve the precision of the ore deposit classification environment, with a transparent and interpretable machine learning approach, we apply decision maps. We underscore the potential of decision maps within a fundamental geological domain: the genesis of mineral deposits. The dwindling supply of near-surface ore deposits necessitates deeper exploration\,\citep{gregory2019DistinguishingOre}. The ability to recognize the type of mineralization present in a given context can offer critical insights, thus streamlining exploration efforts and minimizing associated costs\,\citep{gregory2019DistinguishingOre}. Trace elements measured in specific minerals, such as quartz, pyrite, apatite, and zircon, can serve as unique identifiers for understanding their genesis, revealing types of minerals deposits and host rock genetic environments\,\citep{belousova2002Igneouszircon,rusk2012CathodoluminescentTextures,osullivan2020traceelement,wang2021MachineLearning,zhong2021Revealingmultistage,zhu2022Machinelearningbased,zhou2023apatiteSDBM}. \caption{Discriminant diagrams for minerals using their trace elements concentrations (a) Ti versus Al diagrams for quartz\,\citep{rusk2012CathodoluminescentTextures}. (b) Sr versus Y diagram for apatite\,\citep{belousova2002Apatiteindicator}.}

as unique identifiers for understanding their genesis, revealing types of minerals deposits and host rock genetic environments\,\citep{belousova2002Igneouszircon,rusk2012CathodoluminescentTextures,osullivan2020traceelement,wang2021MachineLearning,zhong2021Revealingmultistage,zhu2022Machinelearningbased,zhou2023apatiteSDBM}. \caption{Discriminant diagrams for minerals using their trace elements concentrations (a) Ti versus Al diagrams for quartz\,\citep{rusk2012CathodoluminescentTextures}. (b) Sr versus Y diagram for apatite\,\citep{belousova2002Apatiteindicator}.} Classification of mineral deposits environments has traditionally been studied using visual tools, including discriminant diagrams (Fig~\ref{fig:diagrams})\,\citep{pearce1973Tectonicsetting,bralia1979revaluationCo,belousova2002Apatiteindicator,rusk2012CathodoluminescentTextures,li2019MagmaticNiCu,breiter2020Chemicalsignature,zhou2022ApatiteEu}, and, more recently, machine learning-assisted approaches\,\citep{belousova2002Apatiteindicator,belousova2002Igneouszircon,petrelli2016Solvingpetrological,gregory2019DistinguishingOre,wang2021MachineLearning,zhong2021Revealingmultistage,liu2023Mineralprospectivity}. However, striking a balance between visual interpretability and accuracy is still a challenge. There have also been some attempts of using machine learning to optimize geochemistry discriminant diagrams\,\citep{osullivan2020traceelement,wang2022QuartzTi}. Such applications improve the quality of the patterns depicted by the diagrams but still do not take full advantage of high-dimensional information. Here, decision maps come to the fore, combining the high-accuracy of machine learning models with visual accessibility to decision boundaries, greatly promoting transparency and interpretability. This study represents the first application of visualization to elucidate machine learning classification in mineral deposit genetic types, highlighting the paramount role of visualization techniques in modern data interpretation and decision-making.

accessibility to decision boundaries, greatly promoting transparency and interpretability. This study represents the first application of visualization to elucidate machine learning classification in mineral deposit genetic types, highlighting the paramount role of visualization techniques in modern data interpretation and decision-making. Here, our contributions straddle both information visualization and mineralogy domains: (1) We offer a unique pyrite trace elements dataset comprising six genetic populations. (2) We illuminate the added value of the decision map technique in deciphering the machine learning classification results, opening up new avenues for using decision maps. (3) We introduce a method that seamlessly blends the merits of traditional 2D discriminant diagrams (visual interpretability) and machine learning methods (high accuracy), providing a robust framework for mineral genesis classification problems. This blend of visualization and machine learning underlines the evolving landscape of data science, championing transparency and interpretability. We divide our discussion of related work into two main topics -- traditional mineral genetic discriminant diagrams (\ref{sec:2.1}) and machine learning for mineral genetic type classification (\ref{sec:2.2}). For each discussed topic, we outline the main advantages and disadvantages, in support of our overall claim of creating a new technique with decision maps that optimizes across existing approaches.

(\ref{sec:2.1}) and machine learning for mineral genetic type classification (\ref{sec:2.2}). For each discussed topic, we outline the main advantages and disadvantages, in support of our overall claim of creating a new technique with decision maps that optimizes across existing approaches. \subsection{Traditional trace element discriminant diagrams} Trace element \emph{discriminant diagrams} were introduced in the 1970s and are still widely used as a tool for identifying the types of deposit/host rock/tectonic setting with which a sample is associated\,\citep{belousova2002Apatiteindicator,belousova2002Igneouszircon,bralia1979revaluationCo,breiter2020Chemicalsignature,pearce1973Tectonicsetting,rusk2012CathodoluminescentTextures}. Discriminant diagrams are basically classical scatterplots that use only a few elements (data dimensions) plotted as binary or ternary diagrams. The axes of the diagrams, \emph{i.e.}, data dimensions to explore, are usually selected based on the geologists' experience. For example, diagrams of Co $vs$ Ni and Au $vs$ As are widely used to discriminate the type of pyrite\,\citep{bajwah1987Traceelement,bralia1979revaluationCo,deditius2014coupledgeochemistry}; Ti, Al, Ge are common axes used in quartz forming environment classification (Fig.~\ref{fig:diagrams}a)\,\citep{breiter2020Chemicalsignature,rusk2012CathodoluminescentTextures}; finally, Sr, Y, Mn and rare earth elements (REE) of apatite are useful axes for recognizing the apatite's host rock type (Fig.~\ref{fig:diagrams}b)\,\citep{belousova2002Apatiteindicator}.

discriminate the type of pyrite\,\citep{bajwah1987Traceelement,bralia1979revaluationCo,deditius2014coupledgeochemistry}; Ti, Al, Ge are common axes used in quartz forming environment classification (Fig.~\ref{fig:diagrams}a)\,\citep{breiter2020Chemicalsignature,rusk2012CathodoluminescentTextures}; finally, Sr, Y, Mn and rare earth elements (REE) of apatite are useful axes for recognizing the apatite's host rock type (Fig.~\ref{fig:diagrams}b)\,\citep{belousova2002Apatiteindicator}. The most important property of discriminant diagrams is that they offer a direct, visual, insight into how the depicted data dimensions relate to each other. The information that geologists can get from such diagrams includes labels of the depicted data samples and, at a higher level, trends, correlations, and outliers of the depicted elements and labels, which next help interpreting the phenomenon captured by the plotted data. In some cases, information -- such as mineralization stages within a deposit, which often involves sequential patterns or transitions -- cannot be easily conveyed to machine learning. Visualizing how such data points scatter in the diagram helps scientists understand data evolution. Based on domain knowledge, scientists can explore and get valuable understanding from such diagrams.

sequential patterns or transitions -- cannot be easily conveyed to machine learning. Visualizing how such data points scatter in the diagram helps scientists understand data evolution. Based on domain knowledge, scientists can explore and get valuable understanding from such diagrams. The key shortcoming of discriminant diagrams is their inability to fully use all high-dimensional information available in the studied dataset. Only two elements (or ratios of a few more elements) can be depicted. These dimensions have to be hand-picked by geologists based on their experience, which means that potentially interesting, but unknown, data patterns present in other dimensions will not be found\,\citep{petrelli2016Solvingpetrological}.In addition, when classifying more complicated cases, the diagrams are usually heavily overlapped\,\citep{rottier2020Traceelement}. There are also some examples of using machine learning to optimize X-Y geochemistry diagrams\,\citep{hu2022OriginDiscrimination,osullivan2020traceelement,wang2022QuartzTi}. Such applications improve the quality of the patterns depicted by the diagrams but still do not take full advantage of high-dimensional information.

are usually heavily overlapped\,\citep{rottier2020Traceelement}. There are also some examples of using machine learning to optimize X-Y geochemistry diagrams\,\citep{hu2022OriginDiscrimination,osullivan2020traceelement,wang2022QuartzTi}. Such applications improve the quality of the patterns depicted by the diagrams but still do not take full advantage of high-dimensional information. Such limitations of low-dimensional diagrams are well-known in information visualization. Several partial solutions have been proposed to address them. Arguably the simplest and most used solution is to plot several diagrams side-by-side, a technique known as \emph{small multiples}\,\citep{tufte1983visualdisplay}. Users can then extract insights involving multiple attributes by visually comparing such diagrams. However, this solution scales poorly with the number of involved dimensions and requires careful ordering of the diagrams in the plotted sequence for efficient visual inspection. Another class of methods generically known as \emph{scagnostics}\,\citep{wilkinson2005Graphtheoreticscagnostics} computes all possible diagrams from a given set of data dimensions and next selects  ``interesting'' diagrams to be shown to the user based on predefined data patterns. In contrast to small multiples, this selection scales visually well with the number of dimensions. However, computing and analyzing all possible diagrams is computationally very demanding. More importantly, defining and detecting what ``interesting''  patterns are is a challenging problem.

patterns. In contrast to small multiples, this selection scales visually well with the number of dimensions. However, computing and analyzing all possible diagrams is computationally very demanding. More importantly, defining and detecting what ``interesting'' patterns are is a challenging problem. Discriminant diagrams have an additional important limitation: As they only plot a \emph{sparse} set of observations (samples), users have to mentally group these visually into clusters and decide by themselves where actual ``boundaries'' exist that separate different phenomena captured by the data. Doing this can be challenging especially in cases when the depicted scatterplots do not show clear-cut visual separation between point clusters. As we shall see, DBMs compute and depict such boundaries explicitly, thereby significantly easing the analyst's task. \subsection{Machine learning classifiers for mineral genetic type classification} Machine learning is the emerging approach to solving geochemistry data classification problems\,\citep{gregory2019DistinguishingOre,osullivan2020traceelement,petrelli2016Solvingpetrological,wang2021MachineLearning}. We start by recapping a few notations introduced earlier in Sec.~\ref{sec:background_ch2}.

such boundaries explicitly, thereby significantly easing the analyst's task. \subsection{Machine learning classifiers for mineral genetic type classification} Machine learning is the emerging approach to solving geochemistry data classification problems\,\citep{gregory2019DistinguishingOre,osullivan2020traceelement,petrelli2016Solvingpetrological,wang2021MachineLearning}. We start by recapping a few notations introduced earlier in Sec.~\ref{sec:background_ch2}. Let $D = {\mathbf{x}_i} \subset \mathbb{R}^n$, $1\leq i \leq N$, be a dataset of $n$-dimensional data points $x_i= (x_i^1, \ldots ,x_i^n) $ with corresponding labels $ y_i \in$ C, where C is the set of classes. Given a dataset $D$, a machine learning classifier constructs a function $f$ : $\mathbb{R}^n \rightarrow C$ so that $f(\mathbf{x}_) = \mathbf{y}_i$ for ideally all $\mathbf{x}_i \in D_t$, where $D_t \subseteq D$ is so the called training set. After training, one uses the model $f$ to infer labels of unseen points $\mathbf{x}_i$.

$f$ : $\mathbb{R}^n \rightarrow C$ so that $f(\mathbf{x}_) = \mathbf{y}_i$ for ideally all $\mathbf{x}_i \in D_t$, where $D_t \subseteq D$ is so the called training set. After training, one uses the model $f$ to infer labels of unseen points $\mathbf{x}_i$. In the present work, we considered several machine learning classification algorithms, including Logistic Regression \citep{cox1958Twofurther}, Support Vector Machines (SVM)\,\citep{cortes1995Supportvectornetworks}, Random Forests\,\citep{breiman2001RandomForests}, and Neural Networks. These represent distinct families of algorithms: Logistic Regression is a linear classification model; SVM stands as a maximum margin classifier; Random Forest embodies an ensemble method; and Neural Network signifies deep learning. Crucially, these classifiers are frequently examined in mineral classification studies\,\citep{gregory2019DistinguishingOre,zhong2021Revealingmultistage,wang2023MachineLearning}. This frequent examination not only enables a thorough comparative analysis but also underscores the relevance and robustness of our conclusions within the machine learning applications in geosciences. Machine learning methods can efficiently handle high-dimensional data. More specifically for the mineral genetic type classification problem, such methods can make, by default, full use of all available attributes (\emph{i.e.}, elements in the geochemistry context) of the minerals. Also, ML methods can distinguish significantly more classes, and with higher accuracy (and limited or no user effort) than discriminant diagrams.

can make, by default, full use of all available attributes (\emph{i.e.}, elements in the geochemistry context) of the minerals. Also, ML methods can distinguish significantly more classes, and with higher accuracy (and limited or no user effort) than discriminant diagrams. Most ML models are considered black-box which lack \emph{interpretation} \citep{molnar2020InterpretableMachine}. In particular, ML classifiers only output a label (with optional confidence values) for each sample. This information is usually insufficient to interpret scientific phenomena as it tells which type (class) a sample has but not why. Additionally, when ML methods fail to correctly predict classes, tuning them to do so is often a complex trial-and-error process. Our work aims to reveal the black box by extending machine learning classifiers with decision maps. In other words, we keep the advantages of machine learning for mineral genetic type classification while compensating for their disadvantages by providing extra visual insights into the classification process. Specifically, we use Supervised Decision Boundary Map (SDBM)\,\citep{oliveiraSDBM2022}, as this is the most updated version of decision maps method that increases both the speed and quality of the original DBM method. Thus, SDBM was employed to construct decision maps for all the following experiments in this chapter.

(SDBM)\,\citep{oliveiraSDBM2022}, as this is the most updated version of decision maps method that increases both the speed and quality of the original DBM method. Thus, SDBM was employed to construct decision maps for all the following experiments in this chapter. The dataset used in this study is a compilation of published pyrite trace elements datasets. Pyrite is a ubiquitous mineral in the crust. Appearing in various mineral deposit types, its trace elements can fingerprint its forming environments\,\citep{belousov2016Pyritecompositions,zhong2021Revealingmultistage}. In this study, we compiled a dataset with 3571 pyrite LA-ICP-MS analyses from different origins, including Ni-Cu/platinum group element deposits (Ni-Cu-PGE, igneous deposits), porphyry deposits, orogenic deposits, Carlin-type Au, volcanic-hosted massive sulfide (VHMS) deposits, and barren sedimentary pyrite. Eleven trace elements (Co, Ni, Cu, Zn, Se, As, Ag, Sb, Au, Bi, Pb) are selected as features, or dimensions, for our study. Each trace element was measured in parts per million (ppm) and these measurements were used to train machine learning classifiers which are next explored using the decision map. Detailed information on the compiled dataset is shown in Table~\ref{tab:pyrite_dataset}, including the used data sources. \caption{Published pyrite trace element datasets used in this study.\label{tab:pyrite_dataset}} Class       & No. of samples & References \\ \hline

are next explored using the decision map. Detailed information on the compiled dataset is shown in Table~\ref{tab:pyrite_dataset}, including the used data sources. \caption{Published pyrite trace element datasets used in this study.\label{tab:pyrite_dataset}} Class & No. of samples & References \\ \hline Ni-Cu-PGE & 263 & \cite{mansur2021overviewchalcophile} \\ Porphyry    & 658            & \cite{hong2018Elementmigration,keith2022Phaseseparation,liu2020Oregenesis,mavrogonatos2020MultiStageIntroduction,sheng2022Distalgold,tang2021Originevolution,zhang2016ReOsisotopic}\\ Orogenic    & 615            & \cite{zhong2021Revealingmultistage,large2007MultistageSedimentary}      \\ Carlin      & 487            & \cite{he2021situmultiple,large2009GoldTrace,liang2021EVOLUTIONINVISIBLE,lin2021situsulfur,xie2018MagmaticOrigin}\\ VHMS        & 150            & \cite{revan2014Mineralogytraceelement,zhong2021Revealingmultistage}            \\ Sedimentary & 1421           & \cite{zhong2021Revealingmultistage}           \\ \hline After assembling the dataset to be used for classification, the following workflow was conducted: data preprocessing, SDBM training, search for best classifiers, map building, and evaluation. To select the best (classifier decision-map) pair, we use three metrics, introduced next. We fully elaborate these metrics further in Sec.~\ref{sec:metrics} which is our main contribution to quality assessment of decision maps. Classifier accuracy $ACC_C$, likely the best-known and most frequently used of the three metrics, is the fraction of correct predictions in a high-dimensional dataset and its respective labels. It is defined as {ACC}_{C} = \frac{ |\{ \mathbf{x}_i \in D \;|\; C(\mathbf{x}_i) = f(\mathbf{x}_i) \} | } { |D| },

used of the three metrics, is the fraction of correct predictions in a high-dimensional dataset and its respective labels. It is defined as {ACC}_{C} = \frac{ |\{ \mathbf{x}_i \in D \;|\; C(\mathbf{x}_i) = f(\mathbf{x}_i) \} | } { |D| }, where $| \cdot |$ denotes the size of a set and $D$ is the sample set, with labels in $C$, used for evaluation. Map accuracy $ACC_M$ is the proportion of correctly positioned data points in the decision zones for a given dataset. It is defined as {ACC}_{M} = \frac{ |\{ \mathbf{x}_i \in D \;|\; C(\mathbf{x}_i) = f(P^{-1}(P(\mathbf{x}_i))) \} | } { |D| }. Data consistency $Cons_d$ measures the proportion of samples that retain their predicted labels, as determined by the classifier $f$, after a direct-inverse projection cycle. It is defined as {Cons_{d}} = \frac{ | \{ \mathbf{x}_i \in D \;|\; f(P^{-1}(P(\mathbf{x}_i))) = f(\mathbf{x}_i) \} |}{ |D|}. The data were processed by the following steps: \textbf{Data missing value imputation:} Unless not measured, missing values in the input dataset indicate analyses below detection limits. Missing values were set to half the detection limit to keep the data distribution.

The data were processed by the following steps: \textbf{Data missing value imputation:} Unless not measured, missing values in the input dataset indicate analyses below detection limits. Missing values were set to half the detection limit to keep the data distribution. \textbf{Data transformation:} Normality of the features is desired for downstream machine learning model training. Trace elements in minerals are lognormal distributed. A power transformation\,\citep{yeo2000newfamily}, given by T(x_i^j) = log_{10}(x_i^j + 1) was applied to each sample $i$ in each dimension $j$ to obtain this desired normality. \textbf{Data splitting:} The whole dataset was randomly split into a training set $D_t$ (80\%) and a test set $D_T$ (20\%) by stratified sampling while keeping each class's proportions. $D_t$ was used to train the classifier and SDBM, while the $D_T$  was used to evaluate the performance of the classifier, the quality of the computed SDBM, and finally the classifier-SDBM combination. \textbf{Oversampling:} Decision functions would favor the class with the larger number of samples as our dataset is unbalanced. To correct this, the Synthetic Minority Oversampling Technique (SMOTE)\,\citep{chawla2002SMOTESynthetic} was applied to $D_t$. Note that this does not affect our final results since we split $D_T$ before oversampling.

class with the larger number of samples as our dataset is unbalanced. To correct this, the Synthetic Minority Oversampling Technique (SMOTE)\,\citep{chawla2002SMOTESynthetic} was applied to $D_t$. Note that this does not affect our final results since we split $D_T$ before oversampling. \caption{Workflow of the optimal decision map construction and evaluation. Abbreviation: LR, Logistic Regression; SVM, Support Vector Machine; RF, Random Forest; NN, Neural Network.} \subsubsection{Optimal decision boundary map construction} In the following, we describe the pipeline we use to construct the optimal decision map. The workflow is summarized in Figure~\ref{fig:workflow_ch3}. \textbf{SDBM training:} Building decision maps followed the SDBM pipeline\,\citep{oliveiraSDBM2022}, except that we trained SSNP, the technique used for constructing $P$ and $P^{-1}$ before training the classifier. This was needed because our aim next was to search for the best classifier among candidates evaluated using the same SSNP instance.

pipeline\,\citep{oliveiraSDBM2022}, except that we trained SSNP, the technique used for constructing $P$ and $P^{-1}$ before training the classifier. This was needed because our aim next was to search for the best classifier among candidates evaluated using the same SSNP instance. \textbf{Classifier search:} Four classifiers were evaluated by stratified K-fold cross-validation on the training set using the metrics described by Equations 1-3. These classifiers included Logistic Regression, SVM (with an RBF kernel), Random Forests (200 estimators), and a Neural Network (3 hidden layers of 100 units each). All these models were constructed using scikit-learn\,\citep{pedregosa2011ScikitlearnMachine}. The classifier with the highest cross-validation scores (Equations 1-3) was selected and retrained to build the final decision map.

Forests (200 estimators), and a Neural Network (3 hidden layers of 100 units each). All these models were constructed using scikit-learn\,\citep{pedregosa2011ScikitlearnMachine}. The classifier with the highest cross-validation scores (Equations 1-3) was selected and retrained to build the final decision map. \textbf{Map building:} We created the final decision map following the procedure detailed in\,\citet{oliveiraSDBM2022}. The decision map resolution was set to $300^2$ pixels. Pixels $\mathbf{p}$ were colored by the class value $f(P^{-1}(\mathbf{p}))$. To represent confidence levels (prediction probability of f) on the decision map, we adjusted the brightness of each pixel. Pixels $\mathbf{p}$ in areas with lower confidence, typically near the boundaries where decisions change, are shown in darker shades. In contrast, $\mathbf{p}$ in high confidence areas, well inside a clear decision region, are shown in brighter shades. The visual approach allows users to quickly see where the model's predictions are more or less certain. \textbf{Evaluation:} The retrained classifier and SDBM were finally evaluated on $D_T$ with the metrics in Equations 1-3.

region, are shown in brighter shades. The visual approach allows users to quickly see where the model's predictions are more or less certain. \textbf{Evaluation:} The retrained classifier and SDBM were finally evaluated on $D_T$ with the metrics in Equations 1-3. The results of the classifier search are shown in Table~\ref{tab:hp_search}. Random forests got the highest $ACC_C$ but the lowest $ACC_M$, which can be considered a poor generalization; SVM ranked third in $ACC_C$ and first in both $ACC_M$ and Cons; Neural Network had slightly lower results than Random forests for all three considered metrics; Logistic regression did not obtain competitive results in classifier accuracy compared to the other three models, its $ACC_C$ being 0.09 lower than the penultimate one (SVM). Based on all three metrics, we selected SVM as the best classifier for building the decision map. The resulting map of pyrite classification built for SVM is shown in Fig.~\ref{fig:sdbm_pyrite} with samples of both the training and test set plotted. Test set samples are dots with black outlines; training set samples are dots without outlines. We see that most samples fall within their respective decision zones, which already indicates a good classification performance.

both the training and test set plotted. Test set samples are dots with black outlines; training set samples are dots without outlines. We see that most samples fall within their respective decision zones, which already indicates a good classification performance. \caption{Search results of the classifiers for building the Decision Boundary Map. The highest value per metric type is indicated in bold.} Model                        & $ACC_\text{C}$     & $ACC_\text{M}$  & $Cons_d$             \\ Logistic Regression          & 0.855            & 0.918          & 0.871           \\ SVM                          & 0.942            & \textbf{0.926} & \textbf{0.922}  \\ Random Forest                & \textbf{0.984}   & 0.886          & 0.886           \\ Neural Network               & 0.978            & 0.874          & 0.875           \\ \caption{Decision Map built from the training set and the trained SVM. Training set samples are plotted as colored dots without outlines. Test set samples are plotted as colored dots with black outlines. Darker pixels in the map (mainly pixels close to the decision boundaries) show lower classification confidence.}

the trained SVM. Training set samples are plotted as colored dots without outlines. Test set samples are plotted as colored dots with black outlines. Darker pixels in the map (mainly pixels close to the decision boundaries) show lower classification confidence.} For the evaluation on the test set $D_T$, SVM got an overall accuracy $ACC_C$ = 0.91 (Equation~\ref{eq:acc_c_ch3}), while the SDBM got an overall accuracy $ACC_M$ = 0.88 (Equation~\ref{eq:acc_m_ch3}) and a consistency $Cons_d$ = 0.90 (Equation~\ref{eq:cons_d_ch3}). The confusion matrices of both the SVM and the SDBM are shown in Fig.~\ref{fig:confusion_matrix}. $ACC_M$ is 0.03 lower than $ACC_C$. This minor discrepancy, which is nearly uniform across all classes, suggests that the SDBM's (inverse) projection process ($P$ and $P^{-1}$) introduces a minimal classification error for the SVM. This negligible drop of accuracy indicates that the SDBM faithfully represents the actual classifier's decision boundaries. \caption{(a) Confusion matrix for the actual SVM classifier. (b) Confusion matrix for the trained decision map for this classifier.}

classification error for the SVM. This negligible drop of accuracy indicates that the SDBM faithfully represents the actual classifier's decision boundaries. \caption{(a) Confusion matrix for the actual SVM classifier. (b) Confusion matrix for the trained decision map for this classifier.} We next present two applications of the decision maps to show their added-value in classifier construction and analysis. First, we demonstrate how decision maps work on samples from unseen locations and show their added-value in conjunction with regular machine learning methods. Second, we demonstrate how decision maps can help data exploration and model explanation. \subsection[Unseen location example]{Unseen location application example} \subsubsection{Case Study: Analysis of the Zaozigou Gold Deposit}

unseen locations and show their added-value in conjunction with regular machine learning methods. Second, we demonstrate how decision maps can help data exploration and model explanation. \subsection[Unseen location example]{Unseen location application example} \subsubsection{Case Study: Analysis of the Zaozigou Gold Deposit} The trained classifier and its decision map were applied to data of pyrite trace elements from a new location – the Zaozigou gold deposit, which is unseen by the models. Zaozigou is the largest gold deposit (118 tons Au) that is under operation in the Gannan area in the Triassic West Qinling orogenic belt in China\,\citep{qiu2020giantZaozigou}. Pyrite is the main gold-bearing mineral in this deposit, and its trace elements can be used to identify the physicochemical conditions of gold mineralization\,\citep{rusk2012CathodoluminescentTextures}. The genetic classification however is still in debate, which hinders our understanding for ore formation and future explanation strategy\,\citep{qiu2020giantZaozigou}.\,\citet{sui2020GenesisZaozigou} considered that the Zaozigou deposit is a reduced intrusion-related gold system (magmatic);\,\citet{qiu2020giantZaozigou} argued that this deposit is best classified as an epizonal orogenic Au-Sb deposit (metamorphic hydrothermal) based on in situ monazite geochronology.

for ore formation and future explanation strategy\,\citep{qiu2020giantZaozigou}.\,\citet{sui2020GenesisZaozigou} considered that the Zaozigou deposit is a reduced intrusion-related gold system (magmatic);\,\citet{qiu2020giantZaozigou} argued that this deposit is best classified as an epizonal orogenic Au-Sb deposit (metamorphic hydrothermal) based on in situ monazite geochronology. The fine-labeled pyrite trace element data from\,\citet{sui2020GenesisZaozigou} was analyzed using the trained classifier and decision map. The pyrites are categorized into three types: (1) Py1a: pyrites in sedimentary rocks, (2) Py1b: pyrites in dike-hosted ores, and (3) Py2: pyrite grains in quartz-sulfide-ankerite veinlets. We believe that this example demonstrates our new approach's utility in solving real scientific problems. \subsubsection{Classifying Pyrite from Zaozigou}

types: (1) Py1a: pyrites in sedimentary rocks, (2) Py1b: pyrites in dike-hosted ores, and (3) Py2: pyrite grains in quartz-sulfide-ankerite veinlets. We believe that this example demonstrates our new approach's utility in solving real scientific problems. \subsubsection{Classifying Pyrite from Zaozigou} The data from the Zaozigou deposits yield results in two parts: the regular machine learning classifier (SVM) results (Table~\ref{tab:zzg_svm}) and the decision map (SDBM) results (Table~\ref{tab:zzg_sdbm}, Fig.~\ref{fig:sdbm_zzg}). (1) For the samples labeled Py1a (pyrite sedimentary rocks), the SVM classified 56\% of them as orogenic pyrite and 44\% as pyrite in Carlin-type deposits; on the decision map, 46\% of these samples were plotted in the sedimentary zone, 39\% in the orogenic zone, and 15\% in the Carlin zone. (2) For samples labeled Py1b (dike-hosted ores), most are classified as orogenic (94\% and 84\% for SVM and SDBM, respectively). (3) Most samples labeled Py2 (grains in quartz-sulfide-ankerite veinlets) are also classified as orogenic (70\% and 78\% for SVM and SDBM, respectively). In summary, SVM and SDBM yield similar results: Py1b and Py2 samples are classified as orogenic class; Py1a samples, however, exhibit ambiguity between Carlin, orogenic, and sedimentary types. The decision map tends to classify Py1a samples as sedimentary more than the SVM.

SVM and SDBM yield similar results: Py1b and Py2 samples are classified as orogenic class; Py1a samples, however, exhibit ambiguity between Carlin, orogenic, and sedimentary types. The decision map tends to classify Py1a samples as sedimentary more than the SVM. \caption{Zaozigou pyrite trace element data classification result from the SVM} & \textbf{Ni-Cu-PGE} & \textbf{Porphyry} & \textbf{Orogenic} & \textbf{Carlin} & \textbf{VMS} & \textbf{Sedimentary}  \\ {Py1a}  & 0 (0.00\%)         & 0 (0.00\%)        & 18 (43.90\%)      & 23 (56.10\%)    & 0 (0.00\%)   & 0 (0.00\%)            \\ {Py1b}  & 0 (0.00\%)         & 0 (0.00\%)        & 30 (93.75\%)      & 2 (6.25\%)      & 0 (0.00\%)   & 0 (0.00\%)            \\ {Py2}   & 0 (0.00\%)         & 2 (5.41\%)        & 26 (70.27\%)      & 7 (18.92\%)     & 2 (5.41\%)   & 0 (0.00\%)            \\ {Total} & 0 (0.00\%)         & 2 (1.82\%)        & 74 (67.27\%)      & 32 (29.09\%)    & 2 (1.82\%)   & 0 (0.00\%)            \\ \caption{Zaozigou pyrite trace element data classification result from the DBM} \textbf{}      & \textbf{Ni-Cu-PGE} & \textbf{Porphyry} & \textbf{Orogenic} & \textbf{Carlin} & \textbf{VMS} & \textbf{Sedimentary}  \\ {Py1a}  & 0 (0.00\%)         & 0 (0.00\%)        & 16 (39.02\%)      & 6 (14.63\%)     & 0 (0.00\%)   & 19 (46.34\%)          \\

data classification result from the DBM} \textbf{} & \textbf{Ni-Cu-PGE} & \textbf{Porphyry} & \textbf{Orogenic} & \textbf{Carlin} & \textbf{VMS} & \textbf{Sedimentary} \\ {Py1a} & 0 (0.00\%) & 0 (0.00\%) & 16 (39.02\%) & 6 (14.63\%) & 0 (0.00\%) & 19 (46.34\%) \\ {Py1b} & 0 (0.00\%) & 2 (6.25\%) & 27 (84.38\%) & 2 (6.25\%) & 0 (0.00\%) & 1 (3.12\%) \\ {Py2}   & 0 (0.00\%)         & 0 (0.00\%)        & 29 (78.38\%)      & 5 (13.51\%)     & 2 (5.41\%)   & 1 (2.70\%)            \\ {Total} & 0 (0.00\%)         & 2 (1.82\%)        & 72 (65.45\%)      & 13 (11.82\%)    & 2 (1.82\%)   & 21 (19.09\%)          \\

{Py2} & 0 (0.00\%) & 0 (0.00\%) & 29 (78.38\%) & 5 (13.51\%) & 2 (5.41\%) & 1 (2.70\%) \\ {Total} & 0 (0.00\%) & 2 (1.82\%) & 72 (65.45\%) & 13 (11.82\%) & 2 (1.82\%) & 21 (19.09\%) \\ Focusing on the decision map (Fig.~\ref{fig:sdbm_zzg}), Py1b and Py2 samples are mainly plotted in the orogenic zone, as expected. However, intriguingly, Py1a samples are divided into two clusters. One cluster is within the orogenic domain, while the other is located around the boundaries of the orogenic, Carlin, and sedimentary zones. From the geological perspective, this bifurcation suggests that the first cluster may have interacted with ore fluids, resulting in a distinct geochemical signature. Consequently, their intricate geochemical features make these data to be a challenge to be classified. As a result, they landed near the decision boundaries of several related decision zones, which are areas of low confidence from the perspective of machine learning classification. \caption{Zaozigou pyrite trace element data plotted on the trained decision map. The pyrite trace element data is from\,\citet{sui2020GenesisZaozigou}.}

near the decision boundaries of several related decision zones, which are areas of low confidence from the perspective of machine learning classification. \caption{Zaozigou pyrite trace element data plotted on the trained decision map. The pyrite trace element data is from\,\citet{sui2020GenesisZaozigou}.} In summary, decision maps offer two significant pieces of additional information beyond mere agreement with the classifier: First, they reveal data clusters, which are crucial for interpreting the data, as demonstrated above; second, the decision maps demonstrate information for each individual sample, not as an aggregate score. This includes the level of classification confidence, for example, whether a sample is close to a decision boundary. Such detailed information offers a more granular understanding than an overall and general aggregate score. \subsection{Exploratory data analysis and model explanation using decision maps}

the level of classification confidence, for example, whether a sample is close to a decision boundary. Such detailed information offers a more granular understanding than an overall and general aggregate score. \subsection{Exploratory data analysis and model explanation using decision maps} \subsubsection{Feature Inverse Projection} The decision maps shown so far are useful to show how all samples spread over the decision zones inferred by the trained model and also allow interpretation of the classification confidence in terms of the distance from a sample to its closest decision boundary in the map. However, they do not show which features are most responsible for the emergence of the respective decision zones. Understanding this is essential to further explain the studied phenomenon. To address this goal, we propose a new visualization called feature inverse projection.

they do not show which features are most responsible for the emergence of the respective decision zones. Understanding this is essential to further explain the studied phenomenon. To address this goal, we propose a new visualization called feature inverse projection. To see the relationship between each feature and the decision zones/boundaries, we created a corresponding map to each feature (pyrite trace elements). For the map of each feature $j$, $j$ $\in$ {Co, Ni, Cu, Zn, Se, As, Ag, Sb, Au, Bi, Pb}, each pixel $\mathbf{p}$ was colored by $T^{-1} (P^{-1}(\mathbf{p})^j)$, which is the value of the respective feature $j$, where $T^{-1}(t) = 10^t - 1$ is the inverse function of the power transformation given by Equation~\ref{eq:power_transformation}. \subsubsection{Ranking the features} To better guide a better reading of the maps, we propose to rank the features quantitatively. While there are multiple ways to rank the features based on their importance, here we suggest two options: (1) permutation feature importance\,\citep{breiman2001RandomForests} for global ranking (all classes), and (2) mutual information (Ross 2014) for local ranking (user selected class).

quantitatively. While there are multiple ways to rank the features based on their importance, here we suggest two options: (1) permutation feature importance\,\citep{breiman2001RandomForests} for global ranking (all classes), and (2) mutual information (Ross 2014) for local ranking (user selected class). The permutation feature importance of the classifier gives an intuition of the importance ranking of these trace elements in pyrite genetic type classification globally. The rank of the permutation feature importance of the SVM classifier on the test set is Ni > Au > Sb > Pb > As > Se > Co > Bi > Cu > Ag > Zn (Fig.~\ref{fig:feature_inv}a). The importance value of each feature is the decrease in SVM accuracy on $D_T$ when a single feature value is randomly shuffled. All the importance scores are above zero, which means that all these trace elements are helpful in the classification. \caption{(a) Permutation feature importance of the SVM classifier. (b-l) Feature inverse projections for all the 11 features of the considered dataset that explain which features and feature-values are most responsible for the appearance of the learned decision zones.}

the classification. \caption{(a) Permutation feature importance of the SVM classifier. (b-l) Feature inverse projections for all the 11 features of the considered dataset that explain which features and feature-values are most responsible for the appearance of the learned decision zones.} The permutation importance provides a glimpse of the overall feature ranking. However, when the users are interested in how much a feature helps with distinguishing a certain class from all the others, a better option is to design an algorithm to rank for this specific class. Therefore, we tailor mutual information to our decision map case. For each feature $j$, we calculate the mutual information $I(S_c(f(P^{-1}(\mathbf{p})))$, $P^{-1}(\mathbf{p})^j$) for all pixel $\mathbf{p}$, where $S_c$ is a function that masks off all labels which are not $c$ (the class label selected by users ). Mutual information is a non-negative value. Simply put, it measures the dependence of the feature $j$ and the user-selected class $c$. It equals 0 if feature $j$ and label $c$ are independent. The higher the value, the stronger the dependency, and thus the visual pattern of the feature aligns better with selected decision zone $c$ and its decision boundaries (discussed below). This quantitative measurement is particularly useful when multiple features show similar patterns.

higher the value, the stronger the dependency, and thus the visual pattern of the feature aligns better with selected decision zone $c$ and its decision boundaries (discussed below). This quantitative measurement is particularly useful when multiple features show similar patterns. Note that the ranking methods are to provide the users (geologists) with clues for exploring the data. Geologists' knowledge is still crucial in interpreting the data in this human-centered application case. \caption{Mutual information scores ranking feature importance for each class respectively. }

the ranking methods are to provide the users (geologists) with clues for exploring the data. Geologists' knowledge is still crucial in interpreting the data in this human-centered application case. \caption{Mutual information scores ranking feature importance for each class respectively. } \subsubsection{Visualizing feature patterns} The resulting inverse projected features are displayed in the permutation importance order (Fig.~\ref{fig:feature_inv}b-l). Black lines in these images show the decision boundaries. Actual feature values of the high-dimensional samples corresponding to every pixel are color-coded on an ordinal colormap (blue=low, red=high feature values). We used a banded colormap having a small number of discrete levels. This way, color changes in the images indicate the actual isolines (equal-feature-value contours) of the respective features in the data. Simply put, if a color band created by the above colormap for value v of feature $j$ has a shape that matches well the shape of a decision zone for class $c$ it is plotted over, it means that the value v of $j$ is a strong predictor of class $c$. Conversely, if all color bands of feature $j$ have shapes that do not match well any of the decision zones, it means that $j$ is not a strongly useful feature for the classification. This can be exemplified by either permutation importance or mutual information ranking: (1) Ni, which is ranked as the most important feature for prediction, shows three color bands (dark blue, light blue, red) which match quite well the Porphyry, VHMS, and Ni-Cu-PGE zones, respectively (Fig.~\ref{fig:feature_inv}b). In contrast, Zn, the least important feature for prediction, shows color bands that match far less well than any of the six decision zones (Fig.~\ref{fig:feature_inv}l). While permutation importance gives us an initial understanding of feature relevance, mutual information can provide a more nuanced view, especially in terms of how specific features align with a certain class. (2) For instance, in mutual information ranking, the features with the highest and lowest scores for the orogenic class are Pb and Au, respectively (Fig.~\ref{fig:mutual_info}). The isolines of Pb align well with the shape of the orogenic decision zone (Fig.~\ref{fig:feature_inv}e), highlighting Pb is a strong indicator for predicting orogenic class. Conversely, the isolines of Au, being roughly perpendicular to the orogenic decision zone (Fig.~\ref{fig:feature_inv}c), indicate that Au is less useful for discriminating this class.

of the orogenic decision zone (Fig.~\ref{fig:feature_inv}e), highlighting Pb is a strong indicator for predicting orogenic class. Conversely, the isolines of Au, being roughly perpendicular to the orogenic decision zone (Fig.~\ref{fig:feature_inv}c), indicate that Au is less useful for discriminating this class. Let us explore in detail how the feature inverse maps show the relationships between pyrite trace elements and their forming environment types learned from the model. We consider both visual patterns (relations between color bands and decision zones) and feature ranking. We do this in order of permutation importance: \item The color bands show that Ni > 1000 ppm can distinguish Ni-Cu-PGE from other classes, and Ni < 1 ppm can distinguish porphyry from other classes (Fig.~\ref{fig:feature_inv}b); Mutual information feature ranking confirms the importance of Ni for both Ni-Cu-PGE and porphyry classes (Fig.~\ref{fig:mutual_info}) \item Au > 100 ppm characterizes pyrites from orogenic and Carlin-type deposits. Au < 0.1 ppm is the character of pyrites from barren sedimentary and Ni-Cu-PGE (magmatic) deposits (Fig.~\ref{fig:feature_inv}c); However, Au is not a strong predictor for any single class, as indicated by its lower mutual information scores (Fig.~\ref{fig:mutual_info}).

and Carlin-type deposits. Au < 0.1 ppm is the character of pyrites from barren sedimentary and Ni-Cu-PGE (magmatic) deposits (Fig.~\ref{fig:feature_inv}c); However, Au is not a strong predictor for any single class, as indicated by its lower mutual information scores (Fig.~\ref{fig:mutual_info}). \item Pyrite with Sb < 0.1 ppm is more likely from Ni-Cu-PGE or porphyry deposits, while pyrite with Sb > 10 ppm is more likely from the other four classes (Fig.~\ref{fig:feature_inv}d); The mutual information score robustly supports the visual pattern indicating the importance of Sb for the porphyry class (Fig.~\ref{fig:mutual_info}). \item Pyrites from VHMS deposits, sedimentary and Carlin-type deposits tend to have Pb values > 100 ppm (Fig.~\ref{fig:feature_inv}e); Moreover, as mentioned above, the color band of Pb concentration ranging 10 - 100 ppm aligns well with the shape of the orogenic decision zone, the significance of which is also confirmed by mutual information score for orogenic class (Fig.~\ref{fig:mutual_info}). \item Pyrite from Carlin, orogenic and VHMS deposits have high As values. Most Carlin pyrite and some orogenic pyrite could have As > 10000 ppm (Fig.~\ref{fig:feature_inv}f); When focusing on a single class, As appears to be an efficient predictor for only Carlin class (Fig.~\ref{fig:mutual_info}).

orogenic and VHMS deposits have high As values. Most Carlin pyrite and some orogenic pyrite could have As > 10000 ppm (Fig.~\ref{fig:feature_inv}f); When focusing on a single class, As appears to be an efficient predictor for only Carlin class (Fig.~\ref{fig:mutual_info}). \item Pyrites with Se < 10 ppm are more likely to be from porphyry or orogenic deposits (Fig.~\ref{fig:feature_inv}g); Se, however, does not have a significant mutual information score for distinguishing any single class, as shown in Figure~\ref{fig:mutual_info}. \item Co > 1000 ppm characterizes Ni-Cu-PGE pyrite (Fig.~\ref{fig:feature_inv}h); Figure~\ref{fig:mutual_info} highlights Co as the most significant element for the Ni-Cu-PGE class. \item VHMS and part of the Ni-Cu-PGE zone have Bi > 10 ppm (Fig.~\ref{fig:feature_inv}i); However, Bi's insignificance for single-class discrimination is evident in Figure~\ref{fig:mutual_info}. \item Cu < 10 ppm is the character of porphyry pyrite. Pyrites in the other four classes have Cu varying from 10 to 10000 ppm (Fig.~\ref{fig:feature_inv}j); The significance of Cu for identifying porphyry class is also strongly confirmed by the mutual information score (Fig.~\ref{fig:mutual_info}).

ppm is the character of porphyry pyrite. Pyrites in the other four classes have Cu varying from 10 to 10000 ppm (Fig.~\ref{fig:feature_inv}j); The significance of Cu for identifying porphyry class is also strongly confirmed by the mutual information score (Fig.~\ref{fig:mutual_info}). \item The color band of Ag < 1 ppm align with the porphyry zone(Fig.~\ref{fig:feature_inv}k); However, similar to Au, Ag is overall also not efficient for identifying any class, as it never ranks in the top 3 for any class in mutual information score (Fig.~\ref{fig:mutual_info}). \item The Zn value color bands do not match the decision zones well, except for the band of Zn > 100 ppm, which matches the VHMS zone well (Fig.~\ref{fig:feature_inv}l); And, indeed, Zn is the most efficient element for distinguishing VHMS from other classes according to mutual information score for VHMS class (Fig.~\ref{fig:mutual_info}). \subsection{Interpretability and limitations of decision maps}

Zn > 100 ppm, which matches the VHMS zone well (Fig.~\ref{fig:feature_inv}l); And, indeed, Zn is the most efficient element for distinguishing VHMS from other classes according to mutual information score for VHMS class (Fig.~\ref{fig:mutual_info}). \subsection{Interpretability and limitations of decision maps} Based on the evaluation metrics (Equations 1-3), we established the optimal decision map for the pyrite genetic type classification task. As shown in the results, the SVM has an $ACC_C$ of 0.91, while the decision map for the aforementioned SVM has an $ACC_M$ of 0.88 and a $Cons_d$ of 0.90, on the test set $D_T$. This means that decision maps can be used to accurately predict how a classifier works. From a visual perspective, there is only a slight overlap of data points in the center of the map (Fig.~\ref{fig:sdbm_pyrite}), a property with which no existing 2D discriminant diagram can compete.

be used to accurately predict how a classifier works. From a visual perspective, there is only a slight overlap of data points in the center of the map (Fig.~\ref{fig:sdbm_pyrite}), a property with which no existing 2D discriminant diagram can compete. Decision maps provide a novel way to get insight into how machine learning classifiers work and where each data point lands in the context of decision boundaries. They should not be seen as a replacement, but rather an enhancement, of traditional classifier metrics (\emph{e.g.}, accuracy): Classifier metrics give a highly aggregated quality score (for the entire problem or per class), but do not tell how specific instances (train, test, or new) get classified. This is exactly the addition that decision maps provide. More specifically, the actual shapes of the decision zones and the spread of instances over them tell how easy is for a given classifier to handle a given data distribution, \emph{e.g.}, which classes are easily separable from the others and/or which parts of the data distribution are easily classifiable.

and the spread of instances over them tell how easy is for a given classifier to handle a given data distribution, \emph{e.g.}, which classes are easily separable from the others and/or which parts of the data distribution are easily classifiable. On top of the samples being categorized into a major class label, decision maps show how samples are similar to certain other classes via their distances to the closest decision boundaries. Samples near decision boundaries are more uncertain about the predicted label and thus more likely to be misclassified. Feature inverse maps (discussed below) provide additional insights into why these samples may have such problems. However, all these tools need to be complemented by an expert's knowledge to lead to effective interpretations and understanding of the studied phenomenon. Decision maps provide thus a way to combine human knowledge with machine learning predictions when interpreting classification results in a way that cannot be obtained from regular machine learning classification routines. The application of the decision map on the Zaozigou pyrite data discussed next further illustrates the added-value of our visualizations (detailed in the next subsection).

classification results in a way that cannot be obtained from regular machine learning classification routines. The application of the decision map on the Zaozigou pyrite data discussed next further illustrates the added-value of our visualizations (detailed in the next subsection). Besides observation-centric interpretation (seeing how samples spread with respect to each other and the inferred decision zones), our new addition to decision maps – the feature inverse projections – provides a class-centric interpretation, \emph{i.e.}, allows analysts to understand which features and feature values are key responsible for the appearance of specific decision zones or even separate sample groups. These feature inverse projections can be seen as a summary of real-world complex data that show what the model learned and how it decides when values vary. We further discuss this in the next subsection.

or even separate sample groups. These feature inverse projections can be seen as a summary of real-world complex data that show what the model learned and how it decides when values vary. We further discuss this in the next subsection. Our visual analysis techniques scale well with both the number of samples and the number of dimensions. As shown in Figure~7, the computation time of SDBM is minimally affected by changes in either the number of dimensions or the number of samples. It consistently remains at approximately 3-10 seconds on a standard desktop computer with a consumer-level graphics card. This contrast is particularly noticeable when compared to machine learning classifiers such as SVM, LDA, and QDA, which can be highly sensitive to changes in data dimensionality (Fig.~\ref{fig:runtime_ch3}). In general, using SDBM to obtain a decision map for a given classifier requires only a few additional seconds after the classifier is trained. Every sample point is reduced to a single 2D scatterplot point. While overplotting does occur, this does not affect, we argue, the usability of our proposal. Indeed, in most applications, one is interested in reasoning about groups of similar samples and not every single individual. Such groups become actually better visible when large amounts of samples are plotted. Decision maps also inherit by construction the scalability of the underlying projection techniques to tens or even hundreds of dimensions. Feature inverse projections are less scalable in this sense since we need to plot (and study) one map per feature. However, as discussed above, such maps can be ordered by feature ranking methods (\emph{e.g.}, permutation feature importance, mutual information), so that analysts can focus on a small set of most relevant features. Similar techniques have been used for explaining projections of high-dimensional data for 3D projections\,\citep{coimbra2016Explainingthreedimensional}.

can be ordered by feature ranking methods (\emph{e.g.}, permutation feature importance, mutual information), so that analysts can focus on a small set of most relevant features. Similar techniques have been used for explaining projections of high-dimensional data for 3D projections\,\citep{coimbra2016Explainingthreedimensional}. \caption{Plot showing the time taken by SDBM and 5 common classifiers, utilizing synthetic datasets of varying dimensionality and number of samples (n\_samples). The time recorded for SDBM includes the duration for fitting training samples and inverse projecting grids (grid size: $300^2$); whereas, for classifiers, it records the time for fitting and predicting labels for the training samples. Abbreviation: RF, Random Forest; SVM, Support Vector Machine NN, Neural Network; LDA, Linear Discriminant Analysis; QDA, Quadratic Discriminant Analysis.}

projecting grids (grid size: $300^2$); whereas, for classifiers, it records the time for fitting and predicting labels for the training samples. Abbreviation: RF, Random Forest; SVM, Support Vector Machine NN, Neural Network; LDA, Linear Discriminant Analysis; QDA, Quadratic Discriminant Analysis.} Decision maps and their proposed extensions are also generically deployable and easy to use: They can be generated automatically using any trained classifier and any relevant data set, whether it be training, test, or new data. Exploring the created visualizations also does not require any complex interaction from the user except the optional brushing of points to show details in a tooltip. While the standard implementation of SDBM\,\citep{oliveiraSDBM2022} does not provide this feature, adding it is very simple. Note that this functionality can target both existing points from the projected dataset $D$ used to construct the SDBM, and more interestingly, new points that correspond to pixels in the decision map to which no actual data point projects. These effectively generate new, unseen, data points in the high-dimensional space (via the inverse projection $P^{-1}$) which allow the analyst to reason about how the classifier, or more generally phenomenon under study, would behave for data outside the actually measured dataset one has.

new, unseen, data points in the high-dimensional space (via the inverse projection $P^{-1}$) which allow the analyst to reason about how the classifier, or more generally phenomenon under study, would behave for data outside the actually measured dataset one has. However, decision maps also have some limitations. As explained earlier, both direct and inverse projections have inevitable errors which cannot be fully eliminated in the generic case. We address this issue by quantifying the magnitude of errors and demonstrating that, for classifier analysis, these errors are minimal and do not significantly impact the interpretability of the decision maps. If desired, one can easily extend our proposal by visualizing errors locally in the decision maps following\,\citet{espadoto2021unprojection}. Studying how such more refined error views can help interpret classifiers is an important future work topic. A separate limitation of decision maps is that they do not explicitly depict individual dimensions along the two axes of the map, unlike classical discrimination diagrams. Combined with the nonlinear nature of the projections used to create the maps, this asks analysts to deploy more effort to understand how dimensions vary across the map. For example, the pyrite decision zones in Fig.~\ref{fig:sdbm_pyrite} show a trend from the high-temperature forming environment to the low-temperature forming environment in sequence: Ni-Cu-PGE – Porphyry – Orogenic – Carlin. The feature inverse projections help this analysis by mapping the feature variations, one by one, to the respective maps. An interesting future work direction is to summarize several such feature inverse projection images in a single map, thereby reducing the number of different visualizations one needs to study to interpret a decision map.

by one, to the respective maps. An interesting future work direction is to summarize several such feature inverse projection images in a single map, thereby reducing the number of different visualizations one needs to study to interpret a decision map. \subsection{Implications for mineral deposit genesis classification studies}

interesting future work direction is to summarize several such feature inverse projection images in a single map, thereby reducing the number of different visualizations one needs to study to interpret a decision map. \subsection{Implications for mineral deposit genesis classification studies} The dataset used in our work includes Carlin-type pyrite and Ni-Cu-PGE pyrite trace elements, which fills the gap of previous pyrite machine learning related work\,\citep{gregory2019DistinguishingOre,zhong2021Multilayerperceptronbased}. This dataset provides a more comprehensive view of pyrite from magmatic to hydrothermal origins. More importantly, we present a solution to the current problem of the lack of visual interpretability of machine learning in geochemical data classification work. Visual interpretability is a valuable property of traditional geochemistry discriminant diagrams, and it is also a desire for geochemistry data exploration and analysis. Our decision maps solution provides a unique perspective to reveal the structure and properties of data hidden from regular machine learning routines, offering new opportunities for analyzing and explaining geological problems. More specifically, the decision map application on Zaozigou pyrite trace elements shows how seeing the data clusters and their locations on the decision map can help interpretation compared to regular machine learning routines; the feature inverse projection application shows how the decision map can uncover what the model learned from the mapping of pyrite trace elements to pyrite forming environments, and displays how the model decides the type of pyrite when the trace element values vary.

projection application shows how the decision map can uncover what the model learned from the mapping of pyrite trace elements to pyrite forming environments, and displays how the model decides the type of pyrite when the trace element values vary. We now discuss the specific findings we obtained using decision maps for our specific use-case of studying mineral deposit genesis. Pyrite trace element data of the Zaozigou deposit from \citet{sui2020GenesisZaozigou} are plotted mainly in the orogenic zone on the decision map. Within this zone, the plotted data clusters are closer to the Carlin and sedimentary zones than the porphyry and Ni-Cu-PGE zones. From the view of pyrite trace elements, Zaozigou shows little similarity to magmatic-related (Ni-Cu-PGE, Porphyry) deposits. Instead, it shows some more similarity to low-temperature Carlin-type deposits. Therefore, it is reasonable that Py1a are plotted around the boundaries of the orogenic zone, Carlin zone, and sedimentary zone (Fig.~\ref{fig:sdbm_zzg}): Since Py1a samples are pyrites in sedimentary within the gold deposit district, Py1a shares similarities to pyrite in barren sedimentary geologically; Carlin-type deposits, which were first found in Nevada, USA, are sediment-hosted, disseminated Au deposits. So, they also share some similarities to pyrite in barren sedimentary. Most of the Carlin-type samples in the dataset are from gold deposits near the edge of the Yangtze craton, in southeast China. These deposits are also argued to be epizonal orogenic gold deposits\,\citep{bodnar2014135Fluid}. If we regard the Carlin class as an epizonal orogenic class, Py1a pyrites are more similar to pyrite from epizonal orogenic deposits than from classic orogenic deposits; Pyrites from Py1b and Py2 are more similar to pyrites from classic orogenic deposits. The conclusion from pyrite trace elements and the decision map method closely agrees with the monazite geochronology conclusion from\,\citet{qiu2020giantZaozigou}.

orogenic deposits than from classic orogenic deposits; Pyrites from Py1b and Py2 are more similar to pyrites from classic orogenic deposits. The conclusion from pyrite trace elements and the decision map method closely agrees with the monazite geochronology conclusion from\,\citet{qiu2020giantZaozigou}. According to the feature inverse projections (Fig.~\ref{fig:feature_inv}, Fig.~\ref{fig:mutual_info}), some trace elements can be considered indicator elements in discriminating the mineral-forming environments. For example, the model learned that Co, Ni, and Pb are efficient features when classifying Ni-Cu-PGE from others. This model learned knowledge is consistent with geologists' experience that Co, Ni, and their ratio in pyrite are considered reliable indicators and geochemical tools in ore deposit genesis\,\citep{bajwah1987Traceelement,bralia1979revaluationCo}. Knowing what the model learned for classifying the pyrite genetic types makes it easy to find other elements as indicators. For example, Pb, which is less discussed in the literature, could be an indicator for discriminating Ni-Cu-PGE, porphyry, and orogenic pyrites from the other classes. In Figure~5, we can observe that the model considers pyrites of Ni-Cu-PGE and porphyry classes to have the feature that Pb < 10 ppm, while orogenic pyrite has Pb roughly between 10 to 100 ppm; Cu could be another indicator for discriminating porphyry pyrite from the other classes, \emph{i.e.}, the model considers porphyry pyrite has the feature that Cu < 10 ppm (Fig.~\ref{fig:feature_inv}j). The effectiveness of Pb and Cu as indicators remains to be further proven in practice.

another indicator for discriminating porphyry pyrite from the other classes, \emph{i.e.}, the model considers porphyry pyrite has the feature that Cu < 10 ppm (Fig.~\ref{fig:feature_inv}j). The effectiveness of Pb and Cu as indicators remains to be further proven in practice. \subsection{Implications for the geoscience community} The union of information visualization and mineralogy, as presented in this study, heralds a transformative era in geoscience research. By harnessing the capabilities of enhanced decision maps, we have illuminated a novel approach to interpret classification models, deepening our comprehension of multifaceted geochemistry data dimensions. The introduction of inverse projections is particularly groundbreaking for geology. This feature unravels the depth of understanding models extracting from complex geochemical data, enabling researchers to directly correlate predictions with specific mineralogical features or value-ranges. In the realm of mineral geochemical discrimination, this research signifies a monumental shift. Transitioning from traditional machine learning classification to the advanced visual analytics of machine learning, we're effectively merging the precision and scalability of modern computational methods with the rich, interpretative legacy of discriminant diagrams.

discrimination, this research signifies a monumental shift. Transitioning from traditional machine learning classification to the advanced visual analytics of machine learning, we're effectively merging the precision and scalability of modern computational methods with the rich, interpretative legacy of discriminant diagrams. As it continues to lean into data-driven methodologies, our work offers a robust toolset for enhanced mineral genesis classification and exploration. Beyond the immediate applications, this study promises to influence a range of geochemistry sub-disciplines, driving more informed, nuanced, and efficient research and exploration endeavors in the future. \subsection{Implications for visualization community}

robust toolset for enhanced mineral genesis classification and exploration. Beyond the immediate applications, this study promises to influence a range of geochemistry sub-disciplines, driving more informed, nuanced, and efficient research and exploration endeavors in the future. \subsection{Implications for visualization community} This chapter shows that decision maps are practically useful in geosciences research, a discipline which is far away from computer science, and whose users are non-specialists in data visualization. While applications of decision maps in geosciences are also reported in \citet{zhou2023apatiteSDBM}, our work is, to our knowledge, the first in this area. On the one hand, these results are a good indicator that decision maps are an effective tools for solving problems in \emph{practice} when one needs to explore and/or improve a trained classification model. On the other hand, we discovered that current decision map methods have several challenges that need to be addressed to make them even more effective in such practical use cases. We summarize these challenges next. First, our work investigated only the SDBM decision map method -- and, while doing so, used only three relatively simple metrics to gauge quality.

to make them even more effective in such practical use cases. We summarize these challenges next. First, our work investigated only the SDBM decision map method -- and, while doing so, used only three relatively simple metrics to gauge quality. As mentioned in Chapter \ref{ch2:related}, several other decision map methods exist. As such, a comprehensive comparison of such methods with additional quality metrics is needed to understand the pros and cons of each method. This is addressed in Chapter~\ref{ch4:metrics_decision_map}. Secondly, we found that a single view -- or single decision map image -- cannot capture everything we aim to understand in the behavior of a trained model. To alleviate this, we customized the decision map with so-called feature inverse projection (Sec.~\ref{sec:eda_dbm_ch3}). While this is a good start, more refined user customizations are needed to make decision maps more flexible and more informative. We propose such user-driven customizations in Chapter~\ref{ch6:controllable_Pinv}.

this, we customized the decision map with so-called feature inverse projection (Sec.~\ref{sec:eda_dbm_ch3}). While this is a good start, more refined user customizations are needed to make decision maps more flexible and more informative. We propose such user-driven customizations in Chapter~\ref{ch6:controllable_Pinv}. Finally, we only created decision maps with a fixed resolution of $300^2$ pixels. The key reason for this was the limited computational efficiency of SDBM. As more user control is eventually incorporated, such as zooming in and out, the resolution of the decision map should be increased, which will require faster ways to compute it. We address this topic in Chapter~\ref{ch7:fastDBM}. \chapter{Qualitative and quantitative evaluation of decision maps} \section{Introduction} %for journal use above \firstsection{..} instead As introduced in \Cref{ch2:related}, several techniques exist to construct decision maps for arbitrary classifiers\,\citep{hamel2006visualizeSVM, migut2015Visualizingmultidimensional, schulz2015Usingdiscriminative, schulz2020DeepViewVisualizing, DBM2019rodrigues, oliveiraSDBM2022},

it. We address this topic in Chapter~\ref{ch7:fastDBM}. \chapter{Qualitative and quantitative evaluation of decision maps} \section{Introduction} %for journal use above \firstsection{..} instead As introduced in \Cref{ch2:related}, several techniques exist to construct decision maps for arbitrary classifiers\,\citep{hamel2006visualizeSVM, migut2015Visualizingmultidimensional, schulz2015Usingdiscriminative, schulz2020DeepViewVisualizing, DBM2019rodrigues, oliveiraSDBM2022}, These techniques have found applications in areas like model steering\,\citep{rodrigues2020VisualAnalytics}, detecting backdoor attacks\,\citep{schulz2020DeepViewVisualizing}, and our own work in interpreting geoscience models~(\Cref{ch3:geo_application}). Overall, such techniques take away some of the complexity of interpreting the working of a machine learning model while, in the same time, depicting the functioning of the model in more detailed ways than classical aggregate performance metrics. As such, decision map techniques are particularly attractive for users of machine learning who are not domain experts in the operation of the underlying ML algorithms\,\citep{zhou2023apatiteSDBM}\footnote{This chapter is based on the paper ``Quantitative and Qualitative Comparison of Decision Map Techniques for Explaining Classification Models''\,\citep{wang2023DMcompare}.}. Despite the above, no framework to comprehensively evaluate and compare such techniques exists~\citep{oliveiraSDBM2022}. This makes it hard for actual users to know which technique to pick and use in a given application context. Lacking an in-depth assessment for the quality of decision maps lead to some issues.

evaluate and compare such techniques exists~\citep{oliveiraSDBM2022}. This makes it hard for actual users to know which technique to pick and use in a given application context. Lacking an in-depth assessment for the quality of decision maps lead to some issues. From a technical perspective, it is unclear how effectively different decision map techniques capture information present in high-dimensional decision zones and boundaries. For instance, it is unclear whether the smoothness or fragmentation visible in decision boundaries truly depicts the same properties of the actual, high-dimensional, boundaries. Similarly, it is unclear whether a sample located close to a decision boundary in the map genuinely reflects its proximity to the high-dimensional point where a classifier changes output. From a practical perspective, this lack of evaluation makes it hard to select the most suitable decision map technique for a given dataset and classifier.

the map genuinely reflects its proximity to the high-dimensional point where a classifier changes output. From a practical perspective, this lack of evaluation makes it hard to select the most suitable decision map technique for a given dataset and classifier. We aim to fill this gap -- and therefore answer our research question \textbf{RQ1} introduced in Chapter~\ref{ch1:intro} -- by proposing a framework that evaluates decision map techniques. We proceed by identifying the desirable aspects that a decision map technique should have -- accuracy, reliability, interpretability, and computational efficiency. Next, we design several metrics to quantify these aspects. Taken together, these metrics aim to capture the concept of `quality' of a decision map technique, much like classical metrics in ML such as accuracy, area under the ROC curve, F1-scores, and similar aim to capture the concept of quality of a ML model. We then use these metrics to conduct a multi-faceted comparison of existing decision map techniques over several classifiers and datasets. This helps us identify several strengths and limitations of the evaluated decision map techniques. Finally, we propose a workflow for choosing the best decision map technique for a given dataset in light of a set of desirable requirements.

and datasets. This helps us identify several strengths and limitations of the evaluated decision map techniques. Finally, we propose a workflow for choosing the best decision map technique for a given dataset in light of a set of desirable requirements. Summarizing the above, our key contributions are as follows: \item {We propose a suite of metrics to quantitatively evaluate decision map techniques;} \item We conduct a comprehensive comparison of existing decision map techniques, both {quantitatively (by comparing the aforementioned metrics) and qualitatively (by comparing visually the obtained decision map images);} \item We propose a workflow to guide the selection of the most suitable decision map technique for a given dataset, based on a set of desirable requirements. \item At a higher level, this chapter answers our question RQ1 introduced in Sec.~\ref{ch1:intro}. We start recapping some notations. Let $ D = \{\mathbf{x}_i\} \subset \mathbb{R}^n$, $ 1 \leq i \leq N$, be a dataset of $n$-dimensional data points (samples) $\mathbf{x}_i = (x_i^1, x_i^2, \ldots, x_i^n)$ with corresponding labels $y_i \in C$. Let $\mathbf{x}^j = \{x_1^j, x_2^j, \cdots, x_N^j\}$, $1 \leq j \leq n$, be the $j$-th feature, or dimension, of $D$.

be a dataset of $n$-dimensional data points (samples) $\mathbf{x}_i = (x_i^1, x_i^2, \ldots, x_i^n)$ with corresponding labels $y_i \in C$. Let $\mathbf{x}^j = \{x_1^j, x_2^j, \cdots, x_N^j\}$, $1 \leq j \leq n$, be the $j$-th feature, or dimension, of $D$. Thus, $D$ can be seen as a table with $N$ rows (samples) and $ n$ columns (dimensions), with $\mathbf{y} = \{y_1, y_2, \ldots, y_N\}$ being their label vector. Given a dataset $D$ and its corresspoding labels $\mathbf{y}$, a \emph{classifier} constructs a function $f: \mathbb{R}^n \rightarrow C$, so that $f(\mathbf{x}_i) = y_i$ ideally for all $\mathbf{x}_i \in D_t$, where $D_t \subseteq D$ is the training set. After training, the classifier $f$ is typically further evaluated in the same way on a test set $D_T \subset D$, $D_T \cap D_t = \emptyset$. After training and testing, the classifier $f$ can be used to predict labels of new samples $\mathbf{x} \in \mathbb{R}^n \setminus D$.

typically further evaluated in the same way on a test set $D_T \subset D$, $D_T \cap D_t = \emptyset$. After training and testing, the classifier $f$ can be used to predict labels of new samples $\mathbf{x} \in \mathbb{R}^n \setminus D$. A dimensionality reduction (DR) method, also called a \emph{projection}, is a function $P: \mathbb{R}^n \rightarrow \mathbb{R}^q$ that maps a $n$-dimensional sample $\mathbf{x} \in \mathbb{R}^n$ to a $q$-dimensional sample $P(\mathbf{x}) \in \mathbb{R}^q$, where $q \ll n$. Typically, one uses $q = 2$ or $3$ for visualization purposes. An \emph{inverse projection} $P^{-1}: \mathbb{R}^q \rightarrow \mathbb{R}^n$, also called backprojection or unprojection\,\citep{espadoto2021unprojection}, is a function that maps a $q$-dimensional sample $\mathbf{z} \in \mathbb{R}^q$ to a $n$-dimensional sample $\mathbf{x} \in \mathbb{R}^n$, so that it approximates the inverse of $P$, \emph{i.e.}, $P^{-1}(P(\mathbf{x})) \approx \mathbf{x}$. We next denote by $P(D)$ the application of $P$ to all points of a dataset $D$ (and analogously for $P^{-1}$). } \subsection{Overall workflow of decision map}

\in \mathbb{R}^n$, so that it approximates the inverse of $P$, \emph{i.e.}, $P^{-1}(P(\mathbf{x})) \approx \mathbf{x}$. We next denote by $P(D)$ the application of $P$ to all points of a dataset $D$ (and analogously for $P^{-1}$). } \subsection{Overall workflow of decision map} Decision maps -- introduced in Sec.~\ref{sec:dbm_ch2} -- are techniques that visualize the decision boundaries and decision zones of a classifier: Given a classifier $f$, a \emph{decision boundary} is a surface in $\mathbb{R}^n$ that separates high-dimensional data points $\mathbf{x} \in \mathbb{R}^n$ into regions, also called \emph{decision zones}. All points in a given zone are assigned the same label $y \in C $ by the model $f$. Decision zones are separated by \emph{decision boundaries}, which are hypersurfaces embedded in $\mathbb{R}^n$ where the classifier $f$ changes output. Understanding how the high-dimensional space is partitioned into such decision zones, and how data samples in a training or test set are distributed across the zones, effectively helps understand how a classifier behaves\,\citep{DBM2019rodrigues,schulz2020DeepViewVisualizing}. For example, seeing how labeled samples distribute close to decision boundaries can help categorize misclassification problems; seeing how unlabeled samples (whose class is to be predicted by $f$) spread across decision zones helps understand how well $f$ can handle a given data distribution.

how labeled samples distribute close to decision boundaries can help categorize misclassification problems; seeing how unlabeled samples (whose class is to be predicted by $f$) spread across decision zones helps understand how well $f$ can handle a given data distribution. The general workflow of constructing decision maps is as follows (see also \Cref{fig:illustration_dm}): \caption{General workflow of decision map techniques (see Sec.~\ref{sec:dm_workflow}).} \item Train a classifier $f$ on a dataset $D_t$. This is the classifier whose decision map we next want to visualize; \item Construct a direct projection $P$ and inverse projection $P^{-1}$ using a dataset $D'$; \item Project $D'$ to create a 2D scatterplot $P(D')$; \item Sample the extent of $P(D')$ on a uniform pixel grid $I$; \item Backproject all pixels $\mathbf{p}\in I$ to the data space using $P^{-1}$; \item Use $f$ to predict the labels of the backprojected points $P^{-1}(\mathbf{p})$; \item Color $I$ according to the predicted labels $f(P^{-1}(\mathbf{p}))$.

$P(D')$ on a uniform pixel grid $I$; \item Backproject all pixels $\mathbf{p}\in I$ to the data space using $P^{-1}$; \item Use $f$ to predict the labels of the backprojected points $P^{-1}(\mathbf{p})$; \item Color $I$ according to the predicted labels $f(P^{-1}(\mathbf{p}))$. In the above, $D'$ can be, at a minimum, a subset of the training set $D_t$. However, if more samples of the target domain are available, such as in the form of test data $D_T$ or even unlabeled data, these can be added to $D'$. By doing this, the constructed decision map will better sample the actual data distribution of the investigated phenomenon and, thus, the classifier's behavior. In this work, we set $D' = D_t$ following earlier examples of this workflow\,\citep{oliveiraSDBM2022}.

added to $D'$. By doing this, the constructed decision map will better sample the actual data distribution of the investigated phenomenon and, thus, the classifier's behavior. In this work, we set $D' = D_t$ following earlier examples of this workflow\,\citep{oliveiraSDBM2022}. We next describe three specific techniques that implement the above workflow and introduce some of their key features. For completeness, these are the techniques we further selected for quantitative and qualitative evaluation in this chapter. The selection was based on the fact that these techniques pioneering in the DBM area\,\citep{DBM2019rodrigues}; are computationally effective and simple to use\,\citep{oliveiraSDBM2022}; and achieve a high quality representation\,\citep{schulz2020DeepViewVisualizing}, respectively. For each technique, we also indicate the exact origin of its source code, for replicability purposes of our comparison. \subsection[DBM]{Decision Boundary Maps (DBM)} Decision Boundary Maps (DBM)\,\citep{DBM2019rodrigues} closely follow the above workflow.

use\,\citep{oliveiraSDBM2022}; and achieve a high quality representation\,\citep{schulz2020DeepViewVisualizing}, respectively. For each technique, we also indicate the exact origin of its source code, for replicability purposes of our comparison. \subsection[DBM]{Decision Boundary Maps (DBM)} Decision Boundary Maps (DBM)\,\citep{DBM2019rodrigues} closely follow the above workflow. While the original DBM used t-SNE\,\citep{tSNEMaaten2008} and, alternatively, UMAP\,\citep{UMAP2018} for the projection $P$, any user-chosen projection method can be used, such as PCA\,\citep{jolliffe16}, LAMP\,\citep{lamp}, Least Square Projection (LSP)\,\citep{paulovich2008least}, or Piecewise Laplacian Projection (PLP)\,\citep{paulovich2010fast}. For the inverse projection $P^{-1}$, DBM evaluated two techniques, namely iLAMP\,\citep{Amorim2012ilamp} (the inverse of the LAMP projection technique mentioned above) and NNinv\,\citep{espadoto2019NNinv}. Compared to iLAMP, which constructs the backprojection by interpolating between the samples of $D$ and $P(D)$ using linear or radial kernels, NNinv deep learns a regressor to output $D$ using $P(D)$ as input. NNinv achieves inverse projections having a lower mean absolute error than P and is also simpler to implement and significantly faster\,\citep{espadoto2019NNinv,DBM2019rodrigues}. Note that the same deep learning idea has been used to construct direct projections with similar speed, quality, and implementation simplicity advantages\,\citep{espadoto2020Deeplearning}.

projections having a lower mean absolute error than P and is also simpler to implement and significantly faster\,\citep{espadoto2019NNinv,DBM2019rodrigues}. Note that the same deep learning idea has been used to construct direct projections with similar speed, quality, and implementation simplicity advantages\,\citep{espadoto2020Deeplearning}. DBM is simple to implement and allows one to use any direct and inverse projection techniques. Also, DBM works without requiring label information, which means that constructing decision maps only depends on the distribution of points in $D'$ in the feature space. In our subsequent evaluation, we use DBM with UMAP as the direct projection $P$ and NNinv for $P^{-1}$, as this combination led to the best results in earlier DBM evaluations\,\citep{DBM2019rodrigues}. {More information about DBM is available at \url{https://mespadoto.github.io/dbm/}.} \subsection[SDBM]{Supervised Decision Boundary Map (SDBM)}

evaluation, we use DBM with UMAP as the direct projection $P$ and NNinv for $P^{-1}$, as this combination led to the best results in earlier DBM evaluations\,\citep{DBM2019rodrigues}. {More information about DBM is available at \url{https://mespadoto.github.io/dbm/}.} \subsection[SDBM]{Supervised Decision Boundary Map (SDBM)} While flexible, the independent choice of $P$ and $P^{-1}$ in DBM means that these operations have to be constructed separately (including fine-tuning their hyperparameters), which incurs additional effort. Self-Supervised Neural Network Projection (SSNP)\,\citep{espadoto2021SSNP} alleviates this by constructing $P$ and $P^{-1}$ jointly. Briefly put, SSNP follows a classical autoencoder architecture but adds a classification loss atop the standard reconstruction loss. To minimize this classification loss, either true labels (supervised mode) or pseudo-labels (semi-supervised mode; labels are obtained by running a clustering algorithm on the feature space) can be used. The encoder part then delivers $P$, whereas the decoder delivers $P^{-1}$.

To minimize this classification loss, either true labels (supervised mode) or pseudo-labels (semi-supervised mode; labels are obtained by running a clustering algorithm on the feature space) can be used. The encoder part then delivers $P$, whereas the decoder delivers $P^{-1}$. Supervised Decision Boundary Map (SDBM)\,\citep{oliveiraSDBM2022} directly applies SSNP to construct decision maps following the workflow in Fig.~\ref{fig:illustration_dm}. Like DBM, SDBM is also simple to implement and has a similar speed. More interestingly, SDBM seems to produce smoother decision boundaries than DBM and these boundaries appear to better agree with ground-truth information on the visualized classifiers. The price to pay for this is the inability to choose a specific direct projection $P$. This can be suboptimal in cases where one has such a technique that is known to be best for depicting the structure of a given dataset. {The code of SDBM is available at \url{https://github.com/mespadoto/sdbm}.}

choose a specific direct projection $P$. This can be suboptimal in cases where one has such a technique that is known to be best for depicting the structure of a given dataset. {The code of SDBM is available at \url{https://github.com/mespadoto/sdbm}.} DeepView (DV)\,\citep{schulz2020DeepViewVisualizing} constructs the direct projection $P$ by using UMAP. However, in contrast to classical UMAP, the points' similarities are computed using a Fischer distance which combines their high-dimensional features with the classification function $f$. This approach, also called discriminative dimensionality reduction \citep{schulz2015Usingdiscriminative}, favors grouping points in the projection that are similar both feature-wise and in terms of classification by $f$. The inverse projection $P^{-1}$ of DV also uses UMAP, with the roles of input and output swapped. The inverse projection is next extrapolated to all 2D points by minimizing a Kullback-Leibler (KL) divergence that captures probabilities of closeness in both 2D and the data space. Once $P^{-1}$ is available, DV colors the 2D pixels following the same approach as DBM and SDBM (Fig.~\ref{fig:illustration_dm}, step 7).

2D points by minimizing a Kullback-Leibler (KL) divergence that captures probabilities of closeness in both 2D and the data space. Once $P^{-1}$ is available, DV colors the 2D pixels following the same approach as DBM and SDBM (Fig.~\ref{fig:illustration_dm}, step 7). While DV produces decision maps with smooth boundaries, the process is quite expensive, given that the computation of the Fisher distance is squared in the number of samples in $D'$. Also, the process involves optimizing several hyperparameters to construct an accurate $P^{-1}$. Just as DBM, the direct and inverse projections $P$ and $P^{-1}$ can be in principle freely chosen. However, given the aforementioned optimization complexity, we next use the exact original proposal in \citet{schulz2020DeepViewVisualizing} to construct both $P$ and $P^{-1}$. {The code of DV is available at \url{https://github.com/LucaHermes/DeepView}.} As indicated in \Cref{ch2:related}, the by far most evident issue of current decision map techniques is the very limited \emph{evaluation} they come with. DBM was evaluated qualitatively by using 28 projection techniques $P$ (and iLAMP for $P^{-1}$)

available at \url{https://github.com/LucaHermes/DeepView}.} As indicated in \Cref{ch2:related}, the by far most evident issue of current decision map techniques is the very limited \emph{evaluation} they come with. DBM was evaluated qualitatively by using 28 projection techniques $P$ (and iLAMP for $P^{-1}$) to conclude that UMAP and t-SNE are among the best options for $P$ for creating smooth decision boundaries\,\citep{DBM2019rodrigues}. However, this evaluation was purely \emph{qualitative}, \emph{i.e.}, based on visually examining the respective decision maps for smoothness. SDBM was evaluated on four classifiers and four real-world datasets and compared against DBM. However, as in the previous case, the comparison was purely qualitative, based on a visual assessment of the map's smoothness. Finally, DV was evaluated on two real-world datasets. Its quality was measured by computing two metrics related to our map accuracy and data consistency (discussed further in Sec.~\ref{sec:metrics}). However, it was not compared to any other decision map technique. We expand on all these aspects by our proposed evaluation method described next.

by computing two metrics related to our map accuracy and data consistency (discussed further in Sec.~\ref{sec:metrics}). However, it was not compared to any other decision map technique. We expand on all these aspects by our proposed evaluation method described next. As mentioned in Sec.~\ref{sec:rw_limitations}, current evaluations of decision map techniques are limited. Due to inevitable errors in the decision map creation process (detailed next in Sec.~\ref{sec:metrics}), using the classifier's accuracy to gauge a decision map's quality is neither appropriate nor comprehensive. Simple visual inspection of a decision map is equally limited in gauging its ability to correctly capture a classifier's behavior since we do not usually know this ground-truth behavior upfront. We aim to improve upon this by proposing an extensive set of both quantitative and qualitative evaluations, each characterizing a different desirable property of decision maps, as follows. To address these challenges, a more comprehensive evaluation of decision maps should consider several key aspects. \item A decision map should be able to `classify' the projections, assuming that the classifier is accurate (R1); \item A decision map should be consistent with the classifier, i.e., the classification results of the map should be the same as the classifier (R2);

map should be able to `classify' the projections, assuming that the classifier is accurate (R1); \item A decision map should be consistent with the classifier, i.e., the classification results of the map should be the same as the classifier (R2); \item A decision map should be smooth, i.e., a small change on the 2D map should not lead to a large change in original high-dimensional space (R3); \item A decision map should be computable efficiently (R4); \item A decision map should be stable in class labels, i.e., the (inverse) projection of error should not affect the prediction of the classifier (R5); \item Distances on the decision map should be representative, i.e., a point close to the decision boundary on the 2D map should also be close to the decision boundary in the original high-dimensional space; a point close to data distribution on the 2D map should also be close to the data distribution in the original high-dimensional space (R6). \caption{Illustration of the metrics used to evaluate decision maps. (see Sec.~\ref{sec:method_ch4})}

in the original high-dimensional space; a point close to data distribution on the 2D map should also be close to the data distribution in the original high-dimensional space (R6). \caption{Illustration of the metrics used to evaluate decision maps. (see Sec.~\ref{sec:method_ch4})} Global metrics aim to characterize the quality of a decision map by a single (scalar) value, much like metrics for direct projections such as trustworthiness, continuity, or normalized stress\,\citep{venna2006visualizing} (see also Eqn.~\ref{eqn:projmetrics} and related text). We propose five such metrics, as follows (see also \Cref{fig:illustration_metrics}). Note that we used the first three metrics in Sec.~\ref{sec:ch3_metrics} for selecting the best (classifier, SDBM) combinations for our geoscience application. In this chapter, we discuss these three metrics in further detail and also introduce additional metrics for decision map evaluation. We then use these metrics to evaluate three decision map techniques (DBM, SDBM, DV) with a broader set of classifiers and datasets than in our work in \Cref{ch3:geo_application}. \textbf{Classifier accuracy} $ACC_{C}$: Accuracy is one of the most common metrics for evaluating the performance of a classifier and is defined as the ratio of correct predictions made by the model $f$ to the total number of predictions, \emph{i.e.} {ACC}_{C} =

accuracy} $ACC_{C}$: Accuracy is one of the most common metrics for evaluating the performance of a classifier and is defined as the ratio of correct predictions made by the model $f$ to the total number of predictions, \emph{i.e.} {ACC}_{C} = \frac{ |\{ \mathbf{x}_i \in D \;|\; C(\mathbf{x}_i) = f(\mathbf{x}_i) \} | } { |D| }, where $ | \cdot | $ indicates set size, $D$ is the sample set used for evaluation, and $C(\mathbf{x}_i)$ is $\mathbf{x}_i$'s ground-label $y_i$ assigned by the trained model $f$. We will use these notations throughout this section when defining global metrics. In the following, we will set $D$ to either $D_t$ (training set) or $D_T$ (test set) and thereby compute the corresponding classifier accuracies which we denote as $ACC_C^t$ and $ACC_C^T$, respectively. $ACC_{C}$ ranges in $[0, 1]$, where $ACC_{C} = 1$ indicates perfect classification. While accuracy does not, as we already noted, gauge the quality of a decision map, it helps calibrate the understanding of subsequent metrics. For example, if we know a classifier is accurate, we expect its decision map to reflect this accordingly.

accuracy does not, as we already noted, gauge the quality of a decision map, it helps calibrate the understanding of subsequent metrics. For example, if we know a classifier is accurate, we expect its decision map to reflect this accordingly. \textbf{Map accuracy} ${ACC}_{M}$: We define map accuracy as the fraction of data points (of a given dataset $D$) that are drawn in the correct decision zones. We define map accuracy as {ACC}_{M} = \frac{ |\{ \mathbf{x}_i \in D \;|\; C(\mathbf{x}_i) = f(P^{-1}(P(\mathbf{x}_i))) \} | } { |D| }, Intuitively, this says that data points $\mathbf{x}_i$ for which we have ground-truth labels $y_i$ are indeed colored correctly (\emph{i.e.}, by $y_i$) in the computed decision map. As for class accuracy, we compute $ACC_M^t$ and $ACC_M^T$ for the training, respectively test, sets.

this says that data points $\mathbf{x}_i$ for which we have ground-truth labels $y_i$ are indeed colored correctly (\emph{i.e.}, by $y_i$) in the computed decision map. As for class accuracy, we compute $ACC_M^t$ and $ACC_M^T$ for the training, respectively test, sets. Note that map accuracy only evaluates the map pixels onto which the data in $D$ projects since, for all other pixels, we do not have ground truth labels. The range of ${ACC}_{M}$ is $[0, 1]$, where ${ACC}_{M} = 1$ tells that all data points are drawn on the pixels with the same color as their ground-truth labels. Another way to evaluate map accuracy is to employ an additional 2D classifier, as used in the DV evaluation (see $Q_{kNN}$ in\,\citep{schulz2020DeepViewVisualizing}). We do not use this option since we believe it introduces an additional degree of complexity in the selection and training of this additional classifier.

to employ an additional 2D classifier, as used in the DV evaluation (see $Q_{kNN}$ in\,\citep{schulz2020DeepViewVisualizing}). We do not use this option since we believe it introduces an additional degree of complexity in the selection and training of this additional classifier. \textbf{Data consistency} $Cons_{d}$: In general, both direct and inverse projections $P$ and $P^{-1}$ unavoidably introduce errors\,\citep{espadoto19,nonato18,Amorim2012ilamp,espadoto2019NNinv}. That is, in general, $P^{-1}$ is not an exact inverse of $P$, \emph{i.e.}, $P^{-1}(P(\mathbf{x})) \neq \mathbf{x}$ for several data points $\mathbf{x}$. Such errors can be evaluated by the mean square error $MSE = \sum_{\mathbf{x} \in D} \|\mathbf{x} - P^{-1}(P(\mathbf{x}))\|^2 / |D|$ computed over the set $D$\,\citep{espadoto2021unprojection}. However, for decision maps, MSE is not the most relevant metric to consider: A pixel $P(\mathbf{x})$ may be backprojected away from $\mathbf{x}$, \emph{i.e.}, the MSE may be nonzero; still, if the backprojection has the same label as the original point $\mathbf{x}$, then there is no visible error in the map. To account for this, we gauge whether the error introduced by the `round-trip' direct-and-inverse projection creates inconsistencies in the decision maps. For this, we define the projection consistency metric {Cons_{d}} = \frac{ | \{ \mathbf{x}_i \in D \;|\; f(P^{-1}(P(\mathbf{x}_i))) = f(\mathbf{x}_i) \} |}{ |D|}.

we gauge whether the error introduced by the `round-trip' direct-and-inverse projection creates inconsistencies in the decision maps. For this, we define the projection consistency metric {Cons_{d}} = \frac{ | \{ \mathbf{x}_i \in D \;|\; f(P^{-1}(P(\mathbf{x}_i))) = f(\mathbf{x}_i) \} |}{ |D|}. Simply put, $Cons_d$ measures the fraction of so-called consistent projections, \emph{i.e.}, samples $\mathbf{x}_i$ from a given set $D$ that keep the same classification label after one round-trip given by the projection $P$ and inverse-projection $P^{-1}$. This metric is also used in DV's evaluation under the name $Q_{data}$\,\citep{schulz2020DeepViewVisualizing}. As for class and map accuracy, we compute $Cons_d^t$ and $Cons_d^T$ for the training, respectively test, sets. Note that, in contrast to $ACC_C$ and $ACC_M$, $Cons_p$ does not use ground truth labels $y_i$. That is, consistency assesses only how much the decision map can represent a given classifier, and not whether the DBM is correct with respect to ground-truth labels. The range of $Cons_{d}$ is $[0, 1]$, where $Cons_{d} = 1$ indicates perfect consistency. \textbf{Map consistency} $Cons_{p}$: All above metrics evaluate a decision map only at locations where an actual data point in $D$ would project.

respect to ground-truth labels. The range of $Cons_{d}$ is $[0, 1]$, where $Cons_{d} = 1$ indicates perfect consistency. \textbf{Map consistency} $Cons_{p}$: All above metrics evaluate a decision map only at locations where an actual data point in $D$ would project. However, as already noted, most pixels in such a map are not covered by data points. We extend $Cons_d$ to cover all pixels in a map by {Cons_{p}} = \frac{ | \{ \mathbf{p} \in I \;|\; f(P^{-1}(P(P^{-1}(\mathbf{p})))) = f(P^{-1}(\mathbf{p})) \} | }{ |I|}, where $\mathbf{p}$ is a pixel of the decision map image $I$. $Cons_p$ calculates the fraction of consistent pixels in the decision map, that is, pixels whose corresponding data points (obtained by inverse projection $P^{-1}$) have the same classification label after a round-trip of projection $P$ and inverse-projection $P^{-1}$. Note that $Cons_p$ extends the idea of using the round-trip to visually evaluate an inverse projection\,\citep{espadoto2021unprojection} to a quantitative metric used to evaluate a decision map. $Cons_{p}$ ranges in $[0, 1]$, where $Cons_{p} = 1$ implies perfect pixel-level consistency in a decision map.

extends the idea of using the round-trip to visually evaluate an inverse projection\,\citep{espadoto2021unprojection} to a quantitative metric used to evaluate a decision map. $Cons_{p}$ ranges in $[0, 1]$, where $Cons_{p} = 1$ implies perfect pixel-level consistency in a decision map. \textbf{Class stability} $\bar{S}$: One application of decision maps concerns improving a classifier by, for instance, adding extra labeled training samples by backprojecting selected decision map pixels. In this case, \emph{multiple} round-trips between the 2D map space and data space occur. Since $P^{-1}$ is not an exact inverse of $P$, several such round-trips can increasingly accumulate errors. We measure this as follows. Given a pixel in the decision map, we apply $P^{-1}$ to obtain a data point and then apply the classifier to it. Next, we project this point to 2D and repeat the backprojection-and-labeling process until the class label changes or a maximum number of iterations (set to $k_{max} = 10$ in our experiments) is reached. Let $S$ be an image recording this maximum iteration count (normalized by $k_{max}$) that keeps the class label constant at every map pixel. We then define the class stability $\bar{S}$ as the average of $S$ over all pixels, with values in $[0,1]$. A value $\bar{S}$ close to 1 indicates a stable decision map with consistent class assignments through multiple $P$ and $P^{-1}$ round-trips. Conversely, a value $\bar{S}$ close to 0 suggests an unstable decision map, sensitive to distortions introduced by (inverse) projection, potentially misrepresenting the classifier's decisions. As explained next in Sec.~\ref{sec:visualizations}, we also directly visualize $S$ to get local insights in the class stability over the map.

to 0 suggests an unstable decision map, sensitive to distortions introduced by (inverse) projection, potentially misrepresenting the classifier's decisions. As explained next in Sec.~\ref{sec:visualizations}, we also directly visualize $S$ to get local insights in the class stability over the map. \textbf{Average gradient} $\bar{G}$: Since $P^{-1}$ is a function of two variables (the $x$ and $y$ coordinates of a pixel), one can measure its gradient magnitude $G$ at every pixel $\mathbf{p}$ by central differences\,\citep{espadoto2021unprojection} as G_x(\mathbf{p}) = \frac{P^{-1}(\mathbf{p} + (w,0)) - P^{-1}(\mathbf{p} - (w,0))}{2}, G_y(\mathbf{p}) = \frac{P^{-1}(\mathbf{p} + (0,w)) - P^{-1}(\mathbf{p} - (0,w))}{2}, G(\mathbf{p}) = \sqrt{\lVert G_x(\mathbf{p}) \rVert^2 + \lVert G_y(\mathbf{p}) \rVert^2}, where $w$ is a small step size, set to the size of one pixel for all our experiments. Large gradient values indicate pixels where the decision map has a high likelihood of being incorrect since neighboring pixels correspond to faraway data points, thus, data points can be classified differently. Separately, discontinuous changes in this map indicate areas where $P^{-1}$ is not smooth, where we likely expect to see errors in the decision map.

incorrect since neighboring pixels correspond to faraway data points, thus, data points can be classified differently. Separately, discontinuous changes in this map indicate areas where $P^{-1}$ is not smooth, where we likely expect to see errors in the decision map. We can reduce $G$ to a scalar value by computing the average $\bar{G}$ of the normalized values $G(\mathbf{p}) / G_{max}$ over all the decision map pixels, where $G_{max}$ is the maximal value of $G$ over the set of decision maps being compared. The value $1-\bar{G}$, ranging in $[0,1]$ thus signals how smooth a decision map is. Values close to 0 indicate a smooth map (with low average gradients). Values close to 1 indicate a map having many discontinuities -- thus, more prone to errors, as explained above.

$[0,1]$ thus signals how smooth a decision map is. Values close to 0 indicate a smooth map (with low average gradients). Values close to 1 indicate a map having many discontinuities -- thus, more prone to errors, as explained above. The metrics presented so far aggregate the quality of a decision map to a single scalar number. While simple to interpret, such metrics only give a \emph{global} assessment of the decision map. Since typical direct projections and, in any case, inverse projections are nonlinear functions, large errors can occur \emph{locally} in such maps. Such local errors -- if not too numerous -- will not show up in global metrics. Moreover, the \emph{position} of such local errors is very important for interpreting a decision map. For example, errors appearing close to a decision boundary will influence the shape of this boundary and, subsequently, how one uses the map to interpret and/or improve a given ML model.

errors is very important for interpreting a decision map. For example, errors appearing close to a decision boundary will influence the shape of this boundary and, subsequently, how one uses the map to interpret and/or improve a given ML model. To get more insight into such local phenomena, one can use so-called \emph{local metrics}. Introduced for studying direct projections\,\citep{aupetit07}, these metrics evaluate the quality of a map at every 2D spatial position and typically display the result as a color-coded visualization. We propose three local metrics to assess the quality of decision maps, as follows (see also \Cref{fig:illustration_metrics}): \textbf{Gradient map:} We display the gradient map $G$, computed as explained in Sec.~\ref{sec:metrics}, to help understand the smoothness of the inverse projection and, as outlined earlier, check for the presence of high-gradient regions which are prone to creating errors in the map. Figure~\ref{fig:gradmap} shows a typical gradient map and how to interpret it. Simply put, dark regions indicate areas of low gradients $G$, where the decision map is reliable; bright regions indicate areas of high gradients $G$, where the decision map is very likely unreliable.

typical gradient map and how to interpret it. Simply put, dark regions indicate areas of low gradients $G$, where the decision map is reliable; bright regions indicate areas of high gradients $G$, where the decision map is very likely unreliable. \textbf{Distance to boundary:} Due to the nonlinearity of the mappings $P$ and $P^{-1}$, if a pixel is visually close to a decision boundary \emph{in the map}, this does not necessarily mean that its corresponding data point is close to the classifier's decision boundary \emph{in the data space}. Yet, as already mentioned in \Cref{ch2:related}, a key aim of decision maps is precisely to indicate points which are close to decision boundaries, since these are prone to misclassifications upon slight changes in the data or model parameters. We alleviate this  by computing the distance to the boundary using DeepFool\,\citep{moosavi-dezfooli2016DeepFoolsimple}, as described by \citet{differentiableDBM}. An adversarial example for a given classifier is a synthetic data point $\tilde{\mathbf{x}} = \mathbf{x} + \Delta\mathbf{x}$ such that $f(\tilde{\mathbf{x}}) \neq f(\mathbf{x})$ with $\lVert \Delta\mathbf{x} \rVert$ as small as possible. {Here, $\Delta \mathbf{x}$ indicates a small perturbation, or change, of the sample $\mathbf{x}$.}

for a given classifier is a synthetic data point $\tilde{\mathbf{x}} = \mathbf{x} + \Delta\mathbf{x}$ such that $f(\tilde{\mathbf{x}}) \neq f(\mathbf{x})$ with $\lVert \Delta\mathbf{x} \rVert$ as small as possible. {Here, $\Delta \mathbf{x}$ indicates a small perturbation, or change, of the sample $\mathbf{x}$.} In practice, it is complicated to generate the smallest perturbation possible that generates an adversarial example, so DeepFool approximates it instead for every map pixel $\mathbf{p}$ as {d_B(\mathbf{p}) = \min_{\Delta\mathbf{x} \in \mathbb{R}^n} \big\{ \lVert \Delta \mathbf{x} \rVert _2 \;\big|\;f( P^{-1}(\mathbf{p})  + \Delta\mathbf{x}) \neq f(P^{-1}(\mathbf{p})) \big\},} In other words, $d_B(\mathbf{p})$ tells how close $P^{-1}(\mathbf{p})$ is to a change in the class predicted by $f$, \emph{i.e.}, which is the distance between the backprojection of $\mathbf{p}$ and the closest decision boundary in data space. Note that, to compute the above, a differentiable classifier is needed. For this, we use a Logistic Regression classifier, implemented in PyTorch. \textbf{Distance to data:} Besides knowing how close a decision-map pixel $\mathbf{p}$ is to its closest decision boundary, it is also useful to know how close such a pixel is (via backprojection) to the nearest data point in the training set $D_t$ used to construct $f$. Following \citet{differentiableDBM}, as introduced in \Cref{ch2:related} (Eqn.~\ref{eqn:dist_to_data_ch2}), we compute this as

boundary, it is also useful to know how close such a pixel is (via backprojection) to the nearest data point in the training set $D_t$ used to construct $f$. Following \citet{differentiableDBM}, as introduced in \Cref{ch2:related} (Eqn.~\ref{eqn:dist_to_data_ch2}), we compute this as d_D(\mathbf{p}) = \min_{\mathbf{x} \in D_t} \lVert P^{-1}(\mathbf{p}) - \mathbf{x} \rVert, that is, the smallest distance between the data point $P^{-1}(\mathbf{x})$ corresponding to the pixel $\mathbf{p}$ and samples in the training set $D_t$. Interpreting $d_D$ works as follows: Decision map pixels $\mathbf{p}$ which are close (in 2D) to projections $P(\mathbf{x})$ of training-set points $\mathbf{x} \in D_t$ should represent data points which are close to such $\mathbf{x}$. If this is not the case, \emph{i.e.}, if we see high $d_D(\mathbf{p})$ values for pixels close to the training-set projection, it means that the decision map has issues there with extrapolating from the training-set -- that is, it takes classifier values from points \emph{far} from the training-set and depicts them \emph{close} to the projection of the training-set.

the training-set projection, it means that the decision map has issues there with extrapolating from the training-set -- that is, it takes classifier values from points \emph{far} from the training-set and depicts them \emph{close} to the projection of the training-set. \textbf{Class stability map:} As outlined in \Cref{sec:metrics}, the class stability map $S$ can act as a local metric. As explained there, pixels with high $S$ values will have the same class label even after multiple round-trip $P$ and $P^{-1}$ iterations. When a pixel is close to a decision boundary, $S$ will likely be lower since the chance that a point there `jumps' on the other side of a decision boundary due to such round-trips increases. However, if the pixel is far from a decision boundary, its $S$ value should be higher. If this is not the case, then the decision map may be unreliable in such areas. As such, visualizing how $S$ matches the distances to the depicted boundaries in a decision map tells us about confident we are about the quality of that map. We evaluate the three decision-map techniques (DBM, SDBM, and DV) using both a synthetic dataset and real-world datasets.

the distances to the depicted boundaries in a decision map tells us about confident we are about the quality of that map. We evaluate the three decision-map techniques (DBM, SDBM, and DV) using both a synthetic dataset and real-world datasets. The real-world datasets are chosen following the same criteria as in \citet{oliveiraSDBM2022}. They are chosen to be openly accessible, representative of different types of data (e.g., time series, image, text), and to have different numbers of classes and dimensions. The datasets are listed as follows and summarized in Table \ref{tab:datasets_ch4}. \textbf{Synthetic Blobs}: This is a synthetic dataset with 5 classes, 100 dimensions, 1500 samples. All data points in a blob (following a Gaussian distribution) have the same label. Therefore, it is an easily classifiable dataset, for which we expect all three decision-map techniques to produce good-quality metric values. \textbf{Human Activity Recognition (HAR)}\,\citep{anguita2012human}: This a dataset with time-series data from smartphone sensors. The goal is to classify the type of physical activity (e.g., walking, climbing stairs) performed by the user. This dataset has 10299 samples, 561 dimensions, and 6 classes.

Activity Recognition (HAR)}\,\citep{anguita2012human}: This a dataset with time-series data from smartphone sensors. The goal is to classify the type of physical activity (e.g., walking, climbing stairs) performed by the user. This dataset has 10299 samples, 561 dimensions, and 6 classes. \textbf{MNIST}\,\citep{lecun2010MNISThandwritten}: This dataset is a collection of handwritten digits that is commonly used for training various image classification systems. The dataset contains 60,000 training images and 10,000 testing images. Each image is a 28x28 grayscale image, associated with a label from 0 to 9. The dataset is downsized to 10k for all our experiments. \textbf{FashionMNIST}\,\citep{xiao2017FashionMNISTnovel}: A dataset of Zalando's article pictures, with images of 10 fashion categories. It has the same size as MNIST, and is downsized to 10k samples for our experiments. \textbf{Reuters Newswire Dataset}\,\citep{Thom2017-reuters}: This dataset contains 8432 samples of news report documents, from which 5000 features were extracted using the standard TF-IDF\,\citep{salton1986Introductionmodern} text processing method. From the full dataset, we use only the 6 most frequent classes to favor the creation of easily-interpretable decision maps. \caption{Datasets used in our decision map evaluations and their properties.} \textbf{Dataset} & \textbf{Type} & \boldmath$|D_{t}|\unboldmath$ & \boldmath$|D_{T}|\unboldmath$ & \textbf{Dimensionality} & \textbf{No. of Classes} \\  \hline

use only the 6 most frequent classes to favor the creation of easily-interpretable decision maps. \caption{Datasets used in our decision map evaluations and their properties.} \textbf{Dataset} & \textbf{Type} & \boldmath$|D_{t}|\unboldmath$ & \boldmath$|D_{T}|\unboldmath$ & \textbf{Dimensionality} & \textbf{No. of Classes} \\ \hline Synthetic Blobs & Synthetic & 1000 & 500 & 100 & 5 \\ HAR & Time Series & 5000 & 2352 &561 & 6 \\ MNIST & Image & 5000 & 5000 &784 & 10 \\ Fashion MNIST & Image & 5000 & 5000& 784 & 10 \\ Reuters Newswire & Text & 5000 & 2432 & 5000 & 6 \\ We evaluate decision maps for four classifiers -- Logistic Regression\,\citep{cox1958Twofurther}, Random Forest (200 estimators)\,\citep{breiman2001RandomForests}, Neural Network (having 3 hidden layers with 200 units each), and Support Vector Machines (SVM, with an RBF kernel)\,\citep{cortes1995Supportvectornetworks} -- which are all extensively used in machine learning. They represent different families of algorithms: Logistic Regression is a linear classification model; Random Forest is an ensemble method; Neural Network represents deep learning; and SVM is a maximum margin classifier. Importantly, these classifiers are frequently studied in existing decision map research, thus allowing for meaningful comparisons. The four machine learning classifiers are implemented using \emph{scikit-learn}\,\citep{pedregosa2011ScikitlearnMachine}.

an ensemble method; Neural Network represents deep learning; and SVM is a maximum margin classifier. Importantly, these classifiers are frequently studied in existing decision map research, thus allowing for meaningful comparisons. The four machine learning classifiers are implemented using \emph{scikit-learn}\,\citep{pedregosa2011ScikitlearnMachine}. We train all four classifiers on 5000 samples from the four real-world datasets and use the remaining samples for testing. We construct the corresponding three decision maps for each combination. As a result, we get $4 \times 4 \times 3 = 48$ combinations of datasets, classifiers, and decision maps to study. We next discuss the results of our evaluation metrics for the constructed decision maps. \subsection{Global metrics of real-world datasets} \caption{Aggregated global metrics for each combination of decision map, classifier, and dataset. Herein, ${ACC_C}$, ${ACC_M}$, ${Cons_d}$, ${Cons_p}$, ${\bar{S}}$, $1-\bar{G}$ are the global metrics defined in Sec.~\ref{sec:metrics}.% Red dashes `-' show the highest values across each dataset (row). Top-left numbers in each plot give the average value of ${ACC_C}^T$, ${ACC_M}^T$, ${Cons_d}^T$, ${Cons_p}$, and ${\bar{S}}$. Bold values indicate the highest value along the dataset (row). }

in Sec.~\ref{sec:metrics}.% Red dashes `-' show the highest values across each dataset (row). Top-left numbers in each plot give the average value of ${ACC_C}^T$, ${ACC_M}^T$, ${Cons_d}^T$, ${Cons_p}$, and ${\bar{S}}$. Bold values indicate the highest value along the dataset (row). } \Cref{fig:metrics_barplot} shows all global metrics for all combinations of decision map technique, classifier, and dataset. Note that DV failed to run with SVM on the real-world datasets, and thus is not included in the figure. From a dataset perspective, the results differ heavily based on the specific dataset being considered. On HAR, SDBM and DV show comparable (high metric) results while DBM got slightly lower scores; on MNIST, DBM shows the best results in all aspects, even though DBM's training does not use label information. on FashionMNIST, SDBM gets the highest scores, while DV shows comparable results for data-level metrics ($ACC_C$, $ACC_M$, $Cons_d$) but much lower results on pixel-level metrics ($Cons_d$, $\bar{S}$); finally, on Reuters, the overall winner is DV. From a classifier perspective, the results are more stable. The accuracy of classifiers themselves varies little. The most noticeable difference is that random forests always have lower scores (on all global metrics except $ACC_C$ and $1-\bar{G}$) than the other classifiers for all considered metrics, particularly on MNIST dataset. For $1-\bar{G}$, almost all results are close to 1. This is due to a large maximum value of DV which influences the normalization (see the definition of $\bar{G}$ in Sec.~\ref{sec:metrics}). We explore this in more detail next in \Cref{sec:result_local_metrics}.

dataset. For $1-\bar{G}$, almost all results are close to 1. This is due to a large maximum value of DV which influences the normalization (see the definition of $\bar{G}$ in Sec.~\ref{sec:metrics}). We explore this in more detail next in \Cref{sec:result_local_metrics}. The datasets in \Cref{fig:metrics_barplot} are sorted top-to-bottom on increasing dimensionality (see also \Cref{tab:datasets_ch4}). The higher the dimensions, the more difficult for the models to learn. So, it is not surprising that DBM, which is a fully unsupervised method, has trouble when aiming to depict classifiers for higher-dimensional datasets like Reuters (5000 dimensions). In contrast, for this dataset, SDBM reaches higher quality metrics, while DV reaches the highest values. This suggests that DV is the decision map method of choice -- from the perspective of global quality metrics -- for high-dimensional datasets. However, as we shall see next, other aspects weigh the three decision-map methods differently. \subsection{Interpreting local metrics on synthetic data}

DV is the decision map method of choice -- from the perspective of global quality metrics -- for high-dimensional datasets. However, as we shall see next, other aspects weigh the three decision-map methods differently. \subsection{Interpreting local metrics on synthetic data} We start explaining the local metrics proposed in Sec.~\ref{sec:metrics} using the simple synthetic blobs dataset which is, as explained earlier, straightforward to classify. As such, we only study its decision maps for the simple Logistic Regression classifier (we obtained very similar results for the other three considered classifiers). The blobs dataset is split into 1000 training and 500 testing samples. All three map methods -- namely, DBM, SDBM, and DV -- achieve 100\% on the test set for all data-wise global metrics. We next use the blobs dataset to establish and validate our \emph{expectations} for a `good' decision map based on the local metrics introduced in \Cref{sec:visualizations} as follows:

DV -- achieve 100\% on the test set for all data-wise global metrics. We next use the blobs dataset to establish and validate our \emph{expectations} for a `good' decision map based on the local metrics introduced in \Cref{sec:visualizations} as follows: \item Distance to the nearest data $d_D$: We expect this distance to be small for pixels close to actual projections of data points and larger for pixels far away from these points. In other words, we expect that the 2D distance (to projections of data points) to mimic the nD distance (to actual data points). \item Distance to the decision boundary $d_B$: Similar to the above, we expect that points close to decision boundaries (in 2D) are also close to decision boundaries (in nD) and conversely. \item Class stability map $S$: Ideally, we would like to have most pixels with high stability values, especially close to decision boundaries (where we are most interested in studying a decision map). \item Gradient map $G$: We expect (1) a \emph{smooth} gradient map without any discontinuities or peaks. Ideally (2), we would also like to have low gradients close to the decision boundaries (for the same reason mentioned above for class stability).

\item Gradient map $G$: We expect (1) a \emph{smooth} gradient map without any discontinuities or peaks. Ideally (2), we would also like to have low gradients close to the decision boundaries (for the same reason mentioned above for class stability). \caption{Decision maps and local metrics of the synthetic blob dataset (see Sec.~\ref{sec:result_blob}). {Row 1: Decision map with data points projected onto it (marked black). The brightness of pixels encodes the confidence of $f$'s prediction (dark=low, bright=high confidence). Row 2: Distance of each map pixel to the nearest data point $d_D$ depicted using a blue (low distance) to yellow (high distance) colormap. Row 3: Distance of each map pixel to the closest decision boundary $d_B$, using the same colormap as in row 2. Row 4: Class stability map $S$ depicted using a red-white-blue colormap. Blue values indicate stable pixels, while red ones pixels for which fewer direct-and-inverse projection round-trips change the classifier's decision depicted at that pixel. Row 5: Gradient map $G$ showed using a rainbow colormap. Blue and red pixels indicate low, respectively high, values of the derivatives of the inverse projection function.} }

fewer direct-and-inverse projection round-trips change the classifier's decision depicted at that pixel. Row 5: Gradient map $G$ showed using a rainbow colormap. Blue and red pixels indicate low, respectively high, values of the derivatives of the inverse projection function.} } With these expectations in mind, we analyze the results shown in \Cref{fig:maps_blob} for the three decision-map methods and the abovementioned four local metrics. The first row in \Cref{fig:maps_blob} shows the decision maps and projected data points for the three methods. At a glance, all maps appear similar. However, a deeper analysis of the local metrics reveals more. The results for $d_D$ (second row) and $d_B$ (third row) meet our expectations across all methods. Some discontinuities in DV, however, are noticeable -- the $d_D$ and $S$ images appear to contain some `cuts' that perturb their overall smoothness.

more. The results for $d_D$ (second row) and $d_B$ (third row) meet our expectations across all methods. Some discontinuities in DV, however, are noticeable -- the $d_D$ and $S$ images appear to contain some `cuts' that perturb their overall smoothness. $S$ (fourth row) primarily manifests as expected, with all low-value pixels situated around the decision boundaries. A notable difference arises with SDBM showing larger regions of unstable pixels, indicating its vulnerability to projection errors. For $G$, DBM and SDBM are smooth, whereas DV exhibits peaks, thus not satisfying our first expectation of $G$. Even more interestingly, the local maxima for $G$ for DV takes the shape of \emph{lines} roughly connecting the clusters of projected points. As for our second expectation, we find a surprising result: DBM and SDBM show relatively higher values close to decision boundaries, while DV shows lower values in these areas. Consequently, no method satisfies both expectations of $G$ simultaneously.

projected points. As for our second expectation, we find a surprising result: DBM and SDBM show relatively higher values close to decision boundaries, while DV shows lower values in these areas. Consequently, no method satisfies both expectations of $G$ simultaneously. In conclusion, on the synthetic blob dataset, all three methods generally align with our expectations of local metrics. However, despite the decision maps' similarities for this simple example, there are subtle but significant differences between them. This is the first indication that, while superficially similar, the three studied decision map techniques behave quite differently. We examine this aspect further on real-world data. \subsection{Analyzing local metrics on real-world data} To refine our preliminary insights concerning the differences between the three studied decision-map techniques, we now use our four proposed local metrics to study their behavior on the four real-world datasets in Tab.~\ref{tab:datasets_ch4}. We start by presenting the actual decision maps (Sec.~\ref{sec:rw_dmaps}). The next sections (Sec~\ref{sec:rw_smooth}-\ref{sec:rw_dist_data}) interpret these maps using our four local metrics.

we now use our four proposed local metrics to study their behavior on the four real-world datasets in Tab.~\ref{tab:datasets_ch4}. We start by presenting the actual decision maps (Sec.~\ref{sec:rw_dmaps}). The next sections (Sec~\ref{sec:rw_smooth}-\ref{sec:rw_dist_data}) interpret these maps using our four local metrics. \Cref{fig:dm_train} shows the decision maps computed using training, respectively testing, data. The decision zones are color-coded categorically, while the brightness of pixels encodes the confidence of $f$'s prediction (as used earlier in all decision-map techniques). The projected samples are also color-coded according to their class, but made slightly brighter to distinguish them from their surrounding zones. Misclassified samples, \emph{i.e.}, samples for which $f(\mathbf{x}^i) \neq y^i$, are marked with a white outline.

in all decision-map techniques). The projected samples are also color-coded according to their class, but made slightly brighter to distinguish them from their surrounding zones. Misclassified samples, \emph{i.e.}, samples for which $f(\mathbf{x}^i) \neq y^i$, are marked with a white outline. {Theoretically, points near decision boundaries represent samples about which the classifiers are most uncertain. This can be observed at the boundaries between the pink and the yellow zone in the HAR dataset, where some misclassified points are highlighted with white outlines. Notably, there are even some misclassified light blue points at the boundaries of the pink and yellow zones. Knowing the nature of this dataset -- classification of human activities -- this observation aligns well with the understanding that static activities (corresponding to the class labels yellow and pink). are more challenging to distinguish than dynamic activities, such as various walking activities.} % In practice, there are projection errors \caption{Decision maps with training data (top) and test data (bottom).}

static activities (corresponding to the class labels yellow and pink). are more challenging to distinguish than dynamic activities, such as various walking activities.} % In practice, there are projection errors \caption{Decision maps with training data (top) and test data (bottom).} We see that each decision-map technique has its own `signature': DBM is more random; SDBM shows radial patterns; and DV presents blob-like patterns. Also, we see that prediction confidences are always lower near decision boundaries and higher within decision zones, which is expected.

each decision-map technique has its own `signature': DBM is more random; SDBM shows radial patterns; and DV presents blob-like patterns. Also, we see that prediction confidences are always lower near decision boundaries and higher within decision zones, which is expected. In more detail, DBM shows some `island' decision zones that have no data points. Without more information, it is hard to tell whether these islands actually do exist in the high-dimensional data or are artifacts of the decision map. In contrast, DV shows some discontinuities, indicated by `breaks' or `jumps' in the decision boundaries. Regarding the smoothness of decision boundaries, SDBM displays the smoothest ones; DBM has more complex but still smooth boundaries; and DV exhibits the least smoothness. The shapes of the decision zones seem to be strongly influenced by the underlying projection method $P$: SDBM consistently shows radial, elongated, star-like structures (a property known to autoencoders used for dimensionality reduction); DV presents well-separated, blob-like, structures (highly likely due to its use of discriminative dimensionality reduction); and DBM has more variable structures (due to its UMAP projection). It is worth noting that DBM failed on the Reuters dataset, as also reflected by the low global metrics score in \Cref{fig:metrics_barplot}.

to its use of discriminative dimensionality reduction); and DBM has more variable structures (due to its UMAP projection). It is worth noting that DBM failed on the Reuters dataset, as also reflected by the low global metrics score in \Cref{fig:metrics_barplot}. Separately, we examine the issue of overfitting. By displaying both training and test data, we can check whether decision maps can visually indicate overfitting. This overfitting is reflected by the noisier scatters plotted on incorrect background colors in the test sets (\Cref{fig:dm_train} bottom). For the test data, we observed overfitting in all three methods across all four datasets, except for the simplest one, the HAR dataset. Among the three methods, DV appears more prone to overfitting, a fact also evident from the noticeable difference between $ACC_M^t$ - $ACC_M^T$ and $Cons_d^t$ - $Cons_d^T$ in \Cref{fig:metrics_barplot}. \caption{Gradient maps $G$ of the studied decision map methods. Gradient values are scaled to [0,1] within each dataset. The number in the bottom right corner of each plot is the average $\bar{G}$ of each map.}

$Cons_d^t$ - $Cons_d^T$ in \Cref{fig:metrics_barplot}. \caption{Gradient maps $G$ of the studied decision map methods. Gradient values are scaled to [0,1] within each dataset. The number in the bottom right corner of each plot is the average $\bar{G}$ of each map.} The gradient map $G$, as defined in \Cref{eq:grad_map_ch4}, captures the smoothness of decision maps. \Cref{fig:gradient_maps} shows this map for the three studied decision-map techniques. We see that each technique creates its unique type of gradient, as follows. The DV technique shows high gradient values close to the projected points, which indicates a high degree of data compression during the projection $P$ in these areas. Additionally, the DV technique shows some high gradient `lines' connecting the clusters of projected points, indicating discontinuities. The remaining areas have uniformly low gradient values. In contrast, SDBM shows low gradient values almost everywhere, with slightly higher values in the gap areas between point clusters. Both the above patterns for DV and SDBM are similar to what we observed on the blob dataset in \Cref{fig:maps_blob}. Finally, DBM's gradient map presents a more complex pattern, entirely different from what we observed on the easily classifiable blob dataset (\Cref{fig:maps_blob}).

above patterns for DV and SDBM are similar to what we observed on the blob dataset in \Cref{fig:maps_blob}. Finally, DBM's gradient map presents a more complex pattern, entirely different from what we observed on the easily classifiable blob dataset (\Cref{fig:maps_blob}). As stated in \Cref{sec:result_blob}, we expect gradient maps to not show peaks and to have low values in areas close to decision boundaries. However, \emph{no} method simultaneously exhibits these two properties. Notably, SDBM's relatively high gradient values in the gap areas between point clusters are still low in absolute terms, making SDBM the method that best satisfies the two mentioned gradient-map properties. SDBM's high smoothness can be attributed to the joint training of $P$ and $P^{-1}$. DV is clearly the least smooth method, as indicated by the number of discontinuities reflected by the `peaks' in its gradient map. DBM falls in between, with its complex pattern showing less smoothness but at least continuity. As explained earlier in Sec.~\ref{sec:metrics}, the global average value for class stability $\bar{S}$ indicates whether a decision map is robust to (inverse) projection errors. The class stability map $S$, which we study next, reveals \emph{where} the decision map is susceptible to such errors.

in Sec.~\ref{sec:metrics}, the global average value for class stability $\bar{S}$ indicates whether a decision map is robust to (inverse) projection errors. The class stability map $S$, which we study next, reveals \emph{where} the decision map is susceptible to such errors. \Cref{fig:class_stability} shows the $S$ maps for the three studied decision-map techniques. As explained in \Cref{sec:result_blob}, pixels with low $S$ values are expected to appear primarily near decision boundaries. This assumption holds true, especially for simpler datasets such as the HAR dataset. \caption{Class stability map $S$. The number in the bottom right corner of each plot is $\bar{S}$ of each map.} However, some anomalies can be seen for each decision-map technique: DBM displays large zones with low $S$ values. Referring to the decision map (\Cref{fig:dm_train}), most of these zones are \emph{no data zones} (NDZs), in which no data points are projected -- see the zones circled by dotted green lines in \Cref{fig:class_stability} (leftmost image). Another interesting observation is that the most unstable pixels tend to change their labels \emph{immediately} after the first round-trip in DBM. This observation also applies to DV.

-- see the zones circled by dotted green lines in \Cref{fig:class_stability} (leftmost image). Another interesting observation is that the most unstable pixels tend to change their labels \emph{immediately} after the first round-trip in DBM. This observation also applies to DV. Unlike these two methods, SDBM continues to be affected by round-trip errors. This can be observed by the gradual changes in $S$ values for SDBM in \Cref{fig:class_stability}. After several rounds, most of the pixels in SDBM change their labels, except for those with the most robust labels, which almost never change. DV presents another salient pattern: Given the blob-like patterns in this decision map, a particular class often forms the `base' of the blob-like shapes in some cases, \emph{e.g.}, the MNIST dataset with Random Forest and Neural Networks. Pixels in these areas are more prone to round-trip errors. In summary, SDBM performs the worst on $S$, as also indicated by the result global metrics (\Cref{fig:metrics_barplot}) and the synthetic blob dataset (\Cref{fig:maps_blob}). DBM and DV show similar patterns $S$, with DBM slightly outperforming DV on the MNIST and FashionMNIST datasets. However, \emph{all} three methods are quite prone to instabilities and many such instabilities exist along their decision boundaries -- which, as explained earlier in Sec.~\ref{sec:result_blob}, is an undesirable aspect for trustworthy decision maps.

DV on the MNIST and FashionMNIST datasets. However, \emph{all} three methods are quite prone to instabilities and many such instabilities exist along their decision boundaries -- which, as explained earlier in Sec.~\ref{sec:result_blob}, is an undesirable aspect for trustworthy decision maps. \subsubsection{Distance to decision boundary}

and FashionMNIST datasets. However, \emph{all} three methods are quite prone to instabilities and many such instabilities exist along their decision boundaries -- which, as explained earlier in Sec.~\ref{sec:result_blob}, is an undesirable aspect for trustworthy decision maps. \subsubsection{Distance to decision boundary} A key application of decision maps is to show how close a point on the map is to the actual decision boundary of the studied classifier. For this, the 2D distances in the map should depict as closely as possible the corresponding nD distances. \Cref{fig:distance_maps} (left) shows, for each map pixel, its distance to its closest decision boundary in the high-dimensional space, computed via Eqn.~\ref{eqn:b}. From this figure, we see that our expectations align with the observations -- pixels close to the decision boundaries in the map are dark (meaning, they map points close to the actual decision boundaries in nD), while pixels deep in the decision zones are bright (meaning, they map points far away from the nD decision boundaries). Yet, we also see variation in these distances across the decision-map methods. SDBM and DV display patterns closer to the desired outcome than DBM. DV shows a wider area with low distance values, while SDBM shows a more concentrated area. This follows the patterns observed in gradient maps (\Cref{fig:gradient_maps}), where DV expands the gaps between point clusters, whereas SDBM compresses them. DBM exhibits a complex pattern, where certain zones display low distance values entirely, even for pixels located in the center of the decision zones. These zones are particularly noticeable in the upper areas of the HAR and FashionMNIST datasets, as well as the leftmost area of the MNIST dataset. Interestingly, these zones (circled by red dotted lines in \Cref{fig:distance_maps} left) are also NDZs, which coincide with the zones of low $S$ (see \Cref{fig:class_stability}). This correlation indicates reduced confidence in the inverse projection in those zones. The importance of confidence in extrapolation is underscored by these findings and will be discussed further in Sec.~\ref{sec:rw_dist_data}.

which coincide with the zones of low $S$ (see \Cref{fig:class_stability}). This correlation indicates reduced confidence in the inverse projection in those zones. The importance of confidence in extrapolation is underscored by these findings and will be discussed further in Sec.~\ref{sec:rw_dist_data}. \caption{Left: Distance to decision boundary $d_B$; Right: Distance to the nearest training data $d_D$. Projections of training data are shown as white dots.} \subsubsection{Distance to the nearest training data} The distance to the nearest data point $d_D$ (Eqn.~\ref{eqn:n}) indicates how far the inverse projection for a given pixel is from the actual data distribution. If an inverse projection is significantly far from this data distribution, t likely corresponds to a point where the classifier will have generalization difficulties. Ideally, a decision map should (1) not contain points which are far away from the actual training or testing points and (2) pixels close to these points in the map should actually also be close to the respective data points. \Cref{fig:distance_maps} (right) shows the distance $d_D$ for the three decision-map methods. Data points are marked as white dots.

testing points and (2) pixels close to these points in the map should actually also be close to the respective data points. \Cref{fig:distance_maps} (right) shows the distance $d_D$ for the three decision-map methods. Data points are marked as white dots. For DV, pixels near the data points exhibit low distance values, as expected. However, this pattern is less evident for DBM and SDBM. In the SDBM representation of the FashionMNIST dataset, a zone on the right side displays high distance values, even though a data point cluster is projected there. For DBM, an entire region in the HAR dataset map also shows very high distance values. Different from SDBM's case, this is an NDZ again, which also has low $S$ values and low $d_B$ in the entire zone (see \Cref{fig:class_stability} and \Cref{fig:distance_maps} (left) respectively). This high-value region, which is significantly distant from the data distribution, indicates that the inverse projections in this zone are unlikely to align with user expectations based on the class labels.

the entire zone (see \Cref{fig:class_stability} and \Cref{fig:distance_maps} (left) respectively). This high-value region, which is significantly distant from the data distribution, indicates that the inverse projections in this zone are unlikely to align with user expectations based on the class labels. {This might be due to the square shape of the 2D map. Depending on the distribution of the data, the inverse projection is likely not to populate the square region uniformly. In this case, the inverse projection has to extrapolate certain pixels which correspond to locations further away from the training data. This scenario underscores the value of the distance $d_D$ in offering insights into potential issues.} Another interesting observation is that the pattern of the $d_D$ metric is prone to outliers. For instance, we see some ripple-like patterns in HAR with DBM and DV. Although these patterns appear slightly different in each case, they are consistently caused by the presence of outliers. Other NDZs not showing high $d_D$ values might be caused by the same reason.

ripple-like patterns in HAR with DBM and DV. Although these patterns appear slightly different in each case, they are consistently caused by the presence of outliers. Other NDZs not showing high $d_D$ values might be caused by the same reason. {Furthermore, a closer examination of the data point locations (white dots) reveals that not all close decision-map pixels correspond to data \emph{samples} which are close to each other. That is, the 2D distance we see on the map is not the same as the high-dimensional distance we have in the data space.} In other words, decision-map pixels equally (and very) close to data points actually depict points at various distances from such data. This can lead to interpretation problems of the decision maps created by \emph{all} three methods -- more so for DBM and SDBM, where this pattern is more visible, and less for DV. We measured the training and inverse projection time of the three studied decision map techniques on an Intel Core i7-12700 CPU machine with 32GB of RAM and an NVIDIA GeForce RTX 3070 GPU with 8GB of RAM. For this, we used the synthetic blob dataset with varying dimensions (10-500) and numbers of samples (250-5000).

techniques on an Intel Core i7-12700 CPU machine with 32GB of RAM and an NVIDIA GeForce RTX 3070 GPU with 8GB of RAM. For this, we used the synthetic blob dataset with varying dimensions (10-500) and numbers of samples (250-5000). Particularly, we evaluated DV, which requires a pre-trained classifier, with Logistic Regression, Random Forest, Neural Network, and SVM. It is important to note that DV's training time varies based on the classifier used (as detailed further below).

of samples (250-5000). Particularly, we evaluated DV, which requires a pre-trained classifier, with Logistic Regression, Random Forest, Neural Network, and SVM. It is important to note that DV's training time varies based on the classifier used (as detailed further below). \Cref{fig:training_time} shows the training times for the compared methods (that is, the time needed to construct the functions $P$ and $P^{-1}$). When the number of samples $d_D$ and dimensions $n$ are both small ($N < 1000$, $n < 50$), the training time of DV (with Logistic Regression) is comparable to SDBM and DBM. For larger $n$ and/or $d_D$, comparable to the real-world datasets (\Cref{tab:datasets_ch4}), DBM and SDBM showed comparable training times. DV, however, had substantially higher training times. SDBM remained the fastest method, with average training times of less than 10 seconds across all tested scenarios. DBM's training time is affected by the number of samples $d_D$ but far less by the number of dimensions $n$. DV's training time, when using SVM, increased drastically for larger $n$ and/or $d_D$ -- training DV with SVM for $n=500$, $N=5000$ took over 2.7 hours. This steep increase made SVM-based DV experiments unfeasible for our real-world datasets.

of dimensions $n$. DV's training time, when using SVM, increased drastically for larger $n$ and/or $d_D$ -- training DV with SVM for $n=500$, $N=5000$ took over 2.7 hours. This steep increase made SVM-based DV experiments unfeasible for our real-world datasets. \Cref{fig:inv_proj_time} shows the time required to create the decision maps (given a trained direct and inverse projection $P$ and $P^{-1}$) for various numbers of dimensions $n$, samples $d_D$, and grid resolution $I$. As for training, DV took the longest and was heavily influenced by all these parameters, even failing to handle resolutions over $I=150$ pixels squared. Conversely, both DBM and SDBM had a relatively high and consistent performance, roughly linear in the number of pixels in the map and the number of samples $d_D$. Summarizing all the above, we conclude that DBM and SDBM are practical methods for creating decision maps for real-world datasets, while DV is not. \caption{Time to train $P$ and $P^{-1}$ pairs. Left: Training time of DBM, SDBM, and DV with Logistic Regression. Right: Training time of DV with 4 different classifiers.} \caption{Time to create decision maps for DBM, SDBM, and DV.} \subsection{Decision Maps for Deep Learning Variations}

$P$ and $P^{-1}$ pairs. Left: Training time of DBM, SDBM, and DV with Logistic Regression. Right: Training time of DV with 4 different classifiers.} \caption{Time to create decision maps for DBM, SDBM, and DV.} \subsection{Decision Maps for Deep Learning Variations} The previous evaluations have covered the way the three studied decision map methods -- DBM, SDBM, and DeepView -- perform over a wide range of classification models. However, in the respective study, we used a single such model based on a deep learning architecture (see Sec.~\ref{sec:classifiers}). It can be argued that, with the increasing prominence of deep learning, it is especially important to gather insights on how decision map methods perform on this particular family of algorithms. We cover this desiderate next by studying, separately, the decision maps created for four such deep learning architectures, as follows. First, we consider the original, and relatively simple, neural network architecture already used in the previous evaluations, for comparison purposes. This architecture has three fully-connected layers of 200 units each. Note that, albeit small, it is precisely the type of architecture used earlier in all deep-learning applications for computing direct projections\,\citep{espadoto2020Deeplearning}, inverse projections\,\citep{espadoto2021unprojection}, and decision maps\,\citep{oliveiraSDBM2022}.

evaluations, for comparison purposes. This architecture has three fully-connected layers of 200 units each. Note that, albeit small, it is precisely the type of architecture used earlier in all deep-learning applications for computing direct projections\,\citep{espadoto2020Deeplearning}, inverse projections\,\citep{espadoto2021unprojection}, and decision maps\,\citep{oliveiraSDBM2022}. We then used two larger variants of this architecture having four layers of 1024 units, respectively four layers of 2048 units. Finally, we used TabNet \citep{arik2021tabnet}, a very recent architecture designed for tabular data, which combines the principles of decision trees and neural networks and uses attention mechanisms \citep{vaswani2017Attentionall} to prioritize important features during decision-making. We did not include in this comparison more specialized architectures like Convolutional Neural Networks (CNN) \citep{lecun1989backpropagation}, Recurrent Neural Networks (RNN) \citep{elman1990finding}, Long Short-Term Memory (LSTM) networks\,\citep{hochtreiter97}, or Deep Belief Networks (DBMs)\,\citep{dbn}. While these architectures are in themselves quite powerful, they are most often designed for being used with one particular type of data, \emph{e.g.}, images or time-dependent signals. \caption{{Label related metrics for different neural networks. NN $m*n$ denotes a neural network with $m$ hidden layers of $n$ units each.}}

they are most often designed for being used with one particular type of data, \emph{e.g.}, images or time-dependent signals. \caption{{Label related metrics for different neural networks. NN $m*n$ denotes a neural network with $m$ hidden layers of $n$ units each.}} \Cref{fig:NNs_metrics} shows the performance metrics for three decision map techniques trained to classify four datasets using the above-mentioned four deep learning architectures. We see that, across the four deep learning techniques -- that is, within one set of four-colored bars in the respective plots -- is quite similar. The large differences which occur are, rather, dependent on the dataset or metric, much as we have seen for the earlier-evaluated architectures (see Fig.~\ref{fig:metrics_barplot}). The only outliner in this respect is TabNet when run on the Reuters dataset to compute the SDBM map and, up to a much lesser extent, when run on the FashionMNIST dataset to compute the DBM map. Let us examine these two situations separately next.

respect is TabNet when run on the Reuters dataset to compute the SDBM map and, up to a much lesser extent, when run on the FashionMNIST dataset to compute the DBM map. Let us examine these two situations separately next. In the latter case, TabNet has low $ACC_C$ but relatively higher $Cons_d$ and $Cons_p$ values. This demonstrates the situation when the classifier $f$ performs poorly and the decision map has good quality. This is a very good example of the use of DBMs -- the maps can serve as a reliable indicator of the classifier's under-performance. In the former case, TabNet shows clearly lower scores in all metrics. If we look at the actual decision maps (\Cref{fig:NNs_DM}), we see that these are, for all four architectures, overall quite similar for the same dataset. Moreover, these maps look quite different than those of the other classifiers depicted in Figs.~\ref{fig:maps_blob} to~\ref{fig:gradient_maps} -- apart from neural network classifier to the two sets of images, of course.

all four architectures, overall quite similar for the same dataset. Moreover, these maps look quite different than those of the other classifiers depicted in Figs.~\ref{fig:maps_blob} to~\ref{fig:gradient_maps} -- apart from neural network classifier to the two sets of images, of course. In other words, the variability of decision maps is much smaller over one type of architecture (neural networks) than across different architectures, which is expectable. However, \Cref{fig:NNs_DM} also shows some subtle differences between the maps, in the middle of the depicted data clusters, that is, close to the locations where the decision boundaries appear. This indicates that the four studied deep learning classifiers are, indeed, behaving slightly differently in the most uncertain areas. Also, we see that, the more complex a model is, the more complex its decision boundaries are. For example, in the case of SDBM-HAR, the decision boundary along the green zone is relatively simple for the NN $3 * 200$ model. For the NN $ 4 * 1024$ model, the dark blue intersects between the green zone and the other zones (pink and yellow). Further, for the most complex model NN $4 * 2048$, the pink zone thrusts into the middle of the green zone and the light blue zone. Finally, with TabNet, the decision boundary of the green zone becomes rugged, and the whole decision boundaries become more complicated. This can be explained by the fact that the more complicated the classifier is, the more chance it will overfit the given training data. While this aspect is known in machine learning, the fact that we can show it directly using decision maps has, to our knowledge, not been done earlier.

the classifier is, the more chance it will overfit the given training data. While this aspect is known in machine learning, the fact that we can show it directly using decision maps has, to our knowledge, not been done earlier. Summarizing the above, our findings highlight the consistent performance of decision map methods across different datasets and classifiers. The absence of a clear best method for computing decision maps underscores the importance of a comprehensive selection workflow for decision maps. We propose such a workflow for choosing the decision map method in the next section (see Sec.~\ref{sec:workflow}). \caption{{Decision maps for different neural networks. See Sec.~\ref{sec:dec_maps_nn}.}} \subsection{Workflow to guide the selection of decision map techniques} Putting together all the results of our evaluation, we see that there is no clear winner among the studied decision map techniques. When we consider the global quality metrics (Sec.~\ref{sec:metrics}), we found that each method reached its maximal quality on different datasets. Surprisingly, the unsupervised method (DBM) sometimes scored higher than the supervised ones (SDBM and DV) on the MNIST dataset. However, DBM's failure on the Reuters dataset indicates that more challenging datasets may still require some level of supervision.

different datasets. Surprisingly, the unsupervised method (DBM) sometimes scored higher than the supervised ones (SDBM and DV) on the MNIST dataset. However, DBM's failure on the Reuters dataset indicates that more challenging datasets may still require some level of supervision. The results of the local metrics bring more insights that lead us to the no-winner conclusion. For the synthetic dataset, all methods showed quite similar patterns, except for DV's poor smoothness and SDBM's slightly worse class stability. For real-world datasets, the patterns are less distinct. Yet, some trends can be discerned. The smoothness of DV is consistently low, irrespective of the simplicity or complexity of the datasets. Additionally, DV and SDBM consistently show more representative distances ($d_B$ and $d_D$) than DBM. Concerning speed, SDBM and SBM are clear winners -- they definitely surpass DV in both training and inference (map construction) time by one up to three orders of magnitude, which makes the latter not suitable for creating decision maps in real-world application scenarios, especially when using more time-consuming classifiers such as SVM.

DV in both training and inference (map construction) time by one up to three orders of magnitude, which makes the latter not suitable for creating decision maps in real-world application scenarios, especially when using more time-consuming classifiers such as SVM. Compared to SDBM and DV, DBM has a separate advantage. As outlined in \Cref{sec:background1}, DBM allows one to use any chosen direct projection function $P$ to create decision maps. This can be important in cases where one knows that a given $P$ is optimal, either for quantitative reasons or because it creates projections which are easier to interpret by users. In conclusion, since there is no clear winner, the choice of the decision map method should be guided by the specific requirements and constraints of individual use-cases. We capture this by proposing a workflow for choosing a method to create decision maps as illustrated in \Cref{fig:workflow}. Given a specific dataset and a set of potential classifier candidates, the workflow assists users in making the choices following the steps below:

this by proposing a workflow for choosing a method to create decision maps as illustrated in \Cref{fig:workflow}. Given a specific dataset and a set of potential classifier candidates, the workflow assists users in making the choices following the steps below: \item If the user has already chosen a projection function $P$, they should select DBM, as this is the only method that can accommodate a predefined $P$, and proceed to step 4. \item If the user does not have a specific $P$, the next key aspect to consider is computational efficiency. If speed is important and the data to be visualized is large, DV should be excluded from consideration, and the workflow proceeds to step 4. \item If the user does not have a specific $P$, and computational efficiency is not a concern, one should consider if smoothness is important. If yes, DV should be excluded, and one can proceed to step 4. \item If more than a single classifier-decision map combination remains to choose from, global quality metrics can be used to select the optimal one.

smoothness is important. If yes, DV should be excluded, and one can proceed to step 4. \item If more than a single classifier-decision map combination remains to choose from, global quality metrics can be used to select the optimal one. {The key metrics to use here are $ACC_C$, $ACC_M$, and $Cons_d$ (defined in Sec.~\ref{sec:metrics}), which can be computed for any combination of direct projection, inverse projection, and classifier. Note that $Cons_p$ and $\bar{S}$ are not available when the selected projection $P$ cannot infer new data, such as for the case of non-parametric, non-out-of-sample, projections like t-SNE.} \item Finally, visualizations of local metrics can be used to gain more trust and/or understanding of the behavior of the chosen decision map, {such as described in detail in the scenarios in Sec.~\ref{sec:results_ch4}.} \caption{Workflow for choosing the most suitable decision map in a given user application context. } \subsection{What decision maps really are}

and/or understanding of the behavior of the chosen decision map, {such as described in detail in the scenarios in Sec.~\ref{sec:results_ch4}.} \caption{Workflow for choosing the most suitable decision map in a given user application context. } \subsection{What decision maps really are} A key observation running through all our experiments so far is that decision maps are imperfect instruments that map a \emph{part} of the high-dimensional space of a classification model to a 2D surface or map. While our various metrics have shown differences between the three evaluated decision-map methods, it is still unclear how these 2D maps are created. To gain more insight in this, we consider a simple experiment: We create a three-dimensional dataset having 6 concentrated blobs (data point clusters), each of a separate class. Next, we use the three studied techniques to construct the respective decision maps for a given classifier (note that the classifier choice is not important next). Finally, we backproject the map pixels to the data space, which is three-dimensional in our case, and directly visualize the obtained set of 3D points. \Cref{fig:demo3d}a-c show these `backprojected' decision maps for DBM, SDBM, and DV, with colors mapping the six classes.

backproject the map pixels to the data space, which is three-dimensional in our case, and directly visualize the obtained set of 3D points. \Cref{fig:demo3d}a-c show these `backprojected' decision maps for DBM, SDBM, and DV, with colors mapping the six classes. Surprisingly, in all cases, these zones appear as residing on a \emph{surface}, whereas, knowing the structure of the underlying dataset, they should actually be \emph{volumetric} zones that partition the 3D space into six regions corresponding to the six class labels. In other words, the evaluated DBM techniques only choose a very specific two-dimensional surface-like subspace $Z'$ of the entire data space to visualize. For clarity, note that this surface $Z'$ is not the same as the actual decision \emph{boundaries} of the studied classifier. These boundaries cannot be directly shown by the studied DBM visualizations. Rather, only their \emph{intersections} with the artificial surface $Z'$ are shown as the curves that separate different-color patches in $Z'$.

as the actual decision \emph{boundaries} of the studied classifier. These boundaries cannot be directly shown by the studied DBM visualizations. Rather, only their \emph{intersections} with the artificial surface $Z'$ are shown as the curves that separate different-color patches in $Z'$. Image (d) illustrates this for the DBM map shown in image (a). Here, we sketch how the decision zones of three classes (yellow, blue, and purple) would arguably look like in 3D. As said, these are volumetric objects that enclose the training samples of their three respective classes. The border between the yellow and blue zones is the decision boundary $B_{yb}$ between these classes, which is a \emph{surface}. However, only the curve-like intersection $B_{yb} \cap Z'$ of this surface, indicated by the black curve in the figure, is shown by the DBM. Similarly, the border between the yellow and purple zones is the decision boundary $B_{yp}$ between these two classes. However, in this case, the DBM does not show anything, since the surface $Z'$ it constructs does not reach to that area of the 3D space, \emph{i.e.}, since $B_{yb} \cap Z' = \emptyset$ (dotted black curve in the figure). This is due to the finite size of the 2D image that these methods construct.

it constructs does not reach to that area of the 3D space, \emph{i.e.}, since $B_{yb} \cap Z' = \emptyset$ (dotted black curve in the figure). This is due to the finite size of the 2D image that these methods construct. Figure~\ref{fig:demo3d}e summarizes the above by a simpler, lower-dimensional, 2D sketch (all quantities in Fig.~\ref{fig:demo3d}d thus become one dimension lower). We see here the decision zones (2D yellow and pink surfaces), actual decision boundary $B_{yb}$ (1D curve), surface $Z'$ constructed by the DBM method (1D curve), and the part of the decision boundary that a DBM method can depict ($B_{yb} \cap Z'$, 0D point). As stated earlier, DBM methods only visualize a \emph{subset} (0D point in this sketch) of the actual decision boundaries (1D curves in this sketch).

part of the decision boundary that a DBM method can depict ($B_{yb} \cap Z'$, 0D point). As stated earlier, DBM methods only visualize a \emph{subset} (0D point in this sketch) of the actual decision boundaries (1D curves in this sketch). Summarizing our findings: (1) the way that a DBM method constructs the surface $Z'$ will strongly influence which parts of the actual decision zones of a classifier will be offered for visualization; (2) only a part of the actual decision boundaries of a classifier are visualized by DBM methods; and (3) different DBM methods will produce different decision map visualizations for the same dataset and classifier -- therefore, potentially leading to different interpretations. To our knowledge, none of these three findings have been outlined by earlier work on decision maps.

(3) different DBM methods will produce different decision map visualizations for the same dataset and classifier -- therefore, potentially leading to different interpretations. To our knowledge, none of these three findings have been outlined by earlier work on decision maps. A final observation from Fig.~\ref{fig:demo3d} is that the above-mentioned surfaces $Z'$ appear to \emph{smoothly} connect the samples $D$ used by the direct projection $P$ that go into generating the inverse projection $P^{-1}$. Intuitively put, they look like tension surfaces\,\citep{colding2006Shapesembedded} that pass close to samples in $D$. This further suggests that, if the data to classify $Z$ lives in high dimensions on a surface, and if $D$ closely samples this surface, DBM methods will work predictably well and, also, deliver similar results -- so, the choice of the DBM method to use is less relevant. Conversely, if $D$ contains points that cannot be fit along such a surface -- in other words, the sampled phenomenon has intrinsic dimensionality higher than two -- DBM methods may generate very different results depending on the actual dataset and DBM algorithm. This matches our earlier observation concerning the challenge of DBM methods to `squeeze' a high-dimensional space into a 2D image. Designing more refined DBM algorithms that offer users a way to control which part of the high-dimensional space they sample to construct their classifier explanations is, thus, an important direction for future work (see next Sec.~\ref{sec:dr_for_ml}).

into a 2D image. Designing more refined DBM algorithms that offer users a way to control which part of the high-dimensional space they sample to construct their classifier explanations is, thus, an important direction for future work (see next Sec.~\ref{sec:dr_for_ml}). \caption{Decision map methods construct and visualize the classifier (Logistic Regression) only over an implicitly-constructed \emph{surface} embedded in the high-dimensional space (see Sec.~\ref{sec:surfaces}). (a) DBM\,\citep{DBM2019rodrigues}. (b) SDBM\,\citep{oliveiraSDBM2022}; (c) DeepView\,\citep{schulz2020DeepViewVisualizing}. (d) Limitations of decision map visualizations. (e) 2D sketch of (d). We discuss the implications of this important observation in more detail in Chapter~\ref{ch5:surface_like_behavior}.} {We single out two limitations of our current work:}

Sec.~\ref{sec:surfaces}). (a) DBM\,\citep{DBM2019rodrigues}. (b) SDBM\,\citep{oliveiraSDBM2022}; (c) DeepView\,\citep{schulz2020DeepViewVisualizing}. (d) Limitations of decision map visualizations. (e) 2D sketch of (d). We discuss the implications of this important observation in more detail in Chapter~\ref{ch5:surface_like_behavior}.} {We single out two limitations of our current work:} {\textbf{Decision maps:} While they significantly simplify understanding how a trained ML model works, decision maps require quite some effort to become \emph{actionable} -- that is, lead to concrete insights that explain and/or improve the working of a given ML model. In other words, it is still challenging for users to make decisions using decision-maps. Our work helps partially in this direction by helping users to make decision about (a) how and where they trust a given decision map and (b) which decision map technique to next use in practice in a given context. Improvements can, and should, be done to the understandability of decision map visualizations, \emph{e.g.}, explain which parts of a given training set, training process, or model are responsible for visible patterns in such a map. Such extensions are not the scope of this work but are very promising directions for future work.}

visualizations, \emph{e.g.}, explain which parts of a given training set, training process, or model are responsible for visible patterns in such a map. Such extensions are not the scope of this work but are very promising directions for future work.} {\textbf{Evaluation:} Our evaluation, covering only five datasets and seven classifiers, is necessarily limited in its generalizability. More models and architectures exist in machine learning. How the studied three decision map techniques (DBM, SDBM, and DeepView) cope with these models is not necessarily the same as for the models we studied so far, so our evaluation can be extended by considering additional models. However, we argue our choice for our \emph{initial} evaluation consisting of the aforementioned datasets and classifiers as follows: (1) As no similar evaluation of decision maps existed before our work, we had to start with relatively simple cases -- that is, datasets and classifiers with known behavior. This way, we could use these datasets and classifiers as `ground truth' to actually assess the produced decision maps. (2) Our limited evaluation already pointed out several key insights such as the very limited computational scalability of DeepView and the fact that \emph{all} decision map methods only visualize a surface subset that passes close to the training samples in the data space. These limitations will exist for any more complex model visualized by the current methods. As such, future work can already focus on removing these limitations before applying decision map techniques to more complex models.}

training samples in the data space. These limitations will exist for any more complex model visualized by the current methods. As such, future work can already focus on removing these limitations before applying decision map techniques to more complex models.} In this chapter, we have presented a framework for exploring and comparing methods for constructing decision maps for visualizing the behavior of general-purpose classifiers of high-dimensional data. To this end, our framework proposes six global metrics and four local metrics to gauge the overall quality, respectively the local quality, of a decision map. We validated our framework by applying it to a simple synthetic dataset for which the expected behavior of the decision map constructed by three state-of-the-art decision map techniques (DBM, SDBM, DV) was known. Furthermore, we compared these three techniques for a combination of four real-world datasets and four classifiers.

simple synthetic dataset for which the expected behavior of the decision map constructed by three state-of-the-art decision map techniques (DBM, SDBM, DV) was known. Furthermore, we compared these three techniques for a combination of four real-world datasets and four classifiers. Our results showed that there is no decision map method from the evaluated ones that consistently scores better than its competitors in \emph{all} aspects deemed relevant for quality. Furthermore, we outlined that all the studied decision map methods have inherent limitations in various quality aspects and that these limitations can fluctuate quite significantly as a function of the studied dataset and/or classifier being explored. To aid users in choosing a suitable decision map method in a practical setting, we proposed a workflow that considers all studied quality aspects and proceeds by elimination and next optimization of these aspects.

studied dataset and/or classifier being explored. To aid users in choosing a suitable decision map method in a practical setting, we proposed a workflow that considers all studied quality aspects and proceeds by elimination and next optimization of these aspects. Separately, we showed that all studied decision maps have an inherent, previously unknown, limitation -- they only visualize a surface from the entire high-dimensional space. The way this surface is constructed depends on the actual decision map technique. As a consequence, the decision map visualization reflects both the actual decision boundaries in the data and the way these intersect with the surface constructed implicitly by the decision map technique. However, this surface-like behavior is only observed on that 3D datasets with a logistic regression as the classifier. We further investigate this issue in \Cref{ch5:surface_like_behavior}.

and the way these intersect with the surface constructed implicitly by the decision map technique. However, this surface-like behavior is only observed on that 3D datasets with a logistic regression as the classifier. We further investigate this issue in \Cref{ch5:surface_like_behavior}. Several directions of future work exist, as follows. First, our evaluation can be extended by considering more datasets and classifiers and, when these will appear, more decision map techniques. Second, our finding that decision maps actually visualize a single surface from the high-dimensional space can turn this inherent limitation of decision map techniques to a strength. We can imagine ways to parameterize this implicit surface under user control so as to let it `slice' through the actual high-dimensional decision boundaries in an interactive way, thereby offering the user the possibility to examine these decision boundaries in a more controlled, and global, way. \Cref{ch6:controllable_Pinv} will explore this parameterization direction. \chapter{Fundamental Limitations of Decision Maps}

`slice' through the actual high-dimensional decision boundaries in an interactive way, thereby offering the user the possibility to examine these decision boundaries in a more controlled, and global, way. \Cref{ch6:controllable_Pinv} will explore this parameterization direction. \chapter{Fundamental Limitations of Decision Maps} Chapter~\ref{ch4:metrics_decision_map} (Sec.~\ref{sec:surfaces}) explored the behavior of decision maps and their related inverse projection techniques. In particular, an experiment we performed during this evaluation showed that, for a linear regressor model trained on a synthetic 3D dataset, all three techniques we examined, \emph{i.e.}, DBM\,\citep{DBM2019rodrigues}, SDBM\,\citep{oliveiraSDBM2022}, and DeepView\,\citep{schulz2020DeepViewVisualizing}, created essentially smooth \emph{surfaces} that interpolate between the 3D samples (see Fig.~\ref{fig:demo3d}). Practically, this means that decision maps created by these methods only show the model's behavior on a \emph{small} 2D subset of the entire 3D data space the model works on, namely the aforementioned surfaces. How the model behaves on samples away from these surfaces is not shown.

by these methods only show the model's behavior on a \emph{small} 2D subset of the entire 3D data space the model works on, namely the aforementioned surfaces. How the model behaves on samples away from these surfaces is not shown. In this chapter, we conduct an in-depth investigation about the coverage question of decision maps -- thereby answering our research question \textbf{RQ2} introduced in \Cref{ch1:intro}). Our results show that this 2D behavior is intrinsic to all decision maps that we area ware of and is independent on the studied classifier and dataset\footnote{This chapter is based on the papers ``Fundamental Limitations of Inverse Projections and Decision Maps''\,\citep{wang2024FundamentalLimitations} and ``Investigating Desirable Properties of Inverse Projections and Decision Maps''\,\citep{wang2025InvestigatingDesirable}}. To further explore the surface-like behavior of inverse projections observed in the simple experiment in \Cref{ch4:metrics_decision_map}, we aim to answer several questions: \item[Q1] How do decision maps look like for different ML models than the one used in \Cref{fig:demo3d}? %%% chatper 4 \item[Q2] How do the boundaries we see in Figure \ref{fig:demo3d} relate to the actual decision boundaries of a classifier? \item[Q3] Which parts of the data space do decision maps cover for data spaces having more than three dimensions?

chatper 4 \item[Q2] How do the boundaries we see in Figure \ref{fig:demo3d} relate to the actual decision boundaries of a classifier? \item[Q3] Which parts of the data space do decision maps cover for data spaces having more than three dimensions? \item[Q4] How do different inverse projection techniques influence the answers obtained for Q1-Q3? \item[Q5] How is the smoothness of the backprojection influenced by the direct-and-inverse projection technique choice? To answer Q1-Q4, we study the behavior of DBM (which uses the NNInv inverse projection\,\citep{espadoto2019NNinv}), SDBM, DeepView, and a few additional direct projection and inverse projection pairs, on a combination of several datasets (of varying dimensionality) and classification models. Next, we propose a way to measure how far an inverse projection can produce structures away from a single surface using intrinsic dimension estimation\,\citep{bennett1969intrinsicdimensionality,camastra2003Datadimensionality,campadelli2015Intrinsicdimension,bac2021ScikitDimensionPython}. Jointly put, our findings show that, for all datasets, all studied techniques essentially create surface like structures -- with varying local smoothness -- when mapping all points of the 2D space, except for points very close to the ones created by the direct projection (Q1-Q3).

that, for all datasets, all studied techniques essentially create surface like structures -- with varying local smoothness -- when mapping all points of the 2D space, except for points very close to the ones created by the direct projection (Q1-Q3). Answering Q4 is important for practitioners aiming to choose which (inverse) projections to use for any of the aforementioned applications (imputation, morphing, decision maps). In this chapter, we address this by studying not only NNInv, SDBM and DeepView, but also several additional combinations of direct projections (UMAP\,\citep{UMAP2018}, MDS\,\citep{borg2005Modernmultidimensional}) and inverse projections (iLAMP\,\citep{Amorim2012ilamp}, RBF\,\citep{amorim2015Facinghighdimensions}, and iMDS\,\citep{invertingMDS2024EuroVA}). Answering Q5 is important as a smooth backprojection is essential for interactive applications such as morphing where users want that small changes of a selected 2D point yield only small changes of the inferred data sample\,\citep{Amorim2012ilamp}. Conversely, a backprojection that aims to cover as much as possible from the data space will be more effective in \emph{e.g.} creating decision maps that capture more of a ML model's behavior, but will be likely less smooth. Understanding the inverse projection's smoothness is thus important for users to make informed choices for such applications.

will be more effective in \emph{e.g.} creating decision maps that capture more of a ML model's behavior, but will be likely less smooth. Understanding the inverse projection's smoothness is thus important for users to make informed choices for such applications. The structure of this chapter is as follows. Section~\ref{sec:background_ch5} introduces related work. Section~\ref{sec:vis_eval} presents our results for 3D datasets, for which direct visual evaluation can be used to answer our questions. Section~\ref{sec:ID_eval} extends our evaluation with new methods that address high-dimensional data. Section~\ref{sec:discussion_ch5} discusses our findings. Finally, Section~\ref{sec:conclusion_ch5} concludes this chapter. We recap some the notations and concepts further used in this chapter to ease the reading. Let $\mathbf{x} \in \mathbb{R}^n$ be an $n$-dimensional sample or data point and $D = \{\mathbf{x}_j\}$, $j = 1, 2, \dots, N$,  a dataset of $N$ such samples. A \emph{classification model} (or classifier) is a function f: \mathbb{R}^n \rightarrow C that maps a sample $\mathbf{x}$ to a label $f(\mathbf{x})$ in a given label-set $C$. A \emph{decision zone} of $f$, for class $y \in C$, is the point set $\{ \mathbf{x} \in \mathbb{R}^n | f(\mathbf{x}) = y \}$; the boundaries separating decision zones are called the \emph{decision (hyper)surfaces} of $f$.

in a given label-set $C$. A \emph{decision zone} of $f$, for class $y \in C$, is the point set $\{ \mathbf{x} \in \mathbb{R}^n | f(\mathbf{x}) = y \}$; the boundaries separating decision zones are called the \emph{decision (hyper)surfaces} of $f$. A \emph{projection}, also called dimensionality reduction (DR), is a function P : \mathbb{R}^n \rightarrow \mathbb{R}^q, where $q \ll n$. We further use the term projection to denote both the operation $P$ and also its output $P(D)$ for a given input $D$, depending on the context. For visualization purposes, one typically uses $q=2$. An \emph{inverse projection}, or unprojection, is a function P^{-1}: \mathbb{R}^q \rightarrow \mathbb{R}^n, which aims to reverse the mapping of a given projection $P$ for a given dataset $D$. } \emph{Decision maps}, introduced in Sec.~\ref{sec:dbm_ch2}, aim to construct dense visualizations of a trained ML model $f$. We refer to Sec.~\ref{sec:dbm_ch2} and Sec.~\ref{sec:dm_workflow} for a detailed description of how such maps are computed, including notations used in the process. We further denote the set I^{-1} = \{ P^{-1}(\mathbf{p}) | \mathbf{p} \in I \}

trained ML model $f$. We refer to Sec.~\ref{sec:dbm_ch2} and Sec.~\ref{sec:dm_workflow} for a detailed description of how such maps are computed, including notations used in the process. We further denote the set I^{-1} = \{ P^{-1}(\mathbf{p}) | \mathbf{p} \in I \} of all pixels $\mathbf{p}$ of an decision map image $I$ which get mapped via $P^{-1}$ to the data space as the \emph{backprojection} of the decision map. The surfaces in Fig.~\ref{fig:demo3d} are examples hereof. As such, Q3 and Q5 (see Sec.~\ref{sec:introduction_ch5}) relate to how much of the data space does $I^{-1}$ cover, respectively how smooth is $I^{-1}$. Similarly, we denote the set P^{-1}(D) = \{ P^{-1}(P(\mathbf{x})) | \mathbf{x} \in D \} as the backprojection of the dataset $D$. As explained in Sec.~\ref{sec:dbm_ch2}, decision maps can be constructed using any inverse projection $P^{-1}$, which is in turn suitably constructed from any direct projection $P$. However, only a limited number of $(P, P^{-1})$ combinations have been used to this end, as follows. \cite{ivapp19} tested 28 projection techniques $P$ with iLAMP as the inverse projection to construct decision maps and concluded that t-SNE and UMAP were the best choices for $P$.

of $(P, P^{-1})$ combinations have been used to this end, as follows. \cite{ivapp19} tested 28 projection techniques $P$ with iLAMP as the inverse projection to construct decision maps and concluded that t-SNE and UMAP were the best choices for $P$. Following this, DBM\,\citep{rodrigues2018Imagebasedvisualization} used UMAP\,\citep{UMAP2018} or t-SNE\,\citep{tSNEMaaten2008} for $P$ and NNinv\,\citep{espadoto2019NNinv} for $P^{-1}$. \cite{espadoto2019NNinv} used UMAP and t-SNE for direct projection while using iLAMP or RBF for the inverse one. SDBM\,\citep{oliveiraSDBM2022} uses SSNP which, as already mentioned in \Cref{ch2:related} and \Cref{ch4:metrics_decision_map}, provides both $P$ and $P^{-1}$. DeepView\,\citep{schulz2020DeepViewVisualizing} leverages discriminative dimensionality reduction\,\citep{schulz2015Usingdiscriminative} to enhance the direct projection UMAP\,\citep{UMAP2018}, which also provides an inverse projection. Finally, the recent inverse projection iMDS\,\citep{invertingMDS2024EuroVA} can also potentially be used to construct decision maps if $P$ is set to multidimensional scaling (MDS, \cite{torgerson52}). In this chapter, we investigate the coverage and smoothness of decision maps constructed by the all the above mentioned methods/combinations. \section{Visual evaluation on 3D data} To answer questions Q1-Q4 introduced in Sec.~\ref{sec:introduction_ch5}, we first extend the visual evaluation for 3D datasets in Fig.~\ref{fig:demo3d} to use more inverse projection techniques and more classifiers. We next evaluate these techniques on high-dimensional data (Sec.~\ref{sec:ID_eval}).

evaluation on 3D data} To answer questions Q1-Q4 introduced in Sec.~\ref{sec:introduction_ch5}, we first extend the visual evaluation for 3D datasets in Fig.~\ref{fig:demo3d} to use more inverse projection techniques and more classifiers. We next evaluate these techniques on high-dimensional data (Sec.~\ref{sec:ID_eval}). \textbf{Dataset:} We conduct this evaluation using the well-known three-class Iris flower dataset\,\citep{fisher1988IrisPlants}. As explained in~\Cref{ch2:related}, the key idea of visual evaluation is to directly \emph{draw} the backprojected decision maps $I^{-1}$ and see how these actually cover the data space of the trained ML model they are supposed to depict. To be able to create such visualizations, we need our dataset to have maximally $n=3$ dimensions. We achieve this by restricting the Iris dataset to its last three features.

space of the trained ML model they are supposed to depict. To be able to create such visualizations, we need our dataset to have maximally $n=3$ dimensions. We achieve this by restricting the Iris dataset to its last three features. {\textbf{Decision map methods:} Besides the three decision map methods used in Fig.~\ref{fig:demo3d}, \emph{i.e.}, DBM\,\citep{rodrigues2018Imagebasedvisualization}, SDBM\,\citep{oliveiraSDBM2022}, and DeepView\,\citep{schulz2020DeepViewVisualizing}, we also consider three additional inverse projection techniques: iLAMP\,\citep{Amorim2012ilamp}, RBF\,\citep{espadoto2019NNinv}, and iMDS\,\citep{invertingMDS2024EuroVA}. For all methods except SDBM and DeepView (which use their own direct projection techniques, see Sec.~\ref{sec:background_ch5}), we now use MDS as direct projection. This is because (a) one of the considered inverse projections, iMDS, only works with MDS as direct projection; and (b) this setting allows us to minimize the number of direct projection techniques we use and thus provide a more intuitive comparison.}

is because (a) one of the considered inverse projections, iMDS, only works with MDS as direct projection; and (b) this setting allows us to minimize the number of direct projection techniques we use and thus provide a more intuitive comparison.} \textbf{Classifiers:} We study the behavior of decision maps using six classifiers: Logistic Regression\,\citep{cox1958Twofurther}, Support Vector Machines \,\cite[SVM]{cortes1995Supportvectornetworks}, Random Forests\,\citep{breiman2001RandomForests}, Neural Networks, Decision Trees, and K-Nearest Neighbors (KNN). All are implemented using Scikit-Learn\,\citep{pedregosa2011ScikitlearnMachine} with default parameters, except Neural Networks, which uses three hidden layers each with 256 units. For each classifier, we not only construct the backprojected decision maps $I^{-1}$ (see Sec.~\ref{sec:background_ch5}) for the six studied decision map techniques, but also visualize the actual decision boundaries in the 3D data space. {Figure~\ref{fig:IRIS3d_3class} (top two rows) shows the backprojected decision maps, each from two different viewpoints (for better interpretation), for the six studied direct-and-inverse projection combinations (columns). For ease of interpretation of the results, we use here only the simple Logistic Regression classifier. The corresponding 2D decision maps are shown in Fig.~\ref{fig:IRIS3d_3class} (bottom row). This preliminary investigation already reveals  several interesting facts.}

studied direct-and-inverse projection combinations (columns). For ease of interpretation of the results, we use here only the simple Logistic Regression classifier. The corresponding 2D decision maps are shown in Fig.~\ref{fig:IRIS3d_3class} (bottom row). This preliminary investigation already reveals several interesting facts.} {Firstly, we see that the backprojected decision maps for the first three methods (DBM, SDBM, DeepView) have very similar smooth-surface-like shapes as the ones shown in Fig.~\ref{fig:demo3d} for the synthetic blobs dataset. The backprojected surfaces of DBM and SDBM are quite smooth and, as such, cannot get very close to (all) the actual data points; In contrast, DeepView creates a much more noisy surface which `connects' the data points better. This is also observed in the actual 2D decision maps (bottom row in Fig.~\ref{fig:IRIS3d_3class}): The maps for DBM and SDBM have far smoother decision boundaries than the DeepView map. This tells us that DBM and SDBM can depict the classifier's behavior \emph{further} from the training set (extrapolation), whereas DeepView shows this behavior \emph{close to and inside} this set (interpolation).

SDBM have far smoother decision boundaries than the DeepView map. This tells us that DBM and SDBM can depict the classifier's behavior \emph{further} from the training set (extrapolation), whereas DeepView shows this behavior \emph{close to and inside} this set (interpolation). Further on, we see that the backprojected decision maps for MDS+iLAMP, MDS+RBF, and MDS+iMDS behave very differently from the first three techniques. The latter two generate decision maps and backprojections which are very similar to each other and also quite close to a planar surface. Slight differences exist though: MDS+RBF creates a quite smooth backprojection that strictly passes through every sample $\mathbf{x}$, \emph{i.e.}, $P^{-1}(P(\mathbf{x})) = \mathbf{x}$ for all $\mathbf{x} \in D$. In contrast, MDS+iMDS creates a noisier backprojection that does not strictly pass through the data samples. The most noticeable outlier is the result of MDS+iLAMP. It shows the appearance of a `triangle soup' that exhibits practically no smoothness. In contrast, its backprojection covers far more of the 3D data space than all other compared methods. It is worth mentioning here that such discontinuities, originating from the iLAMP inverse projection, is precisely why the iLAMP authors next proposed the RBF inverse projection which is continuous and smooth\,\citep{amorim2015Facinghighdimensions}.}

the 3D data space than all other compared methods. It is worth mentioning here that such discontinuities, originating from the iLAMP inverse projection, is precisely why the iLAMP authors next proposed the RBF inverse projection which is continuous and smooth\,\citep{amorim2015Facinghighdimensions}.} \caption{Top two rows: Backprojections of the decision maps constructed by 6 inverse projection techniques (columns) with Logistic Regression on IRIS dataset, viewed from two viewpoints. Bottom row: Corresponding (2D) decision maps.} \caption{Decision maps (a) backprojected in 3D; (b) original in 2D of six classifiers, modified Iris dataset, computed by six techniques. The decision zones are yellow, respectively purple; the decision surface separating them is beige. The shaded surfaces are the backprojected decision maps.}

(a) backprojected in 3D; (b) original in 2D of six classifiers, modified Iris dataset, computed by six techniques. The decision zones are yellow, respectively purple; the decision surface separating them is beige. The shaded surfaces are the backprojected decision maps.} {We now extend the findings obtained so far using the Linear Regressor classifier to all six classifiers mentioned in Sec.~\ref{sec:more_models}. At the same time, we extend the visual exploration used in Fig.~\ref{fig:IRIS3d_3class} to show not only the backprojections $I^{-1}$ but also the actual decision zones and decision surfaces. As this creates quite complex imagery, we now restrict the Iris dataset to its last two classes. This will decrease the amount of colors we need to use in our visualizations to two. Note that these two classes are not fully linearly separable, which makes our classification task more challenging than the synthetic blob dataset used in Fig.~\ref{fig:demo3d}.}

will decrease the amount of colors we need to use in our visualizations to two. Note that these two classes are not fully linearly separable, which makes our classification task more challenging than the synthetic blob dataset used in Fig.~\ref{fig:demo3d}.} {Figure~\ref{fig:DMvsCLF}a shows the actual decision zones and decision boundaries and the backprojected decision maps for the six classifiers mentioned in Sec.~\ref{sec:more_models} and the same six decision map methods already explored in Fig.~\ref{fig:IRIS3d_3class}. The actual 2D decision maps are shown in Fig.~\ref{fig:DMvsCLF}b.} We further study the differences between the backprojected decision map and the \emph{actual} decision zones and surfaces as follows. We sample the 3D data space on a voxel grid of size $100^3$ (to limit computational effort); compute, for each voxel $\mathbf{v}$, the predicted class $f(\mathbf{v})$, and color code it; and draw this color-coded volume half-transparently (Fig.~\ref{fig:DMvsCLF}a, bottom 6 rows). {The yellow, respectively purple, volumes are thus the \emph{actual} decision zones of $f$. For clarity, we show these volumes, without the backprojection $I^{-1}$, in the leftmost column in Fig.~\ref{fig:DMvsCLF}a.} Also, we draw the actual decision boundary $\mathcal{S}$ that separates the two decision zones in beige -- see Fig.~\ref{fig:DMvsCLF}a, leftmost column, top cell for an example.

we show these volumes, without the backprojection $I^{-1}$, in the leftmost column in Fig.~\ref{fig:DMvsCLF}a.} Also, we draw the actual decision boundary $\mathcal{S}$ that separates the two decision zones in beige -- see Fig.~\ref{fig:DMvsCLF}a, leftmost column, top cell for an example. {Figure~\ref{fig:DMvsCLF} leads us to several insights. First, we see that the backprojected decision maps $I^{-1}$ (shaded surfaces in Fig.~\ref{fig:DMvsCLF}a, top row), \emph{i.e.}, the part of the data space that a decision map visualizes, are roughly orthogonal to, and intersecting, the actual decision surfaces (pale brown in Fig.~\ref{fig:DMvsCLF}a). That is, the boundaries which we \emph{see} in a decision map (curves where yellow meets purple in Fig.~\ref{fig:DMvsCLF}b) are the intersection $\mathcal{S} \cap I^{-1}$. Separately, if we scan a column in Fig.~\ref{fig:DMvsCLF}a, we see that the backprojections $I^{-1}$ are the same -- or almost the same in the case of

map (curves where yellow meets purple in Fig.~\ref{fig:DMvsCLF}b) are the intersection $\mathcal{S} \cap I^{-1}$. Separately, if we scan a column in Fig.~\ref{fig:DMvsCLF}a, we see that the backprojections $I^{-1}$ are the same -- or almost the same in the case of MDS+iMDS and DeepView (the reason for this is discussed separately below). However, as the classifiers change (rows), the decision boundaries change -- see Fig.~\ref{fig:DMvsCLF}a, leftmost column. Hence, the \emph{intersection} of $I^{-1}$ with the actual decision boundary will change. As mentioned, this intersection is precisely what we see as color boundaries in a 2D decision map. So, if the backprojection $I^{-1}$ is not smooth, this intersection can change \emph{significantly}, even when the visualized classifier model changes only slightly. Simply put, this means that decision maps whose backprojections look `crumpled' (non-smooth) can be very unstable and thus unsuited for practical use. Even stronger: the non-smoothness of the backprojection can create the false impression that a classifier has complex decision maps. Take for instance Logistic Regression, SVM, or Neural Networks; these classifiers show very smooth decision boundaries (Fig.~\ref{fig:DMvsCLF}b, leftmost column). However, their 2D decision maps created with DeepView or MDS+iLAMP show complex, non-smooth boundaries, which is clearly misleading. All in all, the above insights argue in favor of \emph{e.g.} (S)DBM and MDS+RBF as techniques for creating decision maps and definitely against DeepView and MDS+iLAMP.}

maps created with DeepView or MDS+iLAMP show complex, non-smooth boundaries, which is clearly misleading. All in all, the above insights argue in favor of \emph{e.g.} (S)DBM and MDS+RBF as techniques for creating decision maps and definitely against DeepView and MDS+iLAMP.} Secondly, we see that no decision map technique can actually depict the \emph{full} decision boundaries of any classifier. For example, the linear decision boundary of Logistic Regression is not well captured, {except by MDS+iMDS. The other decision maps show non-linear boundaries or even disconnected decision zones, see \emph{e.g.} DeepView and MDS+iLAMP. Another example is for Decision Trees. We see that the actual decision zone (purple) is split into two disconnected components (top and bottom purple cubes (Fig.~\ref{fig:DMvsCLF}a, leftmost column)). However, none of the tested decision map techniques shows two such separated purple decision zones (Fig.~\ref{fig:DMvsCLF}b).}

Decision Trees. We see that the actual decision zone (purple) is split into two disconnected components (top and bottom purple cubes (Fig.~\ref{fig:DMvsCLF}a, leftmost column)). However, none of the tested decision map techniques shows two such separated purple decision zones (Fig.~\ref{fig:DMvsCLF}b).} Finally, let us revisit the issue of the backprojection shapes generated by a given technique. DBM, SDBM, iLAMP, and RBF produce exactly the same shapes regardless of the classifier they depict -- indeed, their $P$ and $P^{-1}$ do not depend on the classifier. In contrast, MDS+iMDS and DeepView can generate (slightly) different shapes for different classifiers, for different reasons, as follows. By design, DeepView uses discriminative dimensionality reduction\,\citep{schulz2015Usingdiscriminative}, so its $P$ depends on $f$. {As for the reason why MDS+iMDS has different shapes for rows, this is because iMDS uses \emph{random} selections of samples to compute its $P^{-1}$. While one can argue that DeepView's design shows more information on $f$, \emph{controlling} how DeepView's decision maps actually sample the data space as a function of $f$ is unclear. As such, we believe that the approaches of (S)DBM, iLAMP and RBF where this sampling only depends on the training set, are more intuitive and stable.} \section{Evaluation on high dimensional data}

data space as a function of $f$ is unclear. As such, we believe that the approaches of (S)DBM, iLAMP and RBF where this sampling only depends on the training set, are more intuitive and stable.} \section{Evaluation on high dimensional data} We next conduct a quantitative evaluation of inverse projections and decision maps using high-dimensional datasets for all the six inverse projection techniques listed in Sec.~\ref{sec:more_models}. Additionally, we substitute UMAP for MDS when using it in combination with the inverse projection techniques NNInv, RBF, and iLAMP, as UMAP is far better suited to handle high-dimensional data than MDS. We also present additional quantitative measurements that gauge the quality of the studied inverse projections and corresponding decision maps, as well as a visual exploration of the smoothness of the studied inverse projection techniques.

to handle high-dimensional data than MDS. We also present additional quantitative measurements that gauge the quality of the studied inverse projections and corresponding decision maps, as well as a visual exploration of the smoothness of the studied inverse projection techniques. Since decision maps fundamentally depend on inverse projections, it makes sense to first and foremost quantify the quality of $P^{-1}$. Further on, for $n>3$ dimensional data, we cannot directly \emph{draw} the backprojected images $I^{-1}$, as already mentioned in Sec.~\ref{sec:more_models}. Recall now our question Q3 (Sec.~\ref{sec:introduction_ch5}). To answer it, we measure how far $I^{-1}$ is, locally, from a two-dimensional manifold embedded in $\mathbb{R}^n$. For this, we use intrinsic dimensionality (ID) estimation\,\citep{bac2021ScikitDimensionPython} with a linear method, \emph{i.e.}, Principal Component Analysis (PCA), due to its intuitiveness, computational efficiency, ease of use, and popularity\,\citep{espadoto19,bac2021ScikitDimensionPython,tian2021Usingmultiple}. Finally, we use the gradient map technique \citep{espadoto2021unprojection} to get insights into the decision maps' smoothness. All these steps are detailed further below.

method, \emph{i.e.}, Principal Component Analysis (PCA), due to its intuitiveness, computational efficiency, ease of use, and popularity\,\citep{espadoto19,bac2021ScikitDimensionPython,tian2021Usingmultiple}. Finally, we use the gradient map technique \citep{espadoto2021unprojection} to get insights into the decision maps' smoothness. All these steps are detailed further below. We use five synthetic and real-world datasets, all having $N=5000$ samples (Tab.~\ref{tab:lID_dataset}). The synthetic datasets, with dimensionality $n$ of 10, 30, and 100, consist of each of $C=10$ isotropic Gaussian blobs. Using isotropic blobs ensures that the ID is the same as the dimension count $n$ for these datasets. As real-world datasets, we use HAR\,\citep{anguita2012human} and MNIST\,\citep{lecun2010MNISThandwritten}. The intrinsic dimensionality of these datasets has been estimated by prior work\,\citep{el2016modeling,facco2017Estimatingintrinsic,aumuller2019RoleLocal,bahadur2019DimensionEstimation}. We use Logistic Regression as an example classifier $f$. Note that this does not affect the ID estimation, as $f$ is not involved in the construction of $P^{-1}$. \caption{Datasets used for ID estimation. For each dataset, we list the provenance, dimensionality $n$, sample count $N$, and class count $|C|$.} \textbf{Dataset} & \textbf{$n$} & \textbf{$N$} & \textbf{$|C|$}  \\ \hline Blobs 10D (synthetic)        & 10                            & 5000                         & 10                  \\ Blobs 30D (synthetic)        & 30                            & 5000                         & 10                  \\ Blobs 100D (synthetic)       & 100                           & 5000                         & 10                  \\

$|C|$.} \textbf{Dataset} & \textbf{$n$} & \textbf{$N$} & \textbf{$|C|$} \\ \hline Blobs 10D (synthetic) & 10 & 5000 & 10 \\ Blobs 30D (synthetic) & 30 & 5000 & 10 \\ Blobs 100D (synthetic) & 100 & 5000 & 10 \\ HAR\,\citep{anguita2012human} & 561 & 5000 & 6 \\ MNIST\,\citep{lecun2010MNISThandwritten}            & 784                & 5000                         & 10                  \\ \subsubsection{Error of the inverse projection} {We measure the quality of an inverse projection $P^{-1}$ for a given dataset $D$ and its projection $P(D)$ by the mean squared error (MSE) of the data backprojection $D'=P^{-1}(P(D))$ which is defined as} {   MSE = \frac{1}{|D|} \sum_{\mathbf{x} \in D} \| \mathbf{x} - P^{-1}(P(\mathbf{x})) \|^2.} An ideal inverse projection $P^{-1}$ should yield $P^{-1}(P(\mathbf{x})) = \mathbf{x}$ for all $\mathbf{x} \in D$, \emph{i.e.}, have zero $MSE$, which is the same as saying that $D' = D$\,\citep{espadoto2019NNinv}. Conversely, if this error is large, then the inverse projection is likely poor and will lead to misleading or even meaningless decision maps.

$\mathbf{x} \in D$, \emph{i.e.}, have zero $MSE$, which is the same as saying that $D' = D$\,\citep{espadoto2019NNinv}. Conversely, if this error is large, then the inverse projection is likely poor and will lead to misleading or even meaningless decision maps. Let $X$ be a dataset in $\mathbb{R}^n$ with $S(\mathbf{x})$ being its $k$ nearest neighbors in $X$. Let $\bm{\lambda} = (\lambda_1, \lambda_2, \dots, \lambda_n)$ be the $n$ eigenvalues of $S(\mathbf{x})$'s covariance matrix, sorted decreasingly. A common way to define the ID of $S(\mathbf{x})$ as the smallest $d$ value so that the sum of the first $d$ eigenvalues is larger than a given threshold $\theta$, where $\theta$ was set to a value close to 1, typically 0.95\,\citep{jolliffe02,fan2010Intrinsicdimension,tian2021Usingmultiple}. This method is also known under the name \emph{total variance}\,\citep{tian2021Usingmultiple}.

$d$ value so that the sum of the first $d$ eigenvalues is larger than a given threshold $\theta$, where $\theta$ was set to a value close to 1, typically 0.95\,\citep{jolliffe02,fan2010Intrinsicdimension,tian2021Usingmultiple}. This method is also known under the name \emph{total variance}\,\citep{tian2021Usingmultiple}. When using the total variance method for computing $d_i$, we found that, in the case of iLAMP (an inverse projection method they didn't study but we do), sometimes the first two eigenvalues capture a significant portion of the variance (\emph{e.g.,} 85\%); however, to arrive at 95\%, one would need a large number of additional eigenvalues (\emph{e.g.}, over 500 on MNIST dataset), each contributing less than 1\% to the total variance. Obviously, this is not desirable, as it would highly overestimate the intrinsic dimensionality. To cope with this, we adopted the alternative definition of intrinsic dimensionality known as \emph{minimal variance} which solves precisely this problem\,\citep{tian2021Usingmultiple} -- that is, we define ID as the number of eigenvalues each accounting for at least $\theta$ percent of the data variance, where $\theta$ is set typically to a small value.

known as \emph{minimal variance} which solves precisely this problem\,\citep{tian2021Usingmultiple} -- that is, we define ID as the number of eigenvalues each accounting for at least $\theta$ percent of the data variance, where $\theta$ is set typically to a small value. Algorithm~\ref{alg:local_id} shows our computation of ID values. We set $\theta=0.01$, thereby identifying the principal components that capture more than 1\% of the total variance as significant for the intrinsic dimensionality. The size $k$ of the local neighborhood $S(\mathbf{x})$ needs careful setting. A too large $k$ leads to overestimating the local ID. Conversely, too small $k$ values lead to noisy estimations. Note that $d+1$ independent vectors are required to span $d$ dimensions, so $k$ should be at least equal to the actual ID of $S(\mathbf{x})$\,\citep{verveer1995evaluationintrinsic}. We have ID estimations ranging from 13 to 33 for MNIST\,\citep{facco2017Estimatingintrinsic,aumuller2019RoleLocal,bahadur2019DimensionEstimation}; and from 15 to 61 for HAR, depending on the estimation method\,\citep{el2016modeling}; our synthetic datasets have known ID values of 10, 30, and 100 (see Tab.~\ref{tab:lID_dataset}). To cover all the above cases, we globally set $k=120$. \KwData{$X$, set of data points in $\mathbb{R}^n$ (can be $D$, $D'$, or $I^{-1}$); neighborhood size $k=120$; threshold $\theta=0.01$}

datasets have known ID values of 10, 30, and 100 (see Tab.~\ref{tab:lID_dataset}). To cover all the above cases, we globally set $k=120$. \KwData{$X$, set of data points in $\mathbb{R}^n$ (can be $D$, $D'$, or $I^{-1}$); neighborhood size $k=120$; threshold $\theta=0.01$} \KwResult{$\bar{d}$, the estimated ID of $X$ (average among all local neighborhoods)} Find the $k$ nearest neighbors $S(\mathbf{x})$ of $\mathbf{x}_i$ in $X$\; Compute the covariance matrix $\mathbf{Cov}$ of $S(\mathbf{x})$\; Compute the eigenvalues $\bm{\lambda} = (\lambda_1, \lambda_2, \dots, \lambda_n)$ of $\mathbf{Cov}$\; Sort $\bm{\lambda}$ in descending order\; Calculate ID $d(\mathbf{x})$ of $S(\mathbf{x})$ as d(\mathbf{x}) = \left\vert \left\{ \frac{\lambda_i}{\sum^n_{i=1} \lambda_i} \geq \theta, 1 \leq i \leq n \right\} \right\vert ; } Calculate average ID $\bar{d} = \sum_{\mathbf{x} \in X} d(\mathbf{x}) / |X|$; } We perform two different ID estimations, as follows.

of $S(\mathbf{x})$ as d(\mathbf{x}) = \left\vert \left\{ \frac{\lambda_i}{\sum^n_{i=1} \lambda_i} \geq \theta, 1 \leq i \leq n \right\} \right\vert ; } Calculate average ID $\bar{d} = \sum_{\mathbf{x} \in X} d(\mathbf{x}) / |X|$; } We perform two different ID estimations, as follows. First, for a given dataset $D$ and its 2D projection $P(D)$, we compute the average ID of the data backprojection $D'=P^{-1}(P(D))$ over all neighborhoods $S(\mathbf{x})$, denoted $ID_{D'}$. We then compare $ID_{D'}$ with the ground-truth average ID of $D$, denoted $ID_{D}$. Both $ID_{D}$ and $ID_{D'}$ are computed using Alg.~\ref{alg:local_id} with $D$ and $D'$ as inputs, respectively. For an ideal inverse projection $P^{-1}$ that perfectly reverses the effects of a direct projection $P$ on $D$, we would obtain $ID_{D'} = ID_{D}$. Secondly, to study how well a decision map covers the data space it aims to depict (see Q3, Sec.~\ref{sec:introduction_ch5}), we create a pixel grid $I$ of size $500^2$ and backproject it by $P^{-1}$ to obtain a sample set $I^{-1}$.

ID_{D}$. Secondly, to study how well a decision map covers the data space it aims to depict (see Q3, Sec.~\ref{sec:introduction_ch5}), we create a pixel grid $I$ of size $500^2$ and backproject it by $P^{-1}$ to obtain a sample set $I^{-1}$. We next measure the ID at each sample $\mathbf{p} \in I^{-1}$ using Alg.~\ref{alg:local_id} with $I^{-1}$ as input. Let the resulting value at $\mathbf{p}$ be called $ID_p$. Finally, we color the image $I$ by the values $ID_p$ and also compute the average $\overline{ID_p}$ over all pixels in $I$. \caption{Computing the intrinsic dimensionality of data, backprojection of data, and backprojection of pixels.} Figure~\ref{fig:id_schema} depicts all the above processes: Given a dataset $D$, we compute its 2D projection $P(D)$. We inversely project these points via $P^{-1}$ to get the backprojection $D'$. Separately, we inversely project all pixels in the image $I$ to get the sample set $I^{-1}$. In this example, the intrinsic dimensionality $ID_{D}$ is the same to $ID_{D'}$ for the yellow areas in $D$; and higher than $ID_{D'}$ for the green areas in $D$, respectively. To study the smoothness of the computed decision maps, we use the gradient map technique\,\citep{espadoto2021unprojection}, following \Cref{eq:grad_map_ch4}.

$ID_{D}$ is the same to $ID_{D'}$ for the yellow areas in $D$; and higher than $ID_{D'}$ for the green areas in $D$, respectively. To study the smoothness of the computed decision maps, we use the gradient map technique\,\citep{espadoto2021unprojection}, following \Cref{eq:grad_map_ch4}. Recall from Sec.~\ref{sec:visualizations} that areas in a decision map where $G$ is large mean that neighboring pixels are backprojected by $P^{-1}$ far away from each other in the data space, hence the map is unreliable at those locations. Conversely, areas in a decision map with low $G$ mean that neighboring pixels sample the $\mathbb{R}^n$ space at close locations. Assuming a (relatively) smoothly evolving classifier $f$ over $\mathbb{R}^n$, such areas will accurately capture the local behavior of $f$. Table~\ref{tab:comparison} shows the MSE results for all our datasets and decision map computation methods. Values for the iLAMP and RBF inverse projections are exactly zero since these methods enforce that $P^{-1}(P(\mathbf{x}_i))=x_i$ for all $\mathbf{x}_i \in D$ by construction. We see that the MSEs of DBM, SDBM, and DeepView are quite low and comparable across all datasets, indicating that these inverse projections are similar and reliable.

since these methods enforce that $P^{-1}(P(\mathbf{x}_i))=x_i$ for all $\mathbf{x}_i \in D$ by construction. We see that the MSEs of DBM, SDBM, and DeepView are quite low and comparable across all datasets, indicating that these inverse projections are similar and reliable. In contrast, the MSE of MDS+iMDS is significantly higher. On the synthetic datasets (Blobs), the errors of MDS+iMDS are 2 to 3 orders of magnitude higher than for the other tested methods, which is already quite high. However, on the real-world datasets (HAR and MNIST), the errors of MDS+iMDS become even higher. This indicates that the backprojections of MDS+iMDS, and thus the corresponding decision maps, are likely meaningless. Figure~\ref{fig:mnist_inv} confirms this by running a simple test on the MNIST dataset. For 14 images $\mathbf{x}_i$ in this dataset (top row), we show the corresponding inverse projections $P^{-1}(\mathbf{x}_i)$ computed by DBM, SDBM, DeepView, and MDS+iMDS. The first three methods yield very similar images to the original ones, as expected due to the low MDS thereof. In contrast, MDS+iMDS yields basically noise. Note that this agrees with the preliminary discussion of \citet{invertingMDS2024EuroVA} which mentioned that iMDS may be unsuitable for inversely projecting scatterplots created from data of a too high dimensionality.

to the low MDS thereof. In contrast, MDS+iMDS yields basically noise. Note that this agrees with the preliminary discussion of \citet{invertingMDS2024EuroVA} which mentioned that iMDS may be unsuitable for inversely projecting scatterplots created from data of a too high dimensionality. \caption{MSE of the backprojection for the studied datasets.} & \textbf{Blobs 10D} & \textbf{Blobs 30D} & \textbf{Blobs 100D} & \textbf{HAR} & \textbf{MNIST} \\ \textbf{DBM} & $1.83 \times 10^{-3}$ & $2.13 \times 10^{-3}$ & $2.16 \times 10^{-3}$ & $5.30 \times 10^{-3}$ & $3.67 \times 10^{-2}$ \\ \textbf{SDBM} & $2.22 \times 10^{-3}$ & $2.06 \times 10^{-3}$ & $2.16 \times 10^{-3}$ & $8.69 \times 10^{-3}$ & $5.28 \times 10^{-2}$ \\ \textbf{DeepView} & $1.42 \times 10^{-3}$ & $1.67 \times 10^{-3}$ & $1.90 \times 10^{-3}$ & $4.40 \times 10^{-3}$ & $2.92 \times 10^{-2}$ \\ \textbf{UMAP+iLAMP} & 0 & 0& 0 & 0 & 0\\ \textbf{UMAP+RBF}  & 0 & 0 & 0 & 0 & 0\\ \textbf{MDS+iMDS} & $1.07 \times 10^{-1}$ & $6.11 \times 10^{-1}$ & $5.71 \times 10^{0}$ & $5.15 \times 10^{33}$ & $6.19 \times 10^{5}$ \\ To answer Q3, we first test how well an inverse projection $P^{-1}$ covers the \emph{data space} $D$ it aims to depict.

\times 10^{-1}$ & $6.11 \times 10^{-1}$ & $5.71 \times 10^{0}$ & $5.15 \times 10^{33}$ & $6.19 \times 10^{5}$ \\ To answer Q3, we first test how well an inverse projection $P^{-1}$ covers the \emph{data space} $D$ it aims to depict. For this, we compare the estimated ID of the actual data ($ID_{D}$) with that of the round-trip consisting of the direct and inverse projections ($ID_{D'}$). As explained in Sec.~\ref{sec:ID_estimation}, an ideal inverse projection would yield $ID_{D'} = ID_{D}$. Table~\ref{tab:ID_D_k120} shows the results for our five studied datasets. A first consistency check is to see how good the estimated $ID_{D}$ is. We see that these values align well with the expected (ground-truth) ID values for most datasets. The largest difference occurs for Blobs 100D, which is due to the fact that this dataset isotropically spreads in 100 dimensions at every blob by construction (see Sec.~\ref{sec:ID_estimation}); the ID estimation by PCA (Alg.~\ref{alg:local_id}) is heavily affected by the well-known curse of dimensionality. \caption{Selected samples from MNIST and their corresponding backprojections using different $P^{-1}$ methods. Compare with the MSE values in Tab.~\ref{tab:comparison}.}

every blob by construction (see Sec.~\ref{sec:ID_estimation}); the ID estimation by PCA (Alg.~\ref{alg:local_id}) is heavily affected by the well-known curse of dimensionality. \caption{Selected samples from MNIST and their corresponding backprojections using different $P^{-1}$ methods. Compare with the MSE values in Tab.~\ref{tab:comparison}.} Given the above, we can next compare the estimated $ID_D$ with the round-trip estimation $ID_{D'}$ to gauge the inverse projection quality (see Tab. \ref{tab:ID_D_k120} ). Just as for the 3D data discussed in Sec.~\ref{sec:results_3d}, we see that (S)DBM creates basically a \emph{two-dimensional}, surface-like, structure in the data space. DeepView is slightly better in capturing the $ID_D$ of the data -- which matches the fact observed for 3D datasets that its backprojected surfaces have more complex shapes that aim to connect the data samples (Fig.~\ref{fig:DMvsCLF}). Still, DeepView's $ID_{D'}$ values are much lower than the estimated $ID_D$. Note that iLAMP and RBF are not included in the comparison as they have $P^{-1}(P(\mathbf{x}_i))=\mathbf{x}_i$ for all $\mathbf{x}_i \in D$ by construction, which means $ID_{D'} = ID_{D}$. Finally, for MDS+iMDS, we see that $ID_{D'}$ is much closer to the estimated $ID_D$ than for all other methods. This may suggest that MDS+iMDS is better at capturing the data space. Yet,

D$ by construction, which means $ID_{D'} = ID_{D}$. Finally, for MDS+iMDS, we see that $ID_{D'}$ is much closer to the estimated $ID_D$ than for all other methods. This may suggest that MDS+iMDS is better at capturing the data space. Yet, as observed earlier, this method has a very high MSE (Tab.~\ref{tab:comparison}) and also generates meaningless inverse projections (Fig.~\ref{fig:mnist_inv}). As such, the high $ID_{D'}$ for this method is rather an indication of its random sampling pertaining to its implementation (see \citep{invertingMDS2024EuroVA} for details) than its intrinsic higher quality. The authors of iMDS also noted that this inverse projection may not be effective for datasets of high intrinsic dimensionality. Our experiment here confirmed this observation. \caption{Estimated intrinsic dimensionalities $ID_{D}$ and $ID_{D'}$ for our studied datasets. The expected ID values for HAR and MNIST are taken from prior studies\,\citep{el2016modeling,facco2017Estimatingintrinsic,aumuller2019RoleLocal,bahadur2019DimensionEstimation}.} & \textbf{Blobs 10D} & \textbf{Blobs 30D} & \textbf{Blobs 100D} & \textbf{HAR} & \textbf{MNIST} \\ Expected ID & 10 & 30 & 100 & 15-61 & 13-33 \\ $ID_{D}$  & 10.00 & 29.03 & 39.63 & 24.62 & 20.04 \\ $ID_{D'}$ DBM & 2.04 & 2.10 & 2.04 & 3.56 & 4.71 \\

\textbf{MNIST} \\ Expected ID & 10 & 30 & 100 & 15-61 & 13-33 \\ $ID_{D}$ & 10.00 & 29.03 & 39.63 & 24.62 & 20.04 \\ $ID_{D'}$ DBM & 2.04 & 2.10 & 2.04 & 3.56 & 4.71 \\ $ID_{D'}$ SDBM & 2.23 & 2.14 & 2.11 & 2.09 & 2.47 \\ $ID_{D'}$ DeepView & 4.98 & 4.71 & 4.63 & 8.25 & 7.60 \\ $ID_{D'}$ UMAP+iLAMP & - & - & - & - & - \\ $ID_{D'}$ UMAP+RBF & - & - & - & - & - \\ $ID_{D'}$ MDS+iMDS & 10.00 & 22.95 & 37.69 & 11.77 & 28.68 \\ To further answer Q3, we want to know how well a decision map image covers the entire data space of the classifier it aims to visualize. We measure this by comparing the ID of the backprojected decision map image $ID_p$ at each pixel (see Sec.~\ref{sec:ID_estimation}) with the $ID_{D}$ of the dataset $D$ the classifier is trained (or tested) on.

of the classifier it aims to visualize. We measure this by comparing the ID of the backprojected decision map image $ID_p$ at each pixel (see Sec.~\ref{sec:ID_estimation}) with the $ID_{D}$ of the dataset $D$ the classifier is trained (or tested) on. Areas where $ID_p$ is close to $ID_D$ indicate that the decision map covers well the local distribution of $D$; areas where $ID_p \ll ID_D$ indicate that the decision map can only capture a part of this local distribution. Figures~\ref{fig:lid_blob10}$-$\ref{fig:lid_minst} show this comparison. In each figure, the top row shows the actual decision maps computed by our six decision map techniques for the studied Logistic Regression classifier. Colors in these images indicate the inferred class by the trained model $f$ at each pixel; brightness encodes the confidence of $f$ at those locations (dark values indicate low confidence); for details of this computation, see\,\citep{rodrigues2018Imagebasedvisualization,DBM2019rodrigues}. These decision map images are only provided for illustration purposes \emph{e.g.,} showing the location of decision boundaries and the data clusters; the ID analysis presented next does not depend on the classifier choice. \includegraphics[width=0.9\linewidth]{ch5/blobs_dim10_id_5cols.eps} %% editable versions are the PDFs in ./figures/ \caption{Decision maps and ID estimation, Blobs 10D and 30D.}

showing the location of decision boundaries and the data clusters; the ID analysis presented next does not depend on the classifier choice. \includegraphics[width=0.9\linewidth]{ch5/blobs_dim10_id_5cols.eps} %% editable versions are the PDFs in ./figures/ \caption{Decision maps and ID estimation, Blobs 10D and 30D.} The second rows in Figs.~\ref{fig:lid_blob10}-\ref{fig:lid_minst} show the estimated $ID_p$ at each decision map pixel, with the average value $\overline{ID_p}$ over the entire map shown bottom-right in the images. The results are very interesting to examine. For DBM and SDBM, the estimated $ID_p$  are \emph{exactly 2} almost everywhere, which means that these decision maps precisely correspond to \emph{surfaces} in the data space. This extends our earlier findings (Sec.~\ref{sec:results_3d}) to $n>3$ dimensions. DeepView yields higher $ID_p$ values (but still much lower than $ID_{D}$, peaking at 10 for the HAR dataset) close to the actual data points; and values roughly equal to 2 further away from these points, with an $\overline{ID_p}$ over all datasets of $2.49 \pm 0.03$.

(but still much lower than $ID_{D}$, peaking at 10 for the HAR dataset) close to the actual data points; and values roughly equal to 2 further away from these points, with an $\overline{ID_p}$ over all datasets of $2.49 \pm 0.03$. {For UMAP+RBF, in areas close to the data points, the estimated $ID_p$ is high (about the same as $ID_{D}$), see the red-colored spots in Figs.~\ref{fig:lid_blob10}-\ref{fig:lid_minst} (second rows). This is expected by the design of the RBF method, as mentioned earlier. Between the data points, the estimated $ID_p$ for this method is exactly 2. UMAP+iLAMP shows more complicated patterns: This method also yields high $ID_p$ values (basically equal to $ID_{D}$) close to the data points areas, as expected by construction for this method, as explained earlier. Between the data points, this method yields an estimated $ID_p$ roughly equal to 2. However, in contrast to all other methods, UMAP+iLAMP shows strong radial-like patterns of high estimated $ID_p$ that `fan out' from the data points. These radial patterns in the $ID_p$ images match similar ones in the decision maps (Figs.~\ref{fig:lid_blob10}-\ref{fig:lid_minst}, first rows). We see here again an instance of iLAMP's behavior discussed in Fig.~\ref{fig:DMvsCLF} for the 3-dimensional dataset case: iLAMP covers the data space better than other methods (thus answers Q3 better) but does this at the expense of continuity -- that is, it produces decision maps which can be hard to interpret.}

Fig.~\ref{fig:DMvsCLF} for the 3-dimensional dataset case: iLAMP covers the data space better than other methods (thus answers Q3 better) but does this at the expense of continuity -- that is, it produces decision maps which can be hard to interpret.} \includegraphics[width=0.9\linewidth]{ch5/blobs_dim100_id_5cols.eps} %% editable versions are the PDFs in ./figures/ \caption{Decision maps and ID estimation, Blob 100D and HAR.} {To further understand the high $ID_p$ values for iLAMP in image areas far away from data samples, we select an area having such high values for the MNIST dataset (Fig.~\ref{fig:lid_minst}, second row, white square). We next oversample this area at a resolution of $500^2$ pixels to compute $ID_p$. The result (Fig.~\ref{fig:lid_minst} bottom row) shows that $ID_p$ is actually almost 2 in such areas as well, apart from very close to the data points. {Hence, the observed higher intrinsic dimensionality $ID_p$ for iLAMP (and, actually, all other tested methods) is only an effect of the image resolution; all methods have the low $ID_p$ values they exhibit virtually everywhere except

close to the data points. {Hence, the observed higher intrinsic dimensionality $ID_p$ for iLAMP (and, actually, all other tested methods) is only an effect of the image resolution; all methods have the low $ID_p$ values they exhibit virtually everywhere except infinitesimal neighborhoods around the data points.} Further, an interesting observation is that the aforementioned radial patterns seem to be less noisy as the data dimensionality increases -- compare the Blobs 10D, 30D, and 100D images in Figs.~\ref{fig:lid_blob10}-\ref{fig:lid_blob100}. Indeed, as the data is increasingly higher dimensional, iLAMP has more difficulties to `cover' the entire data space with a two-dimensional map, even close to the data points.} {Finally, MDS+iMDS shows a quite different result: The $ID_p$ values it produces are nearly identical over the entire image and also roughly equal to $ID_{D}$. The fact that $ID_p$ is nearly constant matches the linear behavior of this inverse projection method that we discussed for the 3D dataset case (Sec.~\ref{sec:vis_eval}). Separately, the fact that $ID_p \simeq ID_D$ is due to the random sampling process used by iMDS, see Sec.~\ref{sec:background_ch5}.

is nearly constant matches the linear behavior of this inverse projection method that we discussed for the 3D dataset case (Sec.~\ref{sec:vis_eval}). Separately, the fact that $ID_p \simeq ID_D$ is due to the random sampling process used by iMDS, see Sec.~\ref{sec:background_ch5}. We also see that the decision maps for this method are quite dark in all areas, even in those near the actual samples. This correlates with the relatively high MSE of MDS+iMDS (Tab.~\ref{tab:comparison}). Intuitively put, these findings indicate that this inverse projection quickly `goes away' from the data samples $\mathbf{x}_i$ for pixels which are not very close to the locations $P(\mathbf{x}_i)$. Again, this is due to the linear nature of iMDS -- the backprojected surface $I^{-1}$ cannot, by construction, follow the likely curved manifolds on which the samples $\mathbf{x}_i$ are spread. Separately, we see strong noise in the decision maps for this method, which is due to the aforementioned random sampling process. Again, we see here the earlier-mentioned trade-off between coverage and continuity.} \includegraphics[width=0.9\linewidth]{ch5/mnist_id_5cols.eps} %% editable versions are the PDFs in ./figures/ \caption{Decision maps and ID estimation, MNIST. Bottom images show selected detail zones sampled at a high resolution of $500^2$ pixels.}

Again, we see here the earlier-mentioned trade-off between coverage and continuity.} \includegraphics[width=0.9\linewidth]{ch5/mnist_id_5cols.eps} %% editable versions are the PDFs in ./figures/ \caption{Decision maps and ID estimation, MNIST. Bottom images show selected detail zones sampled at a high resolution of $500^2$ pixels.} {As iMDS has a global linear behavior, we can study it in further detail as follows. We compute the covariance matrix for the whole set of backprojected points $I^{-1}$ and then analyze its eigenvalues $\lambda_1, \ldots, \lambda_{100}$ (Fig.\ref{fig:eigenvalues_iMDS}). We observe that there is always a clear drop from the second to the third eigenvalue, indicating that the backprojected points are also dominated by a 2D planar-like structure. This matches the visual observation for the 3D dataset shown in Fig.~\ref{fig:DMvsCLF}. Hence, the earlier discussed fact that $ID_p$ is overall high (Figs.~\ref{fig:lid_blob10}-\ref{fig:lid_blob100}, second rows) is purely due to the random sampling of iMDS. Separately, we see that as the dimensionality of the data increases, this drop becomes less significant. This suggests that the structure becomes more dominated by noise as the dimensionality increases, which correlates with the fact that the MSE of MDS+iMDS is significantly higher for higher-dimensional data (Tab.\ref{tab:comparison}).} \includegraphics[width=0.8\linewidth]{ch5/global_ID_iMDS} %% editable versions are the PDFs in ./figures/

This suggests that the structure becomes more dominated by noise as the dimensionality increases, which correlates with the fact that the MSE of MDS+iMDS is significantly higher for higher-dimensional data (Tab.\ref{tab:comparison}).} \includegraphics[width=0.8\linewidth]{ch5/global_ID_iMDS} %% editable versions are the PDFs in ./figures/ \caption{Eigenvalues of the covariance matrix of the whole set of backprojected points $I^{-1}$ for MDS+iMDS.} The third and fourth rows in Figs.~\ref{fig:lid_blob10}$-$\ref{fig:lid_minst} refine the above insights. The third rows show the percentage of data variance in a neighborhood $S$ captured by the eigenvectors corresponding to the two largest eigenvalues $\lambda_1$ and $\lambda_2$. {Yellow values indicate that almost all the data variance is captured by these two eigenvalues, so the inverse projection creates locally planar structures there. Dark blue values indicate that the opposite, \emph{i.e.}, the inverse projection creates high-dimensional structures in the respective areas. The fourth rows in the figures show the gradient maps of the inverse projections. Dark values in these maps indicate low values of $G$ (Eqn.~\ref{eq:grad_map_ch4}), \emph{i.e.}, areas where the backprojection changes slowly and smoothly. Bright areas indicate the converse phenomenon -- rapid and potentially non-smooth changes in the backprojection.} The computation details are described in Sec.~\ref{sec:ID_estimation}.

in these maps indicate low values of $G$ (Eqn.~\ref{eq:grad_map_ch4}), \emph{i.e.}, areas where the backprojection changes slowly and smoothly. Bright areas indicate the converse phenomenon -- rapid and potentially non-smooth changes in the backprojection.} The computation details are described in Sec.~\ref{sec:ID_estimation}. {These visualizations lead us to additional interesting observations. First, we see that, in nearly all cases, all inverse projection methods except MDS+iMDS create large yellow areas far away from the data points (third rows in Fig. \ref{fig:lid_blob10}$-$\ref{fig:lid_minst}) -- that is, they essentially create two-dimensional surface-like backprojections $I^{-1}$. In contrast, MDS+iMDS shows dark blue values nearly everywhere in these images, \emph{i.e.}, it creates nearly everywhere a high-dimensional sampling of the data space. As explained earlier, this is due to the random sampling process inherent to this method.} A second observation pertains to the presence of 1D dark filament-like linear structures that connect the projected data points which we notice for DeepView and UMAP+iLAMP.

explained earlier, this is due to the random sampling process inherent to this method.} A second observation pertains to the presence of 1D dark filament-like linear structures that connect the projected data points which we notice for DeepView and UMAP+iLAMP. These filaments seem to connect the projected points much like a Delaunay triangulation. These structures match quite well high values in the corresponding gradient maps. Taken together, these findings indicate that the backprojections of DeepView and UMAP+iLAMP consist of a set of planar-like facets, separated by sharp creases or gaps. This generalizes our earlier findings on the `crumpled' aspect of these backprojections, observed for 3D datasets (Fig.~\ref{fig:DMvsCLF}a) to higher dimensions.

that the backprojections of DeepView and UMAP+iLAMP consist of a set of planar-like facets, separated by sharp creases or gaps. This generalizes our earlier findings on the `crumpled' aspect of these backprojections, observed for 3D datasets (Fig.~\ref{fig:DMvsCLF}a) to higher dimensions. {The gradient maps allow us to draw some other insights on the behavior of the inverse projections. For (S)DBM, these maps have high values that align quite well with the corresponding decision boundaries shown in Figs.~\ref{fig:lid_blob10}$-$\ref{fig:lid_minst}, first rows. In contrast, UMAP+RBF has high gradients systematically close to the projected data samples only. UMAP+iLAMP shows an almost complementary behavior to UMAP+RBF, that is, high gradient values on the aforementioned filaments connecting the projected data points and relatively low gradient values close to the data points. Overall, these insights tell that the studied inverse projection methods have very different smoothness behaviors: (S)DBM is relatively smooth overall except close to the decision boundaries; UMAP+RBF is also quite smooth except close to the data samples; and UMAP+iLAMP is overall smooth except close to lines that connect neighboring data samples. All these findings match our earlier observations in the visual study of these backprojections for the 3D dataset case (Fig.~\ref{fig:DMvsCLF}a).}

except close to the data samples; and UMAP+iLAMP is overall smooth except close to lines that connect neighboring data samples. All these findings match our earlier observations in the visual study of these backprojections for the 3D dataset case (Fig.~\ref{fig:DMvsCLF}a).} We next discuss our findings on the interpretation, added value, and found limitations of decision maps and their accompanying inverse projections, and summarize our answers to the questions Q1-Q5 listed in Sec.~\ref{sec:introduction_ch5}. \subsection{Surface behavior of inverse projections and decision maps}

We next discuss our findings on the interpretation, added value, and found limitations of decision maps and their accompanying inverse projections, and summarize our answers to the questions Q1-Q5 listed in Sec.~\ref{sec:introduction_ch5}. \subsection{Surface behavior of inverse projections and decision maps} All six inverse projection pipelines we studied essentially generate surface-like structures embedded in the high-dimensional data space, with some local differences. (S)DBM tends to create relatively smooth and compact surfaces that closely interpolate the data samples. UMAP+RBF does the same but passes exactly through the data samples while being slightly less smooth. DeepView and UMAP+iLAMP create highly twisted surfaces with a similar type of trade-off, \emph{i.e.}, DeepView interpolates the data points less accurately but yields smoother surfaces, while UMAP+iLAMP interpolates the data points exactly but yields very non-smooth results. Finally, MDS+iMDS yields a structure which formally speaking has higher intrinsic dimensionality than a surface. Upon closer examination, we see that this structure is essentially a plane jittered by high amounts of noise (Q1, Q4). We also saw that this surface-like property does not depend on the intrinsic or total dimensionality of the studied datasets (Q1), the studied classifiers (Q1), or resolutions of the decision map images (see Figs.~\ref{fig:mnist_inv}).

high amounts of noise (Q1, Q4). We also saw that this surface-like property does not depend on the intrinsic or total dimensionality of the studied datasets (Q1), the studied classifiers (Q1), or resolutions of the decision map images (see Figs.~\ref{fig:mnist_inv}). \subsection{Coverage of decision maps} Given the aforementioned surface property, we conclude that current decision maps only depict a \emph{small} part of the behavior of a given classifier (Q3). The only (relative) exception here is MDS+iMDS which succeeds in covering a higher proportion of the data space. However, this is done by using a random sampling mechanism which leads to high inverse projection errors (Tab.~\ref{tab:comparison}) and noisy results in both the inverse projections (Fig.~\ref{fig:mnist_inv}) and decision maps (Figs.~\ref{fig:lid_blob10}, \ref{fig:lid_blob100}). We conclude that this method is not suitable for creating general-purpose inverse projections and decision maps for high-dimensional data.

leads to high inverse projection errors (Tab.~\ref{tab:comparison}) and noisy results in both the inverse projections (Fig.~\ref{fig:mnist_inv}) and decision maps (Figs.~\ref{fig:lid_blob10}, \ref{fig:lid_blob100}). We conclude that this method is not suitable for creating general-purpose inverse projections and decision maps for high-dimensional data. The boundaries shown by the studied decision maps (1D curves separating same-color regions in \emph{e.g.} Fig.~\ref{fig:DMvsCLF}) are actually the intersections of the aforementioned surfaces with the actual decision boundaries in high-dimensional space (Q2). Intuitively put, a decision map thus shows a `slice' through the high dimensional data space -- its pixels are located on the aforementioned 2D surface; and its decision boundaries are 1D curve subsets of the actual decision surfaces. It is tempting to argue that, since inverse projections take a 2D space as input, they will always produce also a 2D surface as output and not a higher-dimensional object. Yet, this does not need to be so.

decision surfaces. It is tempting to argue that, since inverse projections take a 2D space as input, they will always produce also a 2D surface as output and not a higher-dimensional object. Yet, this does not need to be so. Space-filling curves\,\citep{peano90} and space-filling surfaces\,\citep{paulsen23} can map low-dimensional sets to higher-dimensional ones in a continuous fashion. By combining such primitives, we could in principle create continuous mappings of intervals between any two dimensions $q$ and $n$, $q < n$. Our study -- in particular, the ID and gradient map estimations -- showed that all evaluated inverse projections (DBM, SDBM, DeepView, iLAMP, RBF, iMDS) do not even get close to such behavior -- which can be explained by the fact that they are constructed by differentiable mappings which cannot in principle exhibit fractal behavior. iMDS has the highest coverage but, as we saw, this is achieved by random sampling, which completely loses continuity. \subsection{Comparing decision map methods}

by the fact that they are constructed by differentiable mappings which cannot in principle exhibit fractal behavior. iMDS has the highest coverage but, as we saw, this is achieved by random sampling, which completely loses continuity. \subsection{Comparing decision map methods} Different decision map techniques sample the high-dimensional space quite differently (Q4). As such, they produce different maps for the \emph{same} classifier (which, obviously, has a unique set of actual decision surfaces). Each such map provides its own insights for the same classifier (see \emph{e.g.} Figs.~\ref{fig:DMvsCLF}, \ref{fig:lid_blob10}, \ref{fig:lid_blob100}, \ref{fig:lid_minst}), each with its own advantages and limitations. At a global level, we see a clear trade-off between \emph{smoothness} and \emph{precision} (Q5). Methods that generate the smoothest surfaces (DBM, SDBM) cannot approximate very well the data samples. Conversely, methods that pass very close or exactly through the data samples (DeepView, UMAP+iLAMP) generate non-smooth surfaces. UMAP+RBF falls somewhere in the middle of these two types. These aspects affect in turn the \emph{interpretability} and ultimately \emph{usability} of the corresponding decision maps. Smooth-surface methods yield maps which are easier to interpret and show better how a classifier \emph{extrapolates} from its training set but are harder to control in terms of \emph{where} they are actually constructed; tighter-surface methods approximate data samples better and, for the case of DeepView, are also easier to control in terms of where they sample the data space. However, they only \emph{interpolate} the classifier behavior close to and between the training points, and can create decision maps which are hard to interpret (UMAP+iLAMP).

DeepView, are also easier to control in terms of where they sample the data space. However, they only \emph{interpolate} the classifier behavior close to and between the training points, and can create decision maps which are hard to interpret (UMAP+iLAMP). Summarizing the above, we believe that smooth-surface methods are overall preferred to tight-surface ones -- they ultimately yield decision maps which are easier to interpret at the small cost of not perfectly approximating the data samples. As a separate point, we note that none of the studied techniques aims to explicitly sample a classifier close to its \emph{actual} decision boundaries -- which, arguably, are the most interesting areas to understand (Q2). For this task, new inverse projections and/or decision map methods need to be devised. \subsection{Limitations caused by the low dimensionality of decision maps}

close to its \emph{actual} decision boundaries -- which, arguably, are the most interesting areas to understand (Q2). For this task, new inverse projections and/or decision map methods need to be devised. \subsection{Limitations caused by the low dimensionality of decision maps} Inverse projection tasks are structurally similar to data reconstruction or data generation tasks -- all of these aim to output high-dimensional data from low-dimensional representations. From the perspective of data reconstruction, the projection and inverse projection pipeline $(P, P^{-1})$ can be seen as a special case of an encoder-decoder structure, where the bottleneck, or latent space, is two-dimensional. Existing works show that the dimensionality of the latent space, which is analogous to the input of $P^{-1}$, is a critical factor for the reconstruction or generation quality\,\citep{wang2016Autoencoderbased,marin2021effectlatent,padala2021EffectInput}. Inspired by these findings, we wonder how the dimensionality $q$ of the latent representation affects the quality (MSE and ID) of an inverse projection.

analogous to the input of $P^{-1}$, is a critical factor for the reconstruction or generation quality\,\citep{wang2016Autoencoderbased,marin2021effectlatent,padala2021EffectInput}. Inspired by these findings, we wonder how the dimensionality $q$ of the latent representation affects the quality (MSE and ID) of an inverse projection. To explore this, we ran DBM (UMAP+NNInv) and SDBM (SSNP) with $q$ values in the range 2 to 25. We chose these methods since they are easily modifiable to use a different latent dimensionality than $q=2$ and also since, following our earlier results, they seem to offer a good balance in terms of desirable properties of inverse projections. For each $q$ value, we recorded the $ID_{D'}$ and $MSE$ of the inverse projection again on $D$. The results, shown in Fig.~\ref{fig:bottleneck_dim}, reveal that, although DBM and SDBM exhibit similar surface behavior, their outcomes differ. The MSE of both methods decreases with $q$ increasing, which is expected -- a higher $q$ is closer to the data dimensionality $n$, so both $P$ and $P^{-1}$ have an easier task. This drop in MSE is however more pronounced for DBM. The $ID_{D'}$ of both methods increases until reaching a plateau around 10-20 (for DBM) and 5 (for SSNP). This tells that the inverse projection task is \emph{fundamentally} harder than the direct projection -- indeed, even when having a much higher number of dimensions $q$ than two as input for $P^{-1}$, it is not always possible to fully recover the full dimensionality $n$ of the data.

is \emph{fundamentally} harder than the direct projection -- indeed, even when having a much higher number of dimensions $q$ than two as input for $P^{-1}$, it is not always possible to fully recover the full dimensionality $n$ of the data. \includegraphics[width=1.0\linewidth]{ch5/bottleneck.eps} %% editable versions are the PDFs in ./figures/ \caption{Changes of $MSE$ and $ID_{D'}$ as a function of the latent dimensionality $q$ for DBM (UMAP+NNinv) and SDBM (SSNP). See Sec.~\ref{sec:ch5:limitations}.} From this experiment, we infer that the 2D bottleneck of a $(P, P^{-1})$ transformation strongly affects the quality (error and ID) of the inverse projections. Increasing the number of dimensions $q$ available as input for inverse projections can increase the quality of their output, but only up to a given limit. Moreover, from a practical viewpoint, increasing $q$ is not possible -- after all, we need to have $q \in \{2,3\}$ if we want to directly visualize the decision maps. Exploring how this barrier can be overcome, \emph{e.g.}, by more sophisticated inverse projection methods, are an important direction for future work.

-- after all, we need to have $q \in \{2,3\}$ if we want to directly visualize the decision maps. Exploring how this barrier can be overcome, \emph{e.g.}, by more sophisticated inverse projection methods, are an important direction for future work. Our results are limited in their power by several factors. We used only two real-world datasets. Datasets having different characteristics, \emph{e.g.}, local intrinsic dimensionality, data distribution, sparsity, or dimensionality, could potentially lead to new insights on how the tested decision maps and inverse projections work. A challenge here is to find datasets having ground-truth estimations of their intrinsic dimensionality. Separately, apart from the technical estimation of MSE, ID estimation, and gradient maps, we gauged the \emph{suitability} of decision maps to practical applications only by qualitatively interpreting the visual smoothness of the resulting decision zones and boundaries. It is expected that, for the tested datasets and classifiers, such zones and boundaries should be smooth\,\citep{oliveira2023StabilityAnalysis}. A more powerful ranking of decision maps would need to consider their actual use in ML engineering scenarios such as data augmentation or adversarial attacks, see \emph{e.g.}\,\citep{differentiableDBM}.

for the tested datasets and classifiers, such zones and boundaries should be smooth\,\citep{oliveira2023StabilityAnalysis}. A more powerful ranking of decision maps would need to consider their actual use in ML engineering scenarios such as data augmentation or adversarial attacks, see \emph{e.g.}\,\citep{differentiableDBM}. We have analyzed the limitations of current inverse projection and decision map techniques used to visualize the behavior of machine learning classification models. Specifically, we compared the decision zones and boundaries depicted by six inverse projection techniques (and corresponding decision maps), with the actual zones and boundaries created by six classifiers on a three-dimensional real-world dataset. We found out that, in all cases, all the studied maps essentially capture a 2D structure embedded in the data space. We further extended our analysis to high-dimensional data by comparing the intrinsic dimensionality of the data with that of the inverse projection and backprojection of the map to the data space. We found that, as for the 3D data case, all studied map techniques still only cover essentially two-dimensional structures in the data space (modulo a certain amount of noise). We found that this surface-like limitation is particularly visible in areas located between the projected data points.

3D data case, all studied map techniques still only cover essentially two-dimensional structures in the data space (modulo a certain amount of noise). We found that this surface-like limitation is particularly visible in areas located between the projected data points. Apart from this common aspect, we found several differences between the studied methods in terms of smoothness of the generated surface and accuracy by which it approximates the data points. Our conclusion is that, when selecting inverse projection methods for constructing decision maps, methods which create smoother surfaces, \emph{i.e.}, DBM, SDBM, and UMAP+RBF, are preferred in terms of predictability and ease of interpretation of the resulting maps, to methods that create tighter-fitting, less smooth, surfaces, and thus harder to interpret decision maps, \emph{i.e.}, UMAP+iLAMP and DeepView. Finally, we showed that the recent MDS+iMDS inverse projection method is not suitable for constructing meaningful decision maps.

the resulting maps, to methods that create tighter-fitting, less smooth, surfaces, and thus harder to interpret decision maps, \emph{i.e.}, UMAP+iLAMP and DeepView. Finally, we showed that the recent MDS+iMDS inverse projection method is not suitable for constructing meaningful decision maps. Our work highlights fundamental limitations of all studied decision map techniques in terms of how much of a classifier's behavior they capture, but also where and how they choose to capture this behavior. These limitations are essential to understand when choosing which such technique to use in practice to construct decision maps but also when actually interpreting the resulting maps.

they capture, but also where and how they choose to capture this behavior. These limitations are essential to understand when choosing which such technique to use in practice to construct decision maps but also when actually interpreting the resulting maps. Future work can advance in a number of directions. The key one, we believe, is overcoming the `surface limitation' of current decision map techniques. Likely, capturing a full high-dimensional space in a 2D map is not possible in general. Rather, one can focus on capturing specific areas in this space which are important to ML engineering, such as low-dimensional (curved) subspaces which contain most of a given dataset; areas close to the actual decision zones, where a classifier is most interesting to study; or areas where a classifier exhibits poor testing performance. An alternative way is to involve interacting allowing the user to move the current backprojected surfaces so as to sweep interesting zones of the data space, by \emph{e.g.} generalizing the approach of \citet{sohns2023DecisionBoundary}, which interactively explores decision boundaries by the simple but limited PCA projection.

involve interacting allowing the user to move the current backprojected surfaces so as to sweep interesting zones of the data space, by \emph{e.g.} generalizing the approach of \citet{sohns2023DecisionBoundary}, which interactively explores decision boundaries by the simple but limited PCA projection. In \Cref{ch6:controllable_Pinv}, we present a general purpose approach -- that can accommodate any direct projection technique -- that allows users to achieve the above by directly controlling the backprojected surface in an interactive way. As outlined several times from \Cref{ch1:intro} to \Cref{ch4:metrics_decision_map}, multidimensional projections and their inverse projection counterparts methods enable a wealth of applications such as data imputation, classification model evaluation, and shape morphing\,\citep{rodrigues2020VisualAnalytics,DBM2019rodrigues,oliveiraSDBM2022,schulz2020DeepViewVisualizing,benato2024HumanloopUsing,amorim2015Facinghighdimensions,differentiableDBM}.

directly controlling the backprojected surface in an interactive way. As outlined several times from \Cref{ch1:intro} to \Cref{ch4:metrics_decision_map}, multidimensional projections and their inverse projection counterparts methods enable a wealth of applications such as data imputation, classification model evaluation, and shape morphing\,\citep{rodrigues2020VisualAnalytics,DBM2019rodrigues,oliveiraSDBM2022,schulz2020DeepViewVisualizing,benato2024HumanloopUsing,amorim2015Facinghighdimensions,differentiableDBM}. However, it is well known that direct projections suffer from information loss when the intrinsic dimensionality of their input data significantly exceeds that of the projection space (Sec.~\ref{sec:background_ch2}). Additionally, our work in \Cref{ch5:surface_like_behavior} showed that all inverse projection techniques we know can only generate fixed, surface-like, structures in the data space. In other words, both direct and inverse projections are subject to significant \emph{information loss} \citep{zheng2023AutoencodersIntrinsic}. The key problem with this is that users expect to use inverse projections -- or techniques using these such as decision maps -- to examine large parts of a data space starting from the (2D) visual space. If both direct and inverse projections have limitations in this sense, such visualizations may be misleading in several ways.

using these such as decision maps -- to examine large parts of a data space starting from the (2D) visual space. If both direct and inverse projections have limitations in this sense, such visualizations may be misleading in several ways. In this chapter, we address the above-mentioned limitations by proposing a novel controllable inverse projection method, thereby answering research question \textbf{RQ3} introduced in~\Cref{ch1:intro}. In contrast to our contributions in previous chapters, which fall under the topic \emph{VIS4ML} (see~\Cref{ch1:intro,ch2:related}), the work in this chapter falls under the topic \emph{ML4VIS} as it employs deep learning techniques to create better visualizations\footnote{This chapter is based on the paper ``LCIP: Loss-Controlled Inverse Projection of High-Dimensional Data''\,\citep{paperLCIP}.}.

which fall under the topic \emph{VIS4ML} (see~\Cref{ch1:intro,ch2:related}), the work in this chapter falls under the topic \emph{ML4VIS} as it employs deep learning techniques to create better visualizations\footnote{This chapter is based on the paper ``LCIP: Loss-Controlled Inverse Projection of High-Dimensional Data''\,\citep{paperLCIP}.}. The key novelty of our work is that we enable inverse projections to represent surface-like structures that can "sweep" through the high-dimensional data space in a user-controlled manner, achieving significantly higher coverage than all existing fixed-surface inverse projection techniques. In more detail, for a sample $\mathbf{x}$ projecting to $\mathbf{y}=P(\mathbf{x})$ by some user-chosen technique $P$, we find the information $\mathbf{z}$ that is lost during the projection $P$. Next, we enable users to (interactively) control how to combine $\mathbf{y}$ (the information preserved by $P$) and $\mathbf{z}$ (the information lost by $P$) to compute our controllable inverse projection. To realize this, we must answer three questions: \item How to ensure $\mathbf{z}$ is \emph{independent} of $\mathbf{y}$ (as we want to let $P$ compute $\mathbf{y}$ and the user control $\mathbf{z}$)? \item How to compute $\mathbf{z}$ for locations in the 2D space where \emph{no ground-truth} sample $\mathbf{x}$ projects?

questions: \item How to ensure $\mathbf{z}$ is \emph{independent} of $\mathbf{y}$ (as we want to let $P$ compute $\mathbf{y}$ and the user control $\mathbf{z}$)? \item How to compute $\mathbf{z}$ for locations in the 2D space where \emph{no ground-truth} sample $\mathbf{x}$ projects? \item How to \emph{control} the structure created by the inverse projection, to enable users to explore large parts of the data space? Our proposal, called Loss-Controlled Inverse Projection (LCIP) answers the above questions as follows: \item We minimize mutual information to disentangle the $\mathbf{y}$ and $\mathbf{z}$ spaces; \item We use interpolation to fill in the `empty' areas using learned $\mathbf{z}$ values for known data samples; \item We allow users to interactively adapt $\mathbf{z}$ to control the shape of the inverse projection.

disentangle the $\mathbf{y}$ and $\mathbf{z}$ spaces; \item We use interpolation to fill in the `empty' areas using learned $\mathbf{z}$ values for known data samples; \item We allow users to interactively adapt $\mathbf{z}$ to control the shape of the inverse projection. We introduce LCIP in Sec.~\ref{sec:method}. To show our method's abilities, we first study its disentanglement effect (Sec.~\ref{sec:disentanglement}) and next compare LCIP both visually and via quality metrics with 3 state-of-the-art inverse projections for 4 datasets and 2 direct projections (Sec.~\ref{sec:compare_previous}). Further on, we show how LCIP's main feature -- its ability to control the shape of the inversely-projected structure in data space -- creates higher-dimensional structures than surfaces, something existing methods cannot achieve (Sec.~\ref{sec:control_and_ID}). Finally, we illustrate the added-value of being able to control the inverse projection by using LCIP to create a style transfer application (Sec.~\ref{sec:application_afhq}). \section{Background and Related Work}

space -- creates higher-dimensional structures than surfaces, something existing methods cannot achieve (Sec.~\ref{sec:control_and_ID}). Finally, we illustrate the added-value of being able to control the inverse projection by using LCIP to create a style transfer application (Sec.~\ref{sec:application_afhq}). \section{Background and Related Work} \textbf{Preliminaries:} The key concepts and definitions we will use in this chapter have been introduced and used multiple times in this thesis. Projections map high-dimensional datasets in $\mathbb{R}^n$ to two-dimensional scatterplots (see Eqn.~\ref{eqn:projdef} and related text). Inverse projections map the entire 2D projection space to the $\mathbb{R}^n$ data space aiming to reverse the effects of a direct projection (see Eqn.~\ref{sec:nninv} and related text). Most importantly, \Cref{ch5:surface_like_behavior} showed that all existing inverse projections we are aware of map the 2D space to a \emph{surface-like} structure embedded in data space -- or, more formally, a structure with intrinsic dimensionality close to two\,\citep{ansuini2019Intrinsicdimension}. While this is not entirely unexpected given that an inverse projection aims to be a smooth mapping of $\mathbb{R}^2$ to $\mathbb{R}^n$, this means that inverse projections can only cover a very \emph{limited subspace} of a given data space. Moreover, which exact such subspace an inverse projection generates in a given context is completely non-transparent to, and not controllable by, its users.

this means that inverse projections can only cover a very \emph{limited subspace} of a given data space. Moreover, which exact such subspace an inverse projection generates in a given context is completely non-transparent to, and not controllable by, its users. The above imply, in turn, serious limitations for inverse projection applications. Using inverse projections for data augmentation\,\citep{benato2024HumanloopUsing} will lack diversity. Separately, one cannot be sure that decision maps, which need to use inverse projections in their construction (\Cref{ch4:metrics_decision_map}), truly depict a classifier's behavior, as they show only a small part of the data space -- more specifically, the intersections of the true decision boundaries with the surface-like structure created by $P^{-1}$. Finally, inverse projections generate only surface-like structures in the data space (\Cref{ch5:surface_like_behavior}). Hence, how to explore the data space \emph{outside} such structures, and how to \emph{control} this exploration, are challenges not answered by current inverse projection methods.

created by $P^{-1}$. Finally, inverse projections generate only surface-like structures in the data space (\Cref{ch5:surface_like_behavior}). Hence, how to explore the data space \emph{outside} such structures, and how to \emph{control} this exploration, are challenges not answered by current inverse projection methods. \textbf{Information loss:} Direct and inverse projections share some limitations concerning information loss. For direct projections, if the data $X$ do not land on or close to a manifold, it is hard to construct a mapping $P(X)$ that fully preserves the data structure\,\citep{zheng2023AutoencodersIntrinsic}. For inverse projections, the limitations are stronger -- these always create a surface-like structure when mapping the 2D space to data space\,\citep{wang2024FundamentalLimitations}. Combining the above, if we consider the project-unproject cycle, \emph{information loss} will always occur.

preserves the data structure\,\citep{zheng2023AutoencodersIntrinsic}. For inverse projections, the limitations are stronger -- these always create a surface-like structure when mapping the 2D space to data space\,\citep{wang2024FundamentalLimitations}. Combining the above, if we consider the project-unproject cycle, \emph{information loss} will always occur. Inverse projections are structurally similar to data reconstruction or data generation tasks which aim to output high-dimensional data from low-dimensional representations. From this perspective, the ($P$, $P^{-1}$) cycle can be seen as an encoder-decoder structure, where the bottleneck is the 2D latent space. The dimensionality of this space is a critical factor for the quality of reconstruction and generation\,\citep{wang2016Autoencoderbased,marin2021effectlatent,padala2021EffectInput}. Inspired by these observations, we aim to break the limitations imposed by our bottleneck -- the visual space -- by retrieving information lost during $P$ and using it to drive the construction of $P^{-1}$ under user control. This will enable our $P^{-1}$ to span structures with higher intrinsic dimensionality than two, and also control where these are placed in data space.\subsection{Learning disentangled representations and adversarial training} As Sec.~\ref{sec:introduction} stated, our first goal is to find the information $\mathbf{z}$ lost during projection independent of the projection $\mathbf{y}$.

higher intrinsic dimensionality than two, and also control where these are placed in data space.\subsection{Learning disentangled representations and adversarial training} As Sec.~\ref{sec:introduction} stated, our first goal is to find the information $\mathbf{z}$ lost during projection independent of the projection $\mathbf{y}$. Independence is often relaxed to minimizing mutual information or separating complementary factors\,\citep{moyer2018InvariantRepresentations}. When achieved, this improves interpretability, reduces potential bias, and enhances generalization. For example, in handwriting recognition, separating text content $\mathbf{y}$ (what an actual letter \emph{is}) from its style $\mathbf{z}$ (how the letter is \emph{written}) helps model generalization. In speech processing, one aims to separate the speech content $\mathbf{y}$ from the speaker's identity $\mathbf{z}$\,\citep{hadad2018twostepdisentanglement}. Minimizing mutual information between two latent representations, also called learning disentangled representations, can be done via adversarial training\,\citep{mathieu2016Disentanglingfactors,hadad2018twostepdisentanglement,xie2017ControllableInvariance,jaiswal2019UnifiedAdversarial,zheng2019DisentanglingLatent}.

letter is \emph{written}) helps model generalization. In speech processing, one aims to separate the speech content $\mathbf{y}$ from the speaker's identity $\mathbf{z}$\,\citep{hadad2018twostepdisentanglement}. Minimizing mutual information between two latent representations, also called learning disentangled representations, can be done via adversarial training\,\citep{mathieu2016Disentanglingfactors,hadad2018twostepdisentanglement,xie2017ControllableInvariance,jaiswal2019UnifiedAdversarial,zheng2019DisentanglingLatent}. Adversarial training, first used by generative adversarial networks (GANs) for image generation\,\citep{goodfellow2014GenerativeAdversarial}, can generate high-dimensional realistic samples from low-dimensional latent codes. GANs jointly train a generator $G$ and a discriminator $Dis$; $G$ aims to create samples that are indistinguishable from real samples; $Dis$ tries to distinguish real from generated samples. Adversarial training has been extended to other areas like robust machine learning, domain adaptation, and disentangled representation learning. While diffusion models are now more popular for image generation, GANs are significantly more efficient\,\citep{pan2023DragYour}. For example, DragGAN\,\citep{pan2023DragYour}, an interactive image manipulation method, uses the StyleGAN2 architecture\,\citep{karras2020AnalyzingImproving}.

to other areas like robust machine learning, domain adaptation, and disentangled representation learning. While diffusion models are now more popular for image generation, GANs are significantly more efficient\,\citep{pan2023DragYour}. For example, DragGAN\,\citep{pan2023DragYour}, an interactive image manipulation method, uses the StyleGAN2 architecture\,\citep{karras2020AnalyzingImproving}. Directly projecting high-dimensional datasets such as high-resolution images is challenging. In such cases, one typically describes the data using the latent space of a pre-trained classifier such as InceptionV3 or VGG16\,\citep{espadoto2020Deeplearning,benato2024HumanloopUsing}. Since we focus on inverse projection, we prefer to retrieve images from the latent space of a generative model designed for this purpose such as StyleGAN2. In more detail: Let $\mathbf{w} \in \mathcal{W}$ be latent code that StyleGAN2 uses to generate images. Codes $\mathbf{w}$ can be obtained by inverting StyleGAN2 pre-trained on the same dataset\,\citep{karras2020AnalyzingImproving}. For a given $\mathbf{w}$, the corresponding image is then given by $G(\mathbf{w})$. Adversarial training connects to our work in two ways: (1) We use it to enforce disentanglement between the information $\mathbf{z}$ and the projection $\mathbf{y}$ during the training of our inverse projection. (2) We use the $\mathcal{W}$ space of an image dataset to ease the projection process. \section{Design of Loss-Controlled Inverse Projection} \subsection{Inverse Projection Deep Learning Network Architecture}

the information $\mathbf{z}$ and the projection $\mathbf{y}$ during the training of our inverse projection. (2) We use the $\mathcal{W}$ space of an image dataset to ease the projection process. \section{Design of Loss-Controlled Inverse Projection} \subsection{Inverse Projection Deep Learning Network Architecture} We implement our inverse projection using neural networks. Consider a dataset $X$ and its projection $Y = P(X)$ computed by any user-chosen projection technique $P$. Our network has two key parts: an encoder $Enc$ that computes the information in $X$ lost by $P$; and a decoder $Dec$ which is also our inverse projection $P^{-1}$. $Enc$ reads the data $X$ and outputs a latent code $Z = Enc(X) = \{\mathbf{z}_i\}$. $Dec$ reads the concatenation $Y \oplus Z$ and outputs the inversely projected data $X'  = Dec(Y, Z)$ (Fig.~\ref{fig:framework}a). We also need to ensure that $Z$ is not related to $Y$ (see Sec.~\ref{sec:gan}). We do this by a third network $Dis$ which reads $Z$ and outputs $Y'=Dis(Z)$ and is adversarially trained to minimize the error between $Y$ and $Y'$. $Enc$ and $Dec$ are jointly trained to (i) minimize the reconstruction error between $X$ and $X'$, and (ii) maximize the difference between $Y$ and $Y'$.

and outputs $Y'=Dis(Z)$ and is adversarially trained to minimize the error between $Y$ and $Y'$. $Enc$ and $Dec$ are jointly trained to (i) minimize the reconstruction error between $X$ and $X'$, and (ii) maximize the difference between $Y$ and $Y'$. \caption{Illustration of our LCIP method. $\oplus$ denotes concatenation. (a) Training the networks. $P$ is a user-selected DR method that projects $X$ to $Y$. $Enc$ encodes $X$ into $Z$. $Dis$ uses $Z$ to predict $Y$. $Dec$ ($P^{-1}$) uses $Y$ and $Z$ to reconstruct $X$. Disentanglement is enforced by the adversarial network $Dis$ (see Sec.~\ref{sec:design}). (b) Inversely projecting a 2D point $\mathbf{q}$ to the data sample $\mathbf{p}$ (see Sec.~\ref{sec:initialize_z}). (c) The control mechanism refines the inverse projection by maneuvering $\mathbf{z}$ (see Sec.~\ref{sec:control_mechanism}).} Let $\theta_{Enc}$, $\theta_{Dec}$, $\theta_{Dis}$ be the weight and bias parameters of $Enc$, $Dec$, and $Dis$, respectively. Let $L_{adv}(Y, Y')$ be the reconstruction loss between $Y$ and $Y'$, and $L_{rec}(X, X')$ the reconstruction loss between $X$ and $X'$. When optimizing $\theta_{Dis}$, $L_{adv}$ is minimized, so \(Dis\) learns to predict \(Y\) from \(Z\). The cost function $J$ for optimizing $Enc$ and $Dec$ is defined as J = L_{rec}(X, X') - \lambda   L_{adv}(Y, Y'),

reconstruction loss between $X$ and $X'$. When optimizing $\theta_{Dis}$, $L_{adv}$ is minimized, so \(Dis\) learns to predict \(Y\) from \(Z\). The cost function $J$ for optimizing $Enc$ and $Dec$ is defined as J = L_{rec}(X, X') - \lambda L_{adv}(Y, Y'), where $\lambda > 0$ is a hyperparameter that balances reconstruction \emph{vs} adversarial loss, with the target of the optimization being that is, we minimize $L_{rec}$  and maximize $L_{adv}$ while keeping $\theta_{Dis}$ fixed. Once trained, $Enc$ infers $Z$ from $X$ and $Dec$ next inversely projects $Y \oplus Z$ to $X'$.

reconstruction \emph{vs} adversarial loss, with the target of the optimization being that is, we minimize $L_{rec}$ and maximize $L_{adv}$ while keeping $\theta_{Dis}$ fixed. Once trained, $Enc$ infers $Z$ from $X$ and $Dec$ next inversely projects $Y \oplus Z$ to $X'$. \textbf{Implementation:} We use fully-connected networks for $Enc$, $Dec$, and $Dis$. $Enc$ has 3 hidden layers (sizes 512, 256, and 128). $Dec$ has 4 hidden layers (sizes 128, 256, 512, and 1024). Each hidden layer of $Enc$ and $Dec$ is followed by a ReLU activation function. The final layer of $Dec$ is followed by a sigmoid activation function. $Dis$ has 2 hidden layers, each with a size of 128. Each hidden layer is followed by batch normalization and a ReLU activation function. The dimension of $Z$ is set to 16 -- a value we empirically found to be sufficient to capture the information loss of all studied $P$ techniques. We use all these settings consistently for all tested datasets.

ReLU activation function. The dimension of $Z$ is set to 16 -- a value we empirically found to be sufficient to capture the information loss of all studied $P$ techniques. We use all these settings consistently for all tested datasets. We use mean squared error (MSE) for both $L_{rec}$ and $L_{adv}$ (Eqn.~\ref{eqn:j}). For $\lambda$ (Eqn.~\ref{eqn:j}), we ran a grid search over the range $[0.005, 4]$, and found that $\lambda \in [0.01, 0.1]$ gives good results for all tested datasets. We train all networks using the Adam optimizer with a learning rate of $0.001$. While training $Dis$, we have noticed that the adversarial training requires more steps to stabilize, since it learns from a changing input. Hence, at each iteration, we update $Dis$ 5 times, then update $Enc$ and $Dec$ once. Our work is implemented using PyTorch\,\citep{paszke2019PyTorch} with PySide6 (Qt) for the GUI, and is publicly available\,\citep{softwareLCIP}. \subsection{Computing $\mathbf{z}$ for the entire projection space}

changing input. Hence, at each iteration, we update $Dis$ 5 times, then update $Enc$ and $Dec$ once. Our work is implemented using PyTorch\,\citep{paszke2019PyTorch} with PySide6 (Qt) for the GUI, and is publicly available\,\citep{softwareLCIP}. \subsection{Computing $\mathbf{z}$ for the entire projection space} To apply our inverse projection presented in Sec.~\ref{sec:design} at a 2D point $\mathbf{p}_i$, we need a latent code $\mathbf{z}_i = Enc(\mathbf{x}_i)$, which in turn requires that we know the data sample $\mathbf{x}_i \in X$ that projects to $\mathbf{p}_i$, \emph{i.e.}, $\mathbf{p}_i = P(\mathbf{x}_i)$. To apply our method to \emph{any} 2D point $\mathbf{p}$, we need to estimate $\mathbf{z}_\mathbf{p}$ at that location. We do this by interpolating the $\mathbf{z}_i$ values of the training points $X$ (see Fig.~\ref{fig:framework}b). We tested two methods: weighted k-NN ($k=10$ neighbors) and smoothed RBF with a parameter-free thin plate spline kernel. RBF gives a smooth surface, while weighted K-NN is slightly faster. We discuss the results of both interpolation methods in Sec.~\ref{sec:compare_previous}. Having now the latent code $\mathbf{z}_\mathbf{p}$ for any $\mathbf{p} \in \mathbb{R}^2$, we can inversely project $\mathbf{p}$ to the data space (Fig.~\ref{fig:framework}b) as \mathbf{q} = P^{-1}(\mathbf{p}) = Dec(\mathbf{p}, \mathbf{z}_\mathbf{p}). \subsection{Controlling the inverse projection}

the results of both interpolation methods in Sec.~\ref{sec:compare_previous}. Having now the latent code $\mathbf{z}_\mathbf{p}$ for any $\mathbf{p} \in \mathbb{R}^2$, we can inversely project $\mathbf{p}$ to the data space (Fig.~\ref{fig:framework}b) as \mathbf{q} = P^{-1}(\mathbf{p}) = Dec(\mathbf{p}, \mathbf{z}_\mathbf{p}). \subsection{Controlling the inverse projection} To allow users to effectively control the shape of the inverse projection in data space, two questions arise: (1) How to do this \emph{easily}, \emph{i.e.}, by changing a small number of intuitive parameters in a direct, visual, way; and (2) How to make our $P^{-1}$ cover zones in data space where \emph{plausible} samples exist, so that $P^{-1}$ is useful for real-world applications. \caption{Controlling the inverse projection. User parameters are marked in red.}

parameters in a direct, visual, way; and (2) How to make our $P^{-1}$ cover zones in data space where \emph{plausible} samples exist, so that $P^{-1}$ is useful for real-world applications. \caption{Controlling the inverse projection. User parameters are marked in red.} We solve both problems by the pipeline shown in Fig.~\ref{fig:framework}c, which we explain next (see also Fig.~\ref{fig:control}). Let $\mathbf{x}_t \in X$, called a \emph{target} sample, be a data point that we want to make our inverse projection go close to. Users can discover such points by brushing the projection with a tooltip to see their values $P(\mathbf{x}_t)$ (Fig.~\ref{fig:control}, green point). The user next selects a 2D \emph{source} point $\mathbf{p}_s$ and manipulates it to control $P^{-1}$ (Fig.~\ref{fig:control}, yellow point). We then `pull' the inverse projection $P^{-1}(\mathbf{p}_s)$ of $\mathbf{p}_s$ towards $\mathbf{x}_t$ by adjusting $\mathbf{z}_{\mathbf{p}_s}$ (initially computed as described in Sec.~\ref{sec:initialize_z}) -- see the red arrow in Fig.~\ref{fig:control}. To achieve this pull, we first compute the difference \Delta \mathbf{z} &= \mathbf{z}_t - \mathbf{z}_{\mathbf{p}_s} = Enc(\mathbf{x}_t) - \mathbf{z}_{\mathbf{p}_s}

$P^{-1}(\mathbf{p}_s)$ of $\mathbf{p}_s$ towards $\mathbf{x}_t$ by adjusting $\mathbf{z}_{\mathbf{p}_s}$ (initially computed as described in Sec.~\ref{sec:initialize_z}) -- see the red arrow in Fig.~\ref{fig:control}. To achieve this pull, we first compute the difference \Delta \mathbf{z} &= \mathbf{z}_t - \mathbf{z}_{\mathbf{p}_s} = Enc(\mathbf{x}_t) - \mathbf{z}_{\mathbf{p}_s} between $\mathbf{z}_t = Enc(\mathbf{x}_t)$ and $\mathbf{z}_{\mathbf{p}_s}$. Intuitively put, $\Delta z$ tells how much the latent codes of the source and target differ -- that is, what we need to change in the source's inverse projection to make it become the target. Having $\Delta z$, we now compute the user-controlled inverse projection of $\mathbf{p}_s$ as \mathbf{q}_s^{user} = Dec(\mathbf{p}_s, \mathbf{z}_{\mathbf{p}_s} + \alpha  \Delta \mathbf{z}), where $\alpha \in \mathbb{R}$ is a factor giving the pull magnitude (Fig.~\ref{fig:control}, light blue point).

it become the target. Having $\Delta z$, we now compute the user-controlled inverse projection of $\mathbf{p}_s$ as \mathbf{q}_s^{user} = Dec(\mathbf{p}_s, \mathbf{z}_{\mathbf{p}_s} + \alpha \Delta \mathbf{z}), where $\alpha \in \mathbb{R}$ is a factor giving the pull magnitude (Fig.~\ref{fig:control}, light blue point). Yet, this adjustment only changes $P^{-1}$ at the \emph{single} location $\mathbf{q}_s^{user}$. 2D points close to $\mathbf{p}_s$ will not be affected by this pull, as their inverse projections still follow Eqn.~\ref{eq:invproj}. The inverse projection will exhibit a discontinuity or lack of smoothness around $\mathbf{q}_s^{user}$, which is undesired (Sec.~\ref{sec:nninv}). We could get smoothness by applying $\Delta \mathbf{z}$ to all such 2D points. However, this changes the inverse projection \emph{globally} -- the source $\mathbf{p}_s$ will equally influence all inversely-projected points, no matter how far these are from $\mathbf{p}_s$. We jointly achieve smoothness and local control by weighing the adjustment based on distances to the source $\mathbf{p}_s$. That is, after adjusting the inverse projection at $\mathbf{p}_s$ (Eqn.~\ref{eq:control_single}), we replace Eqn.~\ref{eq:invproj} by \mathbf{q}^{user} = Dec(\mathbf{p}, \mathbf{z}_p + \alpha  K_{\sigma}(\mathbf{p}, \mathbf{p}_s)  \Delta \mathbf{z}),

achieve smoothness and local control by weighing the adjustment based on distances to the source $\mathbf{p}_s$. That is, after adjusting the inverse projection at $\mathbf{p}_s$ (Eqn.~\ref{eq:control_single}), we replace Eqn.~\ref{eq:invproj} by \mathbf{q}^{user} = Dec(\mathbf{p}, \mathbf{z}_p + \alpha K_{\sigma}(\mathbf{p}, \mathbf{p}_s) \Delta \mathbf{z}), where $K_{\sigma}(\mathbf{p}, \mathbf{p}_s) = e^{-\frac{\|\mathbf{p} - \mathbf{p}_s\|^2}{2\sigma^2}}$ is a Gaussian centered at $\mathbf{p}_s$ and $\sigma$ controls the source's influence. Larger $\sigma$ values make the control more global and yield smoother inverse projections; smaller values have the opposite effect. When $\mathbf{p}$ is close to the source $\mathbf{p}_s$ (Fig.~\ref{fig:control} purple point), its inverse projection gets pulled towards the target $\mathbf{x}_t$ -- see light-blue bump on the surface in Fig.~\ref{fig:control}. When $\mathbf{p}$ is far from $\mathbf{p}_s$ (Fig.~\ref{fig:control} orange point), its inverse projection stays on the surface given by Eqn.~\ref{eq:invproj}.

(Fig.~\ref{fig:control} purple point), its inverse projection gets pulled towards the target $\mathbf{x}_t$ -- see light-blue bump on the surface in Fig.~\ref{fig:control}. When $\mathbf{p}$ is far from $\mathbf{p}_s$ (Fig.~\ref{fig:control} orange point), its inverse projection stays on the surface given by Eqn.~\ref{eq:invproj}. Figure~\ref{fig:teaser} shows this control mechanism in action for a simple style transfer application. Here, the dataset $X$ contains images of various animal faces. The user selects the source $\mathbf{p}$ by picking a location in the projection -- not necessarily an actual projected sample. The image $\mathbf{q}$ for this point is computed by the inverse projection -- see the sad-looking dog in Fig.~\ref{fig:teaser}a, right. Next, the user selects a target sample $\mathbf{x}_t$ -- see the happy-looking dog in Fig.~\ref{fig:teaser}a, left. Pulling a slider changes $\alpha$ and morphs the source image towards the target. The user can see how far/strong the effect of changing the source propagates over the projection by selecting other images (Fig.~\ref{fig:teaser}e) and assessing their changes during source manipulation.

a slider changes $\alpha$ and morphs the source image towards the target. The user can see how far/strong the effect of changing the source propagates over the projection by selecting other images (Fig.~\ref{fig:teaser}e) and assessing their changes during source manipulation. \caption{LCIP illustrated by a style transfer application. If we want to make a dog smile, we proceed as follows: \textbf{1)} Brush the projection (a) to find a frowning dog which we want to change. The actual image of this dog is shown in (b, right). \textbf{2)} Use the same brushing to find a target image with a smiling face (b, left). \textbf{3)} Pulling a slider (d) changes the source \emph{towards} the target -- that is, makes the frowning dog (b) smile like the target, without changing its identity, see result in (c). Optionally, we can adjust the influence range $\sigma$ to affect how other images close to the source also get changed towards the target -- see the circles centered at mouse location in (a).

changing its identity, see result in (c). Optionally, we can adjust the influence range $\sigma$ to affect how other images close to the source also get changed towards the target -- see the circles centered at mouse location in (a). We can also select a few additional points in the projection -- see locations marked 0-4 in (a) -- and inspect how the user-controlled inverse projection behaves there. Point 3 is close to the source, so it will be strongly influenced by the source's change; points 0, 1, and 4 are far away from the source, so they will not be affected.} %%% not updated yet We evaluate our proposed inverse projection method on several datasets and projection techniques. We first study the effect of disentanglement both qualitatively and quantitatively and show that our latent codes $\mathbf{z}$ are indeed independent on the projected information $\mathbf{y}$ (Sec.~\ref{sec:disentanglement}). Then, we compare our inverse projection (without interactive control) to existing inverse projection methods and show we reach similar quality (Sec.~\ref{sec:compare_previous}). Finally, we show that our interactive control can break the surface-like limitation discussed in Sec.~\ref{sec:control_and_ID}.

projected information $\mathbf{y}$ (Sec.~\ref{sec:disentanglement}). Then, we compare our inverse projection (without interactive control) to existing inverse projection methods and show we reach similar quality (Sec.~\ref{sec:compare_previous}). Finally, we show that our interactive control can break the surface-like limitation discussed in Sec.~\ref{sec:control_and_ID}. For $P$, we consider t-SNE and UMAP as these are the two highest-quality techniques in DR landscape\,\citep{espadoto19} and are also used in other inverse-projection studies\,\citep{espadoto2019NNinv,espadoto2021unprojection,schulz2020DeepViewVisualizing,wang2023DMcompare}. We use the following datasets: \textbf{MNIST:} 70K samples of handwritten digits (0-9), each a $28^2$ grayscale image flattened to a 784-size vector\,\citep{lecun2010MNISThandwritten}. \textbf{Fashion-MNIST:} 70K samples of 10 fashion categories (e.g., T-shirts, trousers, dresses), each a $28^2$ grayscale image flattened to a 784-size vector\,\citep{xiao2017FashionMNISTnovel}. \textbf{HAR:} 10K samples of smartphone accelerometer and gyroscope data capturing six human activities (walking, walking upstairs, walking downstairs, sitting, standing, laying)\,\citep{anguita2012human}. \textbf{$\mathcal{W}$ of AFHQv2:} AFHQv2 contains 15K color images of animal faces in 3 classes: dogs, cats, and wild animals\,\citep{choi2020StarGANv2}. Its $\mathcal{W}$ is a $\mathbb{R}^{512}$ latent space used by StyleGAN2 models\,\citep{karras2020AnalyzingImproving} to generate images (see Sec.~\ref{sec:gan}). In the following, for ease of exposition, we will show the generated images $G(\mathbf{w})$ instead of the raw codes $\mathbf{w}$.

cats, and wild animals\,\citep{choi2020StarGANv2}. Its $\mathcal{W}$ is a $\mathbb{R}^{512}$ latent space used by StyleGAN2 models\,\citep{karras2020AnalyzingImproving} to generate images (see Sec.~\ref{sec:gan}). In the following, for ease of exposition, we will show the generated images $G(\mathbf{w})$ instead of the raw codes $\mathbf{w}$. For a dataset $X = \{\mathbf{x}_i\}$, we first compute the projection $Y = \{\mathbf{y}_i\}$, $\mathbf{y}_i = P(\mathbf{x}_i)$. We next split $D$ in a training $D_T=\{(Y_T, X_T)\}$ and testing $D_v=\{(Y_v, X_v)\}$ set (see Sec.~\ref{sec:compare_previous}). We set $\lambda$ (Eqn.~\ref{eqn:j}) to $0.01$ (AFHQv2), $0.05$ (HAR) and $0.1$ (MNIST, Fashion-MNIST) respectively. Table~\ref{tab:datasets} summarizes the above. \caption{Datasets used in our evaluation with dimensionality $n$, sample count $|D|$, training and testing set sizes $|D_T|$ and $|D_v|$, and values of hyperparameter $\lambda$ (Eqn.~\ref{eqn:j}).} \textbf{Dataset} & $n$ & $|D_T|$ & $|D_v|$ & $|D|$ & $\lambda$\\ MNIST & 784 & 5000 & 5000 & 60000 & $0.1$\\ Fashion-MNIST & 784 & 5000 & 5000 & 60000 & $0.1$\\ HAR & 561 & 5000 & 5000 & 10299 & $0.05$\\ $\mathcal{W}$ of AFHQv2 & 512 & 5000 & 5000 & 14336 & $0.01$\\ \subsection{Added value of disentanglement}

$0.1$\\ Fashion-MNIST & 784 & 5000 & 5000 & 60000 & $0.1$\\ HAR & 561 & 5000 & 5000 & 10299 & $0.05$\\ $\mathcal{W}$ of AFHQv2 & 512 & 5000 & 5000 & 14336 & $0.01$\\ \subsection{Added value of disentanglement} To show the added value of the disentanglement, we compare our results using the loss in Eqn.~\ref{eqn:j} (called next \emph{WithDis}) with the same network trained without $L_{adv}$ (called next \emph{NoDis}). \textbf{Quantitative evaluation:} We measure disentanglement by how well can $\mathbf{z}$ predict $\mathbf{y}$. The \emph{worse} $\mathbf{z}$ can predict $\mathbf{y}$, the better the disentanglement, \emph{i.e.}, $\mathbf{z}$ and $\mathbf{y}$ are more independent\,\citep{jaiswal2018Unsupervisedadversarial,jaiswal2019UnifiedAdversarial,moyer2018InvariantRepresentations}. We measure this by training a \emph{post-hoc} regression model to predict $Y_T$ from $Z_T=Enc(X_T)$. This model is a neural network with one hidden layer (100 units) followed by ReLU activation, trained for 200 epochs using the Adam optimizer. We measure $R^2$ and $MSE$ on a hold-out test set. We expect that

$Y_T$ from $Z_T=Enc(X_T)$. This model is a neural network with one hidden layer (100 units) followed by ReLU activation, trained for 200 epochs using the Adam optimizer. We measure $R^2$ and $MSE$ on a hold-out test set. We expect that \emph{WithDis} should yield low $R^2$ and high $MSE$ -- that is, $\mathbf{z}$ and $\mathbf{y}$ are independent and/or different. Conversely, we expect that \emph{NoDis} should yield high $R^2$ and low $MSE$ -- that is, $\mathbf{z}$ and $\mathbf{y}$ are correlated and/or similar. Table~\ref{tab:predict_y_from_z} shows that $MSE$ and $R^2$ for \emph{WithDis} and \emph{NoDis} indeed match the expectations, thus confirm our claimed disentanglement. \begin{table}[!htb]  %%%% this is meassured on a test set splitted from {Z_t, Y_t} \caption{Measuring disentanglement: How well can $\mathbf{z}$ predict $\mathbf{y}$?} \textbf{Dataset} & $P$    & \multicolumn{2}{c|}{\textbf{\emph{WithDis}}} & \multicolumn{2}{c}{\textbf{\emph{NoDis}}} \\ &               & $MSE$           & \textbf{$R^2$}          & $MSE$           & \textbf{$R^2$}           \\ \hline MNIST	& UMAP	& 17.1	& 0.032	& 1.7	& 0.900 \\ MNIST	& tSNE	& 1497.5	& 0.011	& 161.9	& 0.892 \\ FashionMNIST	& UMAP	& 19.9	& 0.201	& 0.7	& 0.968 \\ FashionMNIST	& tSNE	& 1284.4	& 0.096	& 79.4	& 0.944 \\ HAR	& UMAP	& 48.8	& 0.098	& 6.9	& 0.862 \\

161.9 & 0.892 \\ FashionMNIST & UMAP & 19.9 & 0.201 & 0.7 & 0.968 \\ FashionMNIST & tSNE & 1284.4 & 0.096 & 79.4 & 0.944 \\ HAR & UMAP & 48.8 & 0.098 & 6.9 & 0.862 \\ HAR & tSNE & 1337.8 & 0.170 & 152.7 & 0.892 \\ $\mathcal{W}$ of AFHQv2	& UMAP	& 5.8	& 0.014	& 3.6	& 0.402 \\ $\mathcal{W}$ of AFHQv2	& tSNE	& 664.8	& 0.024	& 336.2	& 0.500 \\ \caption{Showing disentanglement on the MNIST dataset. (a) 2D t-SNE projection $Y$. (b) UMAP projection of $Z$ using $WithDis$. (c) Inverse projections of the linear interpolation between two data points in the $Z$ and $Y$ spaces, using $WithDis$ (c) and $NoDis$ (d).} We visually evaluate disentanglement by selecting two samples $\mathbf{x}_1$ and $\mathbf{x}_2$ of MNIST dataset and their t-SNE projections $\mathbf{y}_1$ and $\mathbf{y}_2$. These are two images of digits 0 and 7 -- see Fig.~\ref{fig:show_func_dis}a. Let $\mathbf{z}_1 = Enc(\mathbf{x}_1)$ and $\mathbf{z}_2 = Enc(\mathbf{x}_2)$ be the codes of these two images. These are shown in a UMAP projection of the $\mathbf{z}$ values for the dataset in Fig.~\ref{fig:show_func_dis}b.

images of digits 0 and 7 -- see Fig.~\ref{fig:show_func_dis}a. Let $\mathbf{z}_1 = Enc(\mathbf{x}_1)$ and $\mathbf{z}_2 = Enc(\mathbf{x}_2)$ be the codes of these two images. These are shown in a UMAP projection of the $\mathbf{z}$ values for the dataset in Fig.~\ref{fig:show_func_dis}b. We linearly interpolate between $\mathbf{y}_1$ and $\mathbf{y}_2$, and $\mathbf{z}_1$ and $\mathbf{z}_2$, respectively, with 10 steps in each of these two dimensions. This yields a total of $10^2$ interpolated values $(\mathbf{y}, \mathbf{z})$ which we next inversely project to the data space using both \emph{WithDis} (Fig.~\ref{fig:show_func_dis}c) and \emph{NoDis} (Fig.~\ref{fig:show_func_dis}d).

and $\mathbf{z}_1$ and $\mathbf{z}_2$, respectively, with 10 steps in each of these two dimensions. This yields a total of $10^2$ interpolated values $(\mathbf{y}, \mathbf{z})$ which we next inversely project to the data space using both \emph{WithDis} (Fig.~\ref{fig:show_func_dis}c) and \emph{NoDis} (Fig.~\ref{fig:show_func_dis}d). Several observations follow. First, we see from the t-SNE projection that the label information is well-preserved in the projection space (Fig.~\ref{fig:show_func_dis}a) -- that is, digits of the same \emph{type}, \emph{e.g.}, zeroes or sevens, are well grouped. In contrast, the UMAP projection of $\mathbf{z}$ strongly mixes labels (Fig.~\ref{fig:show_func_dis}b). This is desired, as $\mathbf{z}$ should capture a digit’s writing style and not its class. We next see that, as we change $\mathbf{z}$ using \emph{WithDis}, the \emph{style} of the digit changes -- see columns in Fig.~\ref{fig:show_func_dis}c; while, as we change $\mathbf{y}$, the digit \emph{itself} changes – see rows in Fig.~\ref{fig:show_func_dis}c. Hence, \emph{WithDis} disentangles $\mathbf{y}$ and $\mathbf{z}$ well. In contrast, for \emph{NoDis}, the inverse projection changes only when $\mathbf{z}$ changes – see columns in Fig.~\ref{fig:show_func_dis}d; when $\mathbf{y}$ changes, the digit stays the same – see rows in Fig.~\ref{fig:show_func_dis}d. Hence, \emph{NoDis} keeps the latent codes $\mathbf{z}$ and $\mathbf{y}$ entangled. \subsection{Comparison to other inverse projection methods}

projection changes only when $\mathbf{z}$ changes – see columns in Fig.~\ref{fig:show_func_dis}d; when $\mathbf{y}$ changes, the digit stays the same – see rows in Fig.~\ref{fig:show_func_dis}d. Hence, \emph{NoDis} keeps the latent codes $\mathbf{z}$ and $\mathbf{y}$ entangled. \subsection{Comparison to other inverse projection methods} We now compare our inverse projection \ourname{} to iLAMP\,\citep{Amorim2012ilamp}, RBF\,\citep{amorim2015Facinghighdimensions}, and NNInv\,\citep{espadoto2019NNinv}. We show that \ourname{} yields similar if not higher quality even without using its control mechanism (which we discuss separately in Sec.~\ref{sec:control_and_ID}). \textbf{Inverse projection error:} We first measure the Mean Squared Error (MSE) of the inverse projection on the test set $D_v$ MSE = \frac{1}{|D_v|} \sum_{(\mathbf{y}, \mathbf{x}) \in D_v} \|\mathbf{x} - P^{-1}(\mathbf{y}, \mathbf{z})\|^2, where $\mathbf{z}$ is ignored for iLAMP, RBF, and NNInv. For \ourname, we compute $\mathbf{z}$ at $\mathbf{y} \in \mathbb{R}^2 \setminus Y$ by smoothed RBF and weighted k-NN interpolation of the $\mathbf{z}_i$ values (see Sec.~\ref{sec:initialize_z}). We call these two interpolation variants \ourname{\,(rbf)} and \ourname{\,(knn)}, respectively. We also evaluate $\mathbf{z}_i = Enc(\mathbf{x}_i), \mathbf{x}_i \in X_v$ and denote this computation as \ourname{*}. While this is not the intended way to get $\mathbf{z}$ (as we cannot do this for \emph{any} 2D point, see Sec.~\ref{sec:initialize_z}), this gives the performance of our method if we could compute $\mathbf{z}$ exactly.

and denote this computation as \ourname{*}. While this is not the intended way to get $\mathbf{z}$ (as we cannot do this for \emph{any} 2D point, see Sec.~\ref{sec:initialize_z}), this gives the performance of our method if we could compute $\mathbf{z}$ exactly. \caption{MSE of the studied inverse projections.} Figure~\ref{fig:compare_mse} shows that iLAMP, RBF, NNInv, and \ourname{} have similar MSE on $D_v$. Yet, \ourname{*} has a lower MSE than the others on MNIST and Fashion-MNIST. Hence, our method can produce inverse projections with \emph{lower error} when $\mathbf{z}$ is properly provided. This quality gap between \ourname{*} and \ourname{\,(rbf)} or \ourname{\,(knn)} can be filled by interaction (see next Sec.~\ref{sec:application_afhq}).

MSE than the others on MNIST and Fashion-MNIST. Hence, our method can produce inverse projections with \emph{lower error} when $\mathbf{z}$ is properly provided. This quality gap between \ourname{*} and \ourname{\,(rbf)} or \ourname{\,(knn)} can be filled by interaction (see next Sec.~\ref{sec:application_afhq}). \textbf{Visual quality in gap areas:} The above MSE testing can only be done for points in $D_v$, \emph{i.e.}, where we have ground truth on $P^{-1}$ in terms of data samples. As explained several times so far, a 2D projection space mainly consists of gap areas where no sample projects, \emph{i.e.}, we have no ground truth -- and it is precisely there that one wants to use inverse projections. One way to assess quality there is to study how inversely projected samples from gap areas \emph{look} like. Ideally, we want to obtain \emph{plausible} samples which follow the overall nature of the data in a given dataset.

use inverse projections. One way to assess quality there is to study how inversely projected samples from gap areas \emph{look} like. Ideally, we want to obtain \emph{plausible} samples which follow the overall nature of the data in a given dataset. \caption{Visual comparison of the inverse projection on MNIST dataset. 13 points are selected in the projection space at various distance from projected samples (A-M, top image). The middle set of images shows the inverse projections at these locations using the tested inverse projection techniques. The bottom set of images shows results obtained by inversely projecting points located along a line between the locations U and V in the projection.}

of images shows the inverse projections at these locations using the tested inverse projection techniques. The bottom set of images shows results obtained by inversely projecting points located along a line between the locations U and V in the projection.} Figure~\ref{fig:compare_visual_mnist} shows the four studied $P^{-1}$ methods on MNIST. Images (b) show that \ourname{} produces more plausible digits than the other methods even at locations far away from data samples (\emph{e.g.}, A, C, M). We also see that iLAMP and RBF are sensitive to outliers. For example, there is a single green point (near K, class 2) located in between pink (class 4) and purple (class 6) clusters, while the main green cluster is far down at the bottom of the projection. It makes sense that this green point is an outlier and this gap area is supposed to be something between 6 and 4. Yet, the inverse projection of `K' (close to the green point) is a `2' in iLAMP and RBF, while it is a `6' in NNinv and \ourname{}. Also, iLAMP creates gray backgrounds (\emph{e.g.}, A,B,C,F,M) which are not only far away from the actual data distribution but also not what one would expect for MNIST. Images (c) show the inverse projections from points along a line between images U (digit 9) and V (digit 8) in the projection. iLAMP, RBF, and NNInv generate spurious shapes that do not resemble any digit during this interpolation process. In contrast, LCIP morphs the 9 to an 8 following, we argue, a more natural set of intermediate images.

8) in the projection. iLAMP, RBF, and NNInv generate spurious shapes that do not resemble any digit during this interpolation process. In contrast, LCIP morphs the 9 to an 8 following, we argue, a more natural set of intermediate images. \caption{Visual comparison of the inverse projection on Fashion-MNIST dataset.} On Fashion-MNIST, iLAMP and RBF mix several images in the reconstruction (Fig.~\ref{fig:compare_visual_fmnist}). For example, at point A, iLAMP and RBF mix high heels and boots; at point B, RBF mixes shoes and pants; at point F, iLAMP and RBF mix a wider and a narrower T-shirt. NNinv doesn't have this problem, but it produces jagged (\emph{e.g.}, at points A, B, C, G) or ambiguous shapes (\emph{e.g.}, at points E, F). \ourname{} keeps the reconstructed shape clear and recognizable in all cases.

wider and a narrower T-shirt. NNinv doesn't have this problem, but it produces jagged (\emph{e.g.}, at points A, B, C, G) or ambiguous shapes (\emph{e.g.}, at points E, F). \ourname{} keeps the reconstructed shape clear and recognizable in all cases. Finally, on AFHQv2, all methods work well when the inversely-projected 2D points are close to training samples. Once a 2D point is further away, iLAMP immediately becomes problematic (see A, B, C, E, J, P). In extreme cases, all methods but ours have some issues. For example, at point C (see Fig.~\ref{fig:compare_visual_afhq}), iLAMP produces a mass of colors with indiscernible shapes; the image by RBF has color distortions and the dog in the image only has one ear; NNinv produces a dog but in a strange appearance, also with color distortions. In contrast, \ourname{} produces realistic images in all cases. \caption{Visual comparison of the inverse projection, AFHQv2 dataset. (a) Dataset projection with 16 selected locations both close and far away from the data samples (A-P). (b) Inversely projected resulting images at these locations created by all tested methods. (c) Results obtained by inversely projecting points along a line between the locations U and V in the projection.}

and far away from the data samples (A-P). (b) Inversely projected resulting images at these locations created by all tested methods. (c) Results obtained by inversely projecting points along a line between the locations U and V in the projection.} We next evaluate smoothness using \emph{gradient maps}, introduced in Sec.~\ref{sec:inv_quality} and used extensively in Chapter~\ref{ch4:metrics_decision_map}. Figure~\ref{fig:gradient_map_mnist} shows these maps for the tested inverse projections on the MNIST dataset with the UMAP projection; Bright colors indicate high gradients, that is, points where the inverse projection `jumps' in data space when its input moves between neighbor pixels. Such jumps are undesirable (see Sec.~\ref{sec:inv_quality}). We see that LCIP achieves the smallest gradient norms (see color legends) for all datasets and direct projections. \caption{Gradient maps for the tested inverse projections for UMAP on MNIST.}

input moves between neighbor pixels. Such jumps are undesirable (see Sec.~\ref{sec:inv_quality}). We see that LCIP achieves the smallest gradient norms (see color legends) for all datasets and direct projections. \caption{Gradient maps for the tested inverse projections for UMAP on MNIST.} Figure~\ref{fig:time_record} shows timings for the four studied inverse projection methods on a desktop with an Intel Core i5-12400 CPU and NVIDIA GeForce GTX 3090 GPU. The y-axis shows total time, \emph{i.e.} the (constant) training time and inference time (linear in the projected points count). All methods show similar speed except iLAMP which is significantly slower. Although \ourname{} requires slightly more training time due to its adversarial training, its slope is nearly identical to NNinv and RBF, showing the same high scalability. Separately, our evaluations show that the results of \ourname{\,(rbf)} and \ourname{\,(knn)} are very similar. Since \ourname{\,(rbf)} is theoretically smoother, we will use it in our following experiments. \caption{Inverse projection speed. The training time equals the y-intercept of the graphs. The slopes of the graphs depict how inference time depends on the number of inversely projected samples.} \subsection{Controllability: Going beyond a fixed surface}

it in our following experiments. \caption{Inverse projection speed. The training time equals the y-intercept of the graphs. The slopes of the graphs depict how inference time depends on the number of inversely projected samples.} \subsection{Controllability: Going beyond a fixed surface} We have shown so far that \ourname{} can construct a `fixed' inverse projection from a given dataset $X$ and its projection $P(X)$ with results which are comparable -- and often better -- than other existing inverse projection techniques in terms of generating plausible results in gap areas, inverse projection MSE, inverse projection smoothness, and speed. Yet, the key feature of our method is its ability to \textbf{dynamically} control the inverse projection, which we describe next.

inverse projection techniques in terms of generating plausible results in gap areas, inverse projection MSE, inverse projection smoothness, and speed. Yet, the key feature of our method is its ability to \textbf{dynamically} control the inverse projection, which we describe next. \textbf{Effects of control:} Figures~\ref{fig:add_z_MNIST}-\ref{fig:add_z_afhq} show how the inverse projection changes when their $\mathbf{z}$ values are adjusted towards selected targets in MNIST, Fashion-MNIST, and AFHQv2. Targets are marked by $\mathbf{x}_t$ (blue) in Fig.~\ref{fig:compare_visual_mnist}-\ref{fig:compare_visual_afhq} with their images shown as insets. In Figs.~\ref{fig:add_z_MNIST}-\ref{fig:add_z_afhq}, topmost rows show the selected source images $(\alpha = 0)$; rows below show how the inverse projection `sweeps' the data space between source and target as the user increases $\alpha$. For example, when selecting an italic-like `0' digit in MNIST as target, as we increase $\alpha$, the inverse projection gradually become more italic, no matter which is the selected source image (Fig.~\ref{fig:add_z_MNIST}). We see similar changes in Fashion-MNIST, such as subtle changes in the style of shoes and shirts (Fig.~\ref{fig:add_z_MNIST} bottom). For AFHQv2, the selected target is a dog tilting its head -- see inset image in Fig.~\ref{fig:compare_visual_afhq}. As we increase $\alpha$, all selected source animals in the inverse projection rotate their heads with similar angles (Fig.~\ref{fig:add_z_afhq}).

and shirts (Fig.~\ref{fig:add_z_MNIST} bottom). For AFHQv2, the selected target is a dog tilting its head -- see inset image in Fig.~\ref{fig:compare_visual_afhq}. As we increase $\alpha$, all selected source animals in the inverse projection rotate their heads with similar angles (Fig.~\ref{fig:add_z_afhq}). \caption{Controlling the inverse projection for MNIST (a) and Fashion-MNIST (b). Targets are the blue-outlined inset images in Fig.~\ref{fig:compare_visual_mnist} for (a) and Fig.~\ref{fig:compare_visual_fmnist} for (b). Rows in the two images show the effect of increasing user control $\alpha$.} \caption{Controlling the inverse projection for AFHQv2. The target is the blue-outlined image in Fig.~\ref{fig:compare_visual_afhq}.}

are the blue-outlined inset images in Fig.~\ref{fig:compare_visual_mnist} for (a) and Fig.~\ref{fig:compare_visual_fmnist} for (b). Rows in the two images show the effect of increasing user control $\alpha$.} \caption{Controlling the inverse projection for AFHQv2. The target is the blue-outlined image in Fig.~\ref{fig:compare_visual_afhq}.} \textbf{Increased reach:} Recall, from Chapter~\ref{ch5:surface_like_behavior}, that the main limitation we identified for existing inverse projections was their fixed surface-like behavior. While \ourname{} can generate inverse projections that go beyond a fixed surface, a key question is \emph{how much} beyond such a surface can we reach. To measure this, we evaluate the Intrinsic Dimensionality (ID) of the inverse projection by the Minimal Variance method\,\citep{tian2021Usingmultiple}, for all pixels in an image ($300 \times 300$), without interaction. We call this ID value the \emph{baseline}. Next, we adjust the inverse projection by globally adding $\Delta \mathbf{z}$ values, for 50 uniformly sampled values of $\alpha \in [0, 0.2]$, and measure the ID of all resulting inversely-projected points taken together.

We call this ID value the \emph{baseline}. Next, we adjust the inverse projection by globally adding $\Delta \mathbf{z}$ values, for 50 uniformly sampled values of $\alpha \in [0, 0.2]$, and measure the ID of all resulting inversely-projected points taken together. Figure~\ref{fig:id_map_compare} shows that the baseline has $ID \simeq 2$ at roughly all pixels (with small higher-ID areas close to the sample points), so we generate roughly a surface embedded in $\mathbb{R}^n$, much like other inverse projection techniques (\Cref{ch5:surface_like_behavior}). When using control, we get an ID roughly equal to 3, \emph{i.e.}, $\Delta \mathbf{z}$ `shifts' our inverse-projection surface to span a 3D space in $\mathbb{R}^n$. Some ID values of 2 are likely due to the $\mathbf{z}$ value of those areas being insensitive to the selected target, \emph{i.e.}, $\mathbf{z}_t$ and $\mathbf{z}_{p_s}$ are close in the first place. Note that we only use a \emph{single} target point here. If we used $k$ target samples $\mathbf{x}_t$ which span a $k$-dimensional space in $\mathbb{R}^n$, we would obtain an inverse projection of ID $\simeq k$. \caption{Intrinsic dimensionality of \ourname{} without and with interaction for different datasets and direct projections.}

point here. If we used $k$ target samples $\mathbf{x}_t$ which span a $k$-dimensional space in $\mathbb{R}^n$, we would obtain an inverse projection of ID $\simeq k$. \caption{Intrinsic dimensionality of \ourname{} without and with interaction for different datasets and direct projections.} \textbf{Flexibility of control:} We further study the impact of using more control targets $\mathbf{x}_t$. For this, we consider a training set $X_T$ with 5000 points. For each $\mathbf{x}_i \in X_T$, we compute its latent code $\mathbf{z}_i = Enc(\mathbf{x}_i)$, consider it as a target for our control mechanism, and consider each pixel $\mathbf{p}$ of the image as source point. For each such $\mathbf{p}$, we measure the variance V(\mathbf{p}) = \frac{\sum_{j=1}^{n}Var(\{P^{-1}(\mathbf{p}, \mathbf{z}_i)\}^j)}{\sum_{j=1}^{n}Var({X_T}^j)} over the set $\{ Dec(\mathbf{p}, \mathbf{z}_i) \}$, across all $n$ data dimensions. Here, superscript $j$ denotes the $j^{th}$ dimension of a data sample. Higher values of $V(\mathbf{p})$ indicate that the inverse projection of $\mathbf{p}$ is more sensitive to control, and vice versa. The denominator normalization factor in Eqn.~\ref{eq:variance_map} accounts for the spread of the training set $X_T$ in data space. If this spread is low, then we should not expect that our inverse projection reaches far further in the data space, and vice versa.\begin{figure}[!htb]

normalization factor in Eqn.~\ref{eq:variance_map} accounts for the spread of the training set $X_T$ in data space. If this spread is low, then we should not expect that our inverse projection reaches far further in the data space, and vice versa.\begin{figure}[!htb] \caption{Normalized variance of inverse projections produced by 5000 different $\mathbf{z}$ values, t-SNE direct projection. See Eqn.~\ref{eq:variance_map} and related text.}

is low, then we should not expect that our inverse projection reaches far further in the data space, and vice versa.\begin{figure}[!htb] \caption{Normalized variance of inverse projections produced by 5000 different $\mathbf{z}$ values, t-SNE direct projection. See Eqn.~\ref{eq:variance_map} and related text.} Figure~\ref{fig:var_stack_sruf} shows the results. On $\mathcal{W}$ of AFHQv2 and HAR, variance is lower near data samples and higher in gap areas, telling that inverse projections are more `nailed' when there is ground-truth data around, and more flexible when no ground truth nearby. On MNIST and Fashion-MNIST, the pattern seems not to be related to the distance to data samples. We believe that this is an effect of t-SNE's known tendency of t-SNE to compress (or stretch) point neighborhoods from data space to projection space\,\citep{tvisne}. That is, areas showing a low normalized variance in Fig.~\ref{fig:var_stack_sruf} may actually map points which are close in data space, where our inverse projection does not have the freedom to move much; conversely, areas of high variance may map points far away in data space, where our inverse projection has more freedom to move. These maps provide users with insights on where interaction is likely to be most \emph{effective}: Interacting with source points in high-variance areas will produce inverse projections which `sweep' the data space more freely, which is the core goal of interaction; interacting with source points in low-variance areas is likely not going to discover new areas in data space.

points in high-variance areas will produce inverse projections which `sweep' the data space more freely, which is the core goal of interaction; interacting with source points in low-variance areas is likely not going to discover new areas in data space. \section{User control of the inverse projection} We now show the added value of the controllability of our inverse projection method. All experiments use the interactive tool shown in Fig.~\ref{fig:teaser}a. Based on how far the selected source $\mathbf{p}_s$ is from the selected target $P(\mathbf{x}_t)$ in projection space, we distinguish two types of control: (1) target is close to source and (2) target is far away from source. In (1), we expect the controlled $P^{-1}$ to gradually but \emph{fully} change towards the target; In (2), we only expect a \emph{partial} change, \emph{e.g.,} style-wise. We detail both scenarios next. \subsection{Local control: Target is close to source}

away from source. In (1), we expect the controlled $P^{-1}$ to gradually but \emph{fully} change towards the target; In (2), we only expect a \emph{partial} change, \emph{e.g.,} style-wise. We detail both scenarios next. \subsection{Local control: Target is close to source} Consider the limit case where $P(\mathbf{x}_t) = \mathbf{p}_s$. The difference between the target $\mathbf{x}_t$ and the controlled inverse projection $Dec(\mathbf{p}_s, \mathbf{z}_{p_s})$ is fully controlled by $\mathbf{z}_{p_s}$. When $\mathbf{z}_{p_s} = Enc(\mathbf{x}_t)$, the inverse projection of $\mathbf{p}_s$ becomes $Dec(P(\mathbf{x}_t), Enc(\mathbf{x}_t)) \simeq \mathbf{x}_t$. In other words, the inverse projection \emph{fully} changes towards the target $\mathbf{x}_t$. We next show how our control helps with two challenges that frequently occur in inverse projection usage. \caption{Close and far-away control of the inverse projection, AFHQv2 dataset. (a) 2D locations where we performed interaction.  See also Figs.~\ref{fig:control_1},\ref{fig:control_2}. (b) Assessing smoothness of controlled inverse projection at various sampling locations (\textbf{\color{red}+}). Color encodes the distance $\| \mathbf{q} -\mathbf{q}^{user} \|$ at each pixel. Figure~\ref{fig:before_after_interaction} shows the inverse projections at these sampling locations.}

locations where we performed interaction. See also Figs.~\ref{fig:control_1},\ref{fig:control_2}. (b) Assessing smoothness of controlled inverse projection at various sampling locations (\textbf{\color{red}+}). Color encodes the distance $\| \mathbf{q} -\mathbf{q}^{user} \|$ at each pixel. Figure~\ref{fig:before_after_interaction} shows the inverse projections at these sampling locations.} \textbf{Inverse projection correction:} While good inverse projections should have a low MSE (Eqn.~\ref{eq:mse}), they do not yield $P^{-1}(P(\mathbf{x})) = \mathbf{x}$ at \emph{all} projected samples $P(\mathbf{x})$. For instance, in Fig.~\ref{fig:locations_on_map}a, $\mathbf{p}_{s0}$ and $\mathbf{p}_{s1}$ are two points at the margin of clusters. As such, their inverse projections $\mathbf{q}_{s0}$ and $\mathbf{q}_{s1}$ should barely be influenced by other points and, ideally, equal the data samples $\mathbf{x}_{t0}$ and $\mathbf{x}_{t1}$ that project there. Yet, this is not the case -- compare images $\mathbf{q}_{s0}$, $\mathbf{q}_{s1}$ to $\mathbf{x}_{t0}$, $\mathbf{x}_{t1}$ in Fig.~\ref{fig:control_1}, respectively. We can adjust our controllable inverse projection to make it \emph{closer to the data}. As we add $\Delta \mathbf{z}$ to $\mathbf{z}_{p_s}$, the inverse projection $\mathbf{q}^{user}$ gradually changes towards the target $\mathbf{x}_t$ -- see Fig.~\ref{fig:control_1} top two rows. The distances $\| \mathbf{q}^{user} - \mathbf{x}_t\|$ are also reduced, reaching minima for $\alpha=1$ (Fig.~\ref{fig:control_1}).

to the data}. As we add $\Delta \mathbf{z}$ to $\mathbf{z}_{p_s}$, the inverse projection $\mathbf{q}^{user}$ gradually changes towards the target $\mathbf{x}_t$ -- see Fig.~\ref{fig:control_1} top two rows. The distances $\| \mathbf{q}^{user} - \mathbf{x}_t\|$ are also reduced, reaching minima for $\alpha=1$ (Fig.~\ref{fig:control_1}). \caption{Local control -- adjusting the inverse projection when $P(\mathbf{x}_t) \simeq \mathbf{p}_s$. Locations of $\mathbf{q}_s$ and $P(\mathbf{x}_t)$ are shown in Fig.~\ref{fig:locations_on_map}a. Numbers in upper right corners show $\|\mathbf{x}_t - \mathbf{q}_s^{user}\|$. Numbers in lower left corners show the control parameter $\alpha$.}

(Fig.~\ref{fig:control_1}). \caption{Local control -- adjusting the inverse projection when $P(\mathbf{x}_t) \simeq \mathbf{p}_s$. Locations of $\mathbf{q}_s$ and $P(\mathbf{x}_t)$ are shown in Fig.~\ref{fig:locations_on_map}a. Numbers in upper right corners show $\|\mathbf{x}_t - \mathbf{q}_s^{user}\|$. Numbers in lower left corners show the control parameter $\alpha$.} \textbf{Overlap separation:} Overlapping points in a projection are common. In such areas, $P$ is not injective, so formally it is not invertible. For example, in Fig.~\ref{fig:locations_on_map}a, the area around point $\mathbf{p}_{s2}$ -- which corresponds to a cat (denoted $\mathbf{x}_{t2}$) -- shows several overlapping samples of cats (blue) and wild animals (green). This overlap is due to the projection -- the samples are actually separable in data space. Existing inverse projection methods will map $\mathbf{p}_{s2}$ to a cat, a wild animal, or a mix of them in a \emph{fixed} and \emph{not controllable} way. Our method allows flexibly controlling the inverse projection of $\mathbf{p}_{s2}$ to be more like a cat or a wild animal: We see that $\mathbf{q}_{s2}$ is initially a fox (Fig.~\ref{fig:control_1}, bottom row, left). Setting the cat image $\mathbf{x}_{t2}$ as the target and $\mathbf{p}_{s2}$ as source, we see how the inverse projection gradually transforms from a fox into a cat as we increase $\alpha$ (Fig.~\ref{fig:control_1}, bottom row, columns $\mathbf{q}_s^{user}$).

fox (Fig.~\ref{fig:control_1}, bottom row, left). Setting the cat image $\mathbf{x}_{t2}$ as the target and $\mathbf{p}_{s2}$ as source, we see how the inverse projection gradually transforms from a fox into a cat as we increase $\alpha$ (Fig.~\ref{fig:control_1}, bottom row, columns $\mathbf{q}_s^{user}$). \subsection{Far-away control: Target is far from source} In this scenario, the controlled $P^{-1}(\mathbf{p}_s, \mathbf{z}_{p_s})$ will not go completely toward $\mathbf{x}_t$, since $P(\mathbf{x}_t) \not\simeq \mathbf{p}_s$. Rather, only what is controlled by $\mathbf{z}$ will change. Yet, it is hard to tell what $\mathbf{z}$ \emph{exactly} controls without prior knowledge or exploration since this depends on the information that is not captured by the projection $\mathbf{y}$, which in turns depends on the actual projection technique $P$ used. In our studies, we found that, for the MNIST--t-SNE combination, the digit is controlled by $\mathbf{y}$, while $\mathbf{z}$ controls the digit's style; for the AFHQv2--t-SNE combination, the type of animal faces is controlled by $\mathbf{y}$, while the animal poses are controlled by $\mathbf{z}$. Controlling $\mathbf{z}$ shapes the inverse-projected surface as desired, enabling applications like user-controlled data generation, as illustrated next.

digit's style; for the AFHQv2--t-SNE combination, the type of animal faces is controlled by $\mathbf{y}$, while the animal poses are controlled by $\mathbf{z}$. Controlling $\mathbf{z}$ shapes the inverse-projected surface as desired, enabling applications like user-controlled data generation, as illustrated next. Consider a source point $\mathbf{p}_s$ far from any target, \emph{i.e.}, within a so-called gap area in the projection. All existing inverse-projection techniques produce surface-like structures in such areas (see \Cref{ch5:surface_like_behavior}). Our control breaks this limitation: Take point $\mathbf{p}_{s3}$ which is in a gap area (Fig.~\ref{fig:locations_on_map}a). Its inverse projection is a sad-looking puppy ($\mathbf{q}_{s3}$, Fig.~\ref{fig:control_2} top-left). We now pick as the target a happy dog ($\mathbf{x}_{t3}$ in Fig.~\ref{fig:locations_on_map}a, Fig.~\ref{fig:control_2} top-right). As we increase $\Delta \mathbf{z}$, the sad puppy gradually smiles and eventually laughs with open mouth (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, top row). Note that the puppy's appearance did not change to the target one (\emph{e.g.}, hair color and ear shapes); only its `style' changed. This control also works with source and target from different \emph{classes}. The inverse projection $\mathbf{q}_{s4}$ of $\mathbf{p}_{s4}$ is a cat looking ahead (Fig.~\ref{fig:control_2}, left column, second-top image). We choose a tiger looking up as the target ($\mathbf{x}_{t4}$ in Fig.~\ref{fig:control_2}, right column, second-top image). By adding $\Delta \mathbf{z}$, the cat gradually looks up without becoming a tiger (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, second row).

looking ahead (Fig.~\ref{fig:control_2}, left column, second-top image). We choose a tiger looking up as the target ($\mathbf{x}_{t4}$ in Fig.~\ref{fig:control_2}, right column, second-top image). By adding $\Delta \mathbf{z}$, the cat gradually looks up without becoming a tiger (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, second row). \caption{Far-away control -- adjusting the inverse projection when $P(\mathbf{x}_t) \not\simeq \mathbf{p}_s$. Locations of $\mathbf{q}_s$ and $P(\mathbf{x}_t)$ are shown in Fig.~\ref{fig:locations_on_map}a.} Decreasing $\Delta \mathbf{z}$ gets an opposite effect. We first choose as source point $\mathbf{p}_{s5}$ a front-facing cheetah (Fig.~\ref{fig:control_2}, left column, third image from top). Our target is a left-facing dog $\mathbf{x}_{t5}$ (Fig.~\ref{fig:control_2}, right column, third image from top). By adding $\Delta \mathbf{z}$, the cheetah gradually turns its head to left (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, row 3). Decreasing $\Delta \mathbf{z}$, the cheetah turns its head to right (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, row 4). At around $\alpha=-0.55$, the cheetah starts changing into a tiger. This is not surprising since cheetahs and tigers overlap in the projection. This decrease operation triggers the overlap separation (see Sec.~\ref{sec:scenario_1}).

turns its head to right (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, row 4). At around $\alpha=-0.55$, the cheetah starts changing into a tiger. This is not surprising since cheetahs and tigers overlap in the projection. This decrease operation triggers the overlap separation (see Sec.~\ref{sec:scenario_1}). A final example considers a point $\mathbf{p}_{s6}$ in a gap area whose inverse projection is a black-and-white, front-facing, cat (Fig.~\ref{fig:control_2}, left column, bottom image). We set a left-facing dog as the target ($\mathbf{x}_{t6}$, Fig.~\ref{fig:control_2}, right column, bottom image). Increasing $\Delta \mathbf{z}$, the cat gradually turns its head left (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, row 5). Decreasing $\Delta \mathbf{z}$, the cat tilts its head to the opposite direction and changes fur color to brown (Fig.~\ref{fig:control_2}, $\mathbf{q}^{user}$, row 6). These examples show the flexibility and potential of LCIP. By adjusting $\Delta \mathbf{z}$, users can control subtle \emph{style} features, such as the expression or direction a subject is facing, without altering a sample's fundamental \emph{identity}. This ability extends across different classes and also in areas far from any projected data sample. This shows LCIP's support for user-controlled data generation, where maintaining the original data's integrity, while introducing desired variations, is crucial\,\citep{wang2022USTNetUnsupervised}. \subsection{Smoothness of controlled projections}

This ability extends across different classes and also in areas far from any projected data sample. This shows LCIP's support for user-controlled data generation, where maintaining the original data's integrity, while introducing desired variations, is crucial\,\citep{wang2022USTNetUnsupervised}. \subsection{Smoothness of controlled projections} Section~\ref{sec:smoothness_compare} showed the smoothness of our inverse projection in absence of user control. User control should create a smooth inverse projection since the input of $Dec$ smoothly varies with $\mathbf{p}$, $\alpha$, and $\sigma$ (Eqn.~\ref{eq:control_kernel}) and since $Dec$ itself is smooth (being the neural network described in Sec.~\ref{sec:method}). Yet, it is worth to practically test this smoothness. For this, we choose 7 source points $\mathbf{p}_{s1} \ldots \mathbf{p}_{s6}$ and sample several lines $A_1 \ldots A_2$ -- $G_1 \ldots G_2$ in the projection space, with 8 samples per line. Figure~\ref{fig:locations_on_map}b shows the sampling locations (\textbf{\color{red}+}) and source points ($\bigstar$). We next set different $\alpha$ and $\sigma$ values for the source points and compute their inverse projections. Figure~\ref{fig:before_after_interaction} shows these inverse projections without and with control. Images in each row -- thus, over a set of linearly-spread sampling points -- smoothly change in both cases. To further confirm this, Fig.~\ref{fig:locations_on_map}b color-codes the difference between the inverse projection without and with control $\| \mathbf{q} - \mathbf{q}^{user} \|$ at each pixel. The result is a \emph{smooth} signal, with large values close to the source points and small values further away, exactly as aimed by the local control aimed in Eqn.~\ref{eq:control_kernel}.

and with control $\| \mathbf{q} - \mathbf{q}^{user} \|$ at each pixel. The result is a \emph{smooth} signal, with large values close to the source points and small values further away, exactly as aimed by the local control aimed in Eqn.~\ref{eq:control_kernel}. \caption{Control smoothness: Inverse projections at 8 sampling points (columns) around 7 different source points (rows), with and without user control. Each image corresponds to a `\textbf{\color{red}+}' in Fig.~\ref{fig:locations_on_map}b.} We next discuss several aspects of our controllable inverse projection method. \textbf{Controllability:} To our knowledge, our method shows for the \emph{first time} that an inverse projection can be controlled by users -- rather than being fully prescribed by a dataset $X$ and its projection $P(X)$. This also breaks the limitation that inverse projections land on a surface embedded in the data space (Sec.~\ref{sec:control_and_ID}).

an inverse projection can be controlled by users -- rather than being fully prescribed by a dataset $X$ and its projection $P(X)$. This also breaks the limitation that inverse projections land on a surface embedded in the data space (Sec.~\ref{sec:control_and_ID}). \textbf{Genericity:} While we illustrated LCIP with style transfer for images, our method is not limited to manipulating image data and, also, should not be seen as \emph{competing} with other dedicated style-transfer methods. We chose the image style-transfer application as it is a very simple and illustrative one for our technique. Yet, our method can be applied to control inverse projections generated by any user-selected projection $P(X)$ of any high-dimensional dataset $X$. To our knowledge, this is the first inverse projection method that allows such control for any projection technique $P$ and any dataset.

can be applied to control inverse projections generated by any user-selected projection $P(X)$ of any high-dimensional dataset $X$. To our knowledge, this is the first inverse projection method that allows such control for any projection technique $P$ and any dataset. \textbf{Quality:} Our evaluations show that our inverse projection has better quality in gap areas as compared to existing inverse projection techniques (Sec.~\ref{sec:compare_previous}). Our inverse projection creates smoother-varying data samples (when the input 2D points smoothly change). This is an important added value point for any user-driven applications of inverse projections since a lack of smoothness in the inverse projection would cause at least difficulty, and most likely confusion, for users who aim to control how 2D points are backprojected to the data space. \textbf{Scalability:} Our method scales in the size and dimensionality of its input data as well as state-of-the-art inverse projection methods, \emph{e.g.}, NNinv (Sec.~\ref{sec:speed}). Its speed is linear in the number of inversely projected points.

are backprojected to the data space. \textbf{Scalability:} Our method scales in the size and dimensionality of its input data as well as state-of-the-art inverse projection methods, \emph{e.g.}, NNinv (Sec.~\ref{sec:speed}). Its speed is linear in the number of inversely projected points. \textbf{Ease of use:} To inversely project a so-called source point, our method requires at minimum only selecting that point in a 2D projection space. If one desires to modify the inverse projection, the user needs additionally to select one target point (from the projected ones) and a `pull' factor $\alpha$ that tells how much the source point will change towards the target. All such operations are simple to perform in a general-purpose user interface by clicking (to select points) and pulling a slider (to change $\alpha$). Additionally, users can change how smooth the inverse projection is around a source point by changing $\sigma$ -- again, by pulling a slider.

in a general-purpose user interface by clicking (to select points) and pulling a slider (to change $\alpha$). Additionally, users can change how smooth the inverse projection is around a source point by changing $\sigma$ -- again, by pulling a slider. \textbf{Limitations:} First and foremost, our method relies implicitly on disentangling the information captured by a projection operation $P$ from what $P$ cannot capture, and subsequently manipulating this information to control the inverse projection. From a technical perspective, we argued that this makes good sense. However, from a practical perspective, it is by far not clear to the common user what a given $P$ would capture (and thus not subject to user control) and what would be, consequently, left to the said control. Our experiments showed that, for selected datasets, the projection captures the core similarity of items, while the lost information -- under user control -- mainly affects a generic attribute we called `style'. Yet, what exactly style is; how it differs from what a given projection captures; and how much is this style controllable in practice, are all questions that we cannot formally answer for all datasets and all projection methods.

called `style'. Yet, what exactly style is; how it differs from what a given projection captures; and how much is this style controllable in practice, are all questions that we cannot formally answer for all datasets and all projection methods. In connection to this, it is not yet obvious how the control of inverse projections which we proposed can assist a \emph{broad} set of applications. Our examples of style transfer on images are, we hope, convincing. Yet, more use-cases, and ideally user studies, would be needed to strengthen our claims of added value concerning our inverse projection method.

can assist a \emph{broad} set of applications. Our examples of style transfer on images are, we hope, convincing. Yet, more use-cases, and ideally user studies, would be needed to strengthen our claims of added value concerning our inverse projection method. In this chapter, we have presented an inverse projection method that allows users to explicitly break the barrier of creating two-dimensional, fixed, surfaces embedded in the data space -- a property exhibited by all inverse projection methods we are aware of. To do this, we split the information present in a high-dimensional dataset into the information which is captured by a projection technique and, separately, the information that such a technique cannot capture -- the latter which we call a latent code. Next, we allow users to control this latent code in a simple but effective manner and thereby generate a dynamic inverse projection which can effectively `sweep' the data space between the samples used by the direct projection it aims to invert.

we allow users to control this latent code in a simple but effective manner and thereby generate a dynamic inverse projection which can effectively `sweep' the data space between the samples used by the direct projection it aims to invert. Several experiments show that our proposal meets a number of key requirements which are important for inverse projections: Our method is -- in absence of user control -- at least as accurate, and practically as computationally scalable, as state-of-the-art inverse projection techniques. When user control is added, our method can generate a family of inverse projections that span the data space in a controlled manner -- that is, by letting users tell where the inverse projection should adapt to so-called target data points, and where it should be purely driven by the underlying direct projection. This control is simple to perform as it involves changing two simple-to-understand linear parameters -- an action radius and an action amount. Moreover, our inverse projection method has smoothness properties which are required from methods in this class for application deployment, up to a higher degree than other inverse projection methods we are aware of. Also, we showed that out method can cover a larger area of the data space -- measured in terms of intrinsic dimensionality -- than other inverse projection methods. Last but not least, we showed that our inverse projection method creates data samples (images in our studies) which look more natural than those created by existing methods in the same class.

intrinsic dimensionality -- than other inverse projection methods. Last but not least, we showed that our inverse projection method creates data samples (images in our studies) which look more natural than those created by existing methods in the same class. The key following point to this work, we believe, is its direct application in practice, to validate its effectiveness. We envisage, in the short term, applications such as data augmentation, pseudolabeling, data interpolation, and morphing. Using our method in such contexts is, we believe, of significant added value -- though, we acknowledge, such concrete studies are a subject of future work.\chapter{Fast Computation for Decision Maps and Classifier Maps} As introduced in \Cref{ch2:related,ch4:metrics_decision_map}, decision maps and their associated extensions such as gradient maps\,\citep{espadoto2021unprojection} or differential decision maps\,\citep{differentiableDBM} are powerful tools for understanding the behavior of ML classification models and exploring high dimensional data.

for Decision Maps and Classifier Maps} As introduced in \Cref{ch2:related,ch4:metrics_decision_map}, decision maps and their associated extensions such as gradient maps\,\citep{espadoto2021unprojection} or differential decision maps\,\citep{differentiableDBM} are powerful tools for understanding the behavior of ML classification models and exploring high dimensional data. However, computing a decision map, even at quite small resolutions of hundreds of pixels squared, can take tens of seconds up to tens of minutes, depending on the decision map technique (\Cref{ch4:metrics_decision_map}). This precludes using such DBMs in scenarios where users aim to \emph{interactively and iteratively} improve a classification model by \emph{e.g.} changing its hyperparameters or performing data pseudo-labeling in active learning settings\,\citep{benato2018SemiSupervisedLearning,schulz2020DeepViewVisualizing,benato2021SemiAutomaticData}. In this chapter, we propose a method that accelerates the computation of decision maps and their enhancements, thereby addressing our last research question \textbf{RQ4} introduced in \Cref{ch1:intro}\footnote{This chapter is based on the papers ``Computing fast and accurate decision boundary maps''\,\citep{fastDBM2024} and ``Computing Fast and Accurate Maps for Explaining Classification Models''\,\citep{wang2025fastDBMcag}.}.

computation of decision maps and their enhancements, thereby addressing our last research question \textbf{RQ4} introduced in \Cref{ch1:intro}\footnote{This chapter is based on the papers ``Computing fast and accurate decision boundary maps''\,\citep{fastDBM2024} and ``Computing Fast and Accurate Maps for Explaining Classification Models''\,\citep{wang2025fastDBMcag}.}. To achieve the above, we propose FastDBM, a set of techniques that speeds up the computation of DBM images. FastDBM has a very low error rate -- only a few tens of pixels, located on decision boundaries, are different from the ground-truth, slow, DBM computation. Our method can speed up any DBM that encodes classifier label and confidence without inner knowledge of how the model operates. Also, the method is simple to implement and has no hidden parameters. Additionally, we show that the basic FastDBM can be easily extended to compute any real-value classifier map that depends only on sample positions, such as the gradient maps and differential decision maps mentioned above, by a simple modification. We show that this modification still keeps the attractive speed-up and low error rates proposed by the basic FastDBM technique. We also explore additional combination of techniques and quality metrics introduced in~\Cref{ch4:metrics_decision_map} to gauge the added value of our proposal.

We show that this modification still keeps the attractive speed-up and low error rates proposed by the basic FastDBM technique. We also explore additional combination of techniques and quality metrics introduced in~\Cref{ch4:metrics_decision_map} to gauge the added value of our proposal. The structure of this chapter is as follows. Section~\ref{sec:background_ch7} introduces related work on decision maps and classifier maps and techniques used for computing these such as direct and inverse projections. Section~\ref{sec:method_ch7} presents the core of our FastDBM technique which can be used to compute decision maps. ection~\ref{sec:results_ch7} evaluates the three acceleration heuristics we proposed for FastDBM and outlines the winning heuristic: binary split. Section~\ref{sec:additional_results} presents additional evaluations focusing on the binary split heuristic. Section~\ref{sec:general_fast_map} presents our extension of FastDBM to handle general real-valued maps as well as examples of accelerating three such map types. Section~\ref{sec:discussion_ch7} discusses the features of our method. Finally, Sec.~\ref{sec:conclusion} concludes the chapter.

evaluations focusing on the binary split heuristic. Section~\ref{sec:general_fast_map} presents our extension of FastDBM to handle general real-valued maps as well as examples of accelerating three such map types. Section~\ref{sec:discussion_ch7} discusses the features of our method. Finally, Sec.~\ref{sec:conclusion} concludes the chapter. Following our by now established notations, which we repeat here for easing the reading, let $D = \{\mathbf{x}_i\} \subset \mathbb{R}^n$ be a high-dimensional dataset with samples $\mathbf{x}_i$. A classification model $f : \mathbb{R}^n \rightarrow C$, trained and/or tested on $D$, maps samples from the data space to a categorical (label) domain $C$. Let $c : \mathbb{R}^n \rightarrow [0,1]$ denote the confidence of this classification. A \emph{decision map} is a two-dimensional image $I$ that aims to capture $f$'s behavior by extrapolating it from $D$. Three elements are key to the construction of $I$, as follows: {\textbf{Direct projection:} Let $P : D \rightarrow \mathbb{R}^2$ be a projection, operation; and $P(D)$ be the mapping of $D$ to a 2D scatterplot computed by $P$, that is, $P(D) = \{ P(\mathbf{x}) | \mathbf{x} \in D \}$. $P$ maps the input space of the model $f$ to $\mathbb{R}^2$, next allowing the construction of the decision map image $I$.}

$D$ to a 2D scatterplot computed by $P$, that is, $P(D) = \{ P(\mathbf{x}) | \mathbf{x} \in D \}$. $P$ maps the input space of the model $f$ to $\mathbb{R}^2$, next allowing the construction of the decision map image $I$.} \textbf{Inverse projection:} An inverse projection $P^{-1} : \mathbb{R}^2 \rightarrow \mathbb{R}^n$ inversely maps, or backprojects, any pixel $\mathbf{p} \in I$ to the data space location $P^{-1}(\mathbf{p})$, aiming to revert the effects of a given direct projection $P$. {\textbf{Decision maps:} These are images {F(\mathbf{p}) = f(P^{-1}(\mathbf{p}))} that depict, at every pixel $\mathbf{p}$, any property of interest $f$ that is measured in the data space $\mathbb{R}^n$ at location $P^{-1}(\mathbf{p})$. In their simplest form, which we extensively discussed in the previous chapters, decision maps depict a classification model. Besides depicting a classifier, maps can be used to visualize additional properties of $f$ or $P^{-1}$. For instance, the gradient map $G$ (Secs.~\ref{sec:inv_quality} and~\ref{sec:metrics}) depicts the smoothness of the inverse projection $P^{-1}$ computed at every pixel via Eqn.~\ref{eq:grad_map_ch4}.

classification model. Besides depicting a classifier, maps can be used to visualize additional properties of $f$ or $P^{-1}$. For instance, the gradient map $G$ (Secs.~\ref{sec:inv_quality} and~\ref{sec:metrics}) depicts the smoothness of the inverse projection $P^{-1}$ computed at every pixel via Eqn.~\ref{eq:grad_map_ch4}. Visualizing $G$ over $I$ shows areas where $P^{-1}$ has high gradients, \emph{i.e.}, where the extrapolation of the model $f$ from the samples in $D$ can be risky due to so-called compression of the high-dimensional space to the 2D projection space created by the direct projection $P$\,\citep{aupetit07,nonato18}. The distance to closest decision boundary $d_B$ (Sec.~\ref{sec:metrics}, Eqn.~\ref{eqn:b}) depicts at each pixel the distance to the closest decision boundary which allows one to find different areas in the data space where the trained model may be brittle\,\citep{rodrigues2018Imagebasedvisualization,differentiableDBM}. Computing $d_B$ is however quite expensive as it requires bisection-like search for the closest decision boundary\,\citep{rodrigues2018Imagebasedvisualization} or running adversarial example generation\,\citep{moosavi-dezfooli2016DeepFoolsimple}. The distance to the closest training sample $d_D$, also called distance to data (Sec.~\ref{sec:metrics}, Eqn.~\ref{eqn:n}) helps locating areas where we extrapolate the behavior of $f$ far away from the training data $D$, \emph{i.e.}, where the model's behavior can be less reliable, despite high confidence values\,\citep{differentiableDBM}. Any other characteristics of interest of $f$ can be visualized via such maps. We now call these \emph{classifier maps} as the mapped functions are in general real-valued ones (see Eqns.~\ref{eq:grad_map_ch4}-\ref{eqn:n}) as opposed to \emph{decision maps} which map class values only (see Eqn.~\ref{eqn:basic_dbm}).}

characteristics of interest of $f$ can be visualized via such maps. We now call these \emph{classifier maps} as the mapped functions are in general real-valued ones (see Eqns.~\ref{eq:grad_map_ch4}-\ref{eqn:n}) as opposed to \emph{decision maps} which map class values only (see Eqn.~\ref{eqn:basic_dbm}).} \textbf{Scalability:} All current decision map techniques are quite slow -- on a typical commodity PC, computing a DBM for resolutions of $250^2$ pixels reach about 10 seconds for all tested methods except DeepView; for the latter, the computation time can extend to several hours, depending on the choice of classifier (\Cref{ch4:metrics_decision_map}). As we shall see in Sec.~\ref{sec:results_ch7}, these costs increase quadratically with the DBM resolution -- and higher resolutions are needed to create maps in which users can see subtle details such as the exact shape of decision boundaries close to groups of data samples {or the variations due to compression in gradient maps\,\citep{aupetit07,nonato18} or distance to boundary/training samples\,\citep{differentiableDBM}}. This makes current DBM methods poorly suitable for visual analytics scenarios that require fast recomputation of decision and/or classifier maps upon re-training of the studied model.

{or the variations due to compression in gradient maps\,\citep{aupetit07,nonato18} or distance to boundary/training samples\,\citep{differentiableDBM}}. This makes current DBM methods poorly suitable for visual analytics scenarios that require fast recomputation of decision and/or classifier maps upon re-training of the studied model. For an image $I$ of $n \times n$ pixels, the complexity of current DBM methods is $O(n^2 K)$, where $K$ is the cost of a single $f(P^{-1}(\cdot))$ operation. Decreasing $K$ is hard if we allow any generic inverse projections $P^{-1}$ and classifier model $f$. Hence, to improve speed, we next aim to reduce the $n^2$ term. A classification model $f$, in general, must fit its decision boundaries so that they (a) surround same-class training points, but (b) the boundaries are sufficiently \emph{smooth} to allow for generalization without overfitting. Given (b), $f$, and thus a decision map that aims to accurately capture $f$, has in general relatively \emph{few} compact decision zones (not necessarily one zone per class). We use this property to devise our acceleration as follows. \caption{{Illustration for binary and confidence-based splitting heuristics. Given the block-set in (a), binary split creates the refined block-set in (b).

\emph{few} compact decision zones (not necessarily one zone per class). We use this property to devise our acceleration as follows. \caption{{Illustration for binary and confidence-based splitting heuristics. Given the block-set in (a), binary split creates the refined block-set in (b). For the block in (c), binary split would create four equal-sized blocks along the thin dashed lines. In contrast, confidence-based splitting (d) examines the confidence values and splits the block in up to 9 smaller blocks along the thin dashed lines.}}

the block in (c), binary split would create four equal-sized blocks along the thin dashed lines. In contrast, confidence-based splitting (d) examines the confidence values and splits the block in up to 9 smaller blocks along the thin dashed lines.}} We start by dividing the image $I$ into $B^2$ blocks -- each such block is a square of $\frac{n}{B} \times \frac{n}{B}$ pixels from $I$. For each block $b$, we evaluate the label $l_b = f(P^{-1}(\mathbf{p}))$ at its central pixel $\mathbf{p}$. Figure~\ref{fig:splitting}a shows this for a binary classifier (cyan and yellow are the two classes). Let $l_u, l_d, l_l, l_r$ be the labels computed similarly for the up, down, left, and right neighbor blocks of $b$. Let $N$ be the number of neighbors with labels different from $l_b$. If $N=0$, then $b$ is surrounded by same-label blocks, so, if we assume that a decision zone in the DBM is locally \emph{thicker} than $\frac{n}{B}$ pixels, no decision boundary crosses it. Hence, we can assign $l_b$ to all pixels in $b$. If $N>0$, we split $b$ into four equal smaller blocks (Fig.~\ref{fig:splitting}b shows the results of this splitting). We repeat the process, in a quadtree-like fashion, until we arrive at pixel-sized blocks or blocks do not need splitting anymore. During this, we note that (1) splitting larger blocks first helps to ensure a uniform refinement all over the image; and (2) splitting blocks having several neighbors with different labels is better than splitting blocks having a single such neighbor since the former cover more decision boundary fragments.

blocks first helps to ensure a uniform refinement all over the image; and (2) splitting blocks having several neighbors with different labels is better than splitting blocks having a single such neighbor since the former cover more decision boundary fragments. We model this by keeping blocks to split in a priority queue sorted decreasingly on $d \cdot d \cdot \frac{N}{C}$ where $d$ is the size of a block, $N$ is its number of different-label neighbors, and $C$ is its neighbor count (4 for blocks inside the DBM, 3 for blocks on the DBM boundary, and 2 for blocks on the DBM corners).

is the size of a block, $N$ is its number of different-label neighbors, and $C$ is its neighbor count (4 for blocks inside the DBM, 3 for blocks on the DBM boundary, and 2 for blocks on the DBM corners). As Sec.~\ref{sec:background_ch7} outlines, a DBM also often shows the \emph{confidence} of the visualized model $f$ at each map pixel. Per block, however, we have a single data sample $P^{-1}(\mathbf{p})$, computed at the block's center pixel $\mathbf{p}$. This is fine for class labels since these are \emph{constant} over decision zones, thus also per block as per our splitting heuristic. In contrast, confidence varies \emph{continuously} within a decision zone, hence can also vary within a block. We avoid computing additional confidence values apart from $c(\mathbf{p})$ by interpolating these values, computed at the blocks' centers $\mathbf{p}$, using nearest-neighbor, bilinear, and bicubic schemes.

contrast, confidence varies \emph{continuously} within a decision zone, hence can also vary within a block. We avoid computing additional confidence values apart from $c(\mathbf{p})$ by interpolating these values, computed at the blocks' centers $\mathbf{p}$, using nearest-neighbor, bilinear, and bicubic schemes. The binary split is a simple bisection procedure to find the places in $\mathbb{R}^2$ where decision boundaries are, up to the pixel precision of $I$. {We can potentially use the confidence values $c(\mathbf{p})$ to refine this process as follows.} Take the block $b$ shown in Fig.~\ref{fig:splitting}c. Binary split would divide $b$ along the dashed lines in the image. Consider the confidence values $c_A$ and $c_B$ for the inferred classes purple, respectively orange, sampled at the centers of cells $b_l$ and $b$, denoted next as $c_{A,l}$, $c_{B,l}$, $c_{A,b}$ and $c_{B,b}$ respectively (Fig.~\ref{fig:splitting}d). We next linearly interpolate these values to find the point where $c_{A} = c_{B}$ (red point, Fig.~\ref{fig:splitting}d). This is likely a good point to split cell $b$ (along the thick dashed line, Fig.~\ref{fig:splitting}d) {since, left to this point, class $A$ has a higher confidence than class $B$ (so the decision zone there should tell $A$) and, right to this point, class $B$ has a higher confidence than class $A$ (so the decision zone there should tell $B$). Note how this confidence maximization is precisely similar to how classification models internally decide on the class to output, albeit using more complex interpolation schemes than our linear one.} We proceed in the same way for all class values with respect to all four boundaries of cell $b$. This yields a possible set of 2, 3, 4, 6, or 9 cells that split $b$ as opposed to the fixed 4 cells done by binary split (see thin dashed red lines in Fig.~\ref{fig:splitting}d). Confidence is next interpolated as for the binary split method.

set of 2, 3, 4, 6, or 9 cells that split $b$ as opposed to the fixed 4 cells done by binary split (see thin dashed red lines in Fig.~\ref{fig:splitting}d). Confidence is next interpolated as for the binary split method. {Our final acceleration heuristic uses the underlying idea that, if we can capture the confidence $c$ at a \emph{coarse} sampling resolution, then we can find decision zones (and boundaries) at pixel resolution by maximizing $c$ over all inferred labels. This will reduce the costs implied by the block-splitting process.} For this, given our initial $B^2$ blocks, we compute confidences $c(\mathbf{p})$ at block centers $\mathbf{p}$, for \emph{all} inferred $|C|$ classes, and next interpolate these over $I$ using nearest-neighbor, bilinear, or bicubic techniques -- as described above, but now only over the initial blocks, which we do not further split. Next, for each pixel $\mathbf{p} \in I$, we compute which class yields the highest interpolated confidence and assign that class to $\mathbf{p}$. \caption{a) Ground-truth DBM with labels and confidence encoded into colors, respectively saturation, MNIST dataset. b-f) Class assignment errors for FastDBM method variants.} \section{Evaluation of acceleration heuristics} \subsection{Comparison of acceleration heuristics}\label{sec:comp_accel}

the highest interpolated confidence and assign that class to $\mathbf{p}$. \caption{a) Ground-truth DBM with labels and confidence encoded into colors, respectively saturation, MNIST dataset. b-f) Class assignment errors for FastDBM method variants.} \section{Evaluation of acceleration heuristics} \subsection{Comparison of acceleration heuristics}\label{sec:comp_accel} {We now compare our three acceleration heuristics (binary split, confidence split, confidence sampling) against each other and with the ground truth. For this, we use two metrics:} \textbf{Label errors:} We ideally want to get the same labels for a FastDBM image $I_{fast}$ and the ground-truth DBM image $I$. We evaluate this by the error {\epsilon_{label} = \frac{100}{n^2}\sum_{1 \leq x \leq n, 1 \leq y \leq n} \delta(I(x,y), I_{fast}(x,y)),} {where $\delta(a,b)$ is 0 if $a=b$ and 1 otherwise. That is, $\epsilon_{label}$ measures the percent of the $n \times n$ FastDBM map image which is different from the ground truth.} \textbf{Confidence errors:} Our interpolated confidence $c_{fast}$ should be as close as possible to the ground-truth one $c$. We evaluate this by the normalized MSE error \epsilon_{conf} = \frac{\sum_{1 \leq x \leq n, 1 \leq y \leq n} (c(x,y) - c_{fast}(x,y))^2}{\sum_{1 \leq x \leq n, 1 \leq y \leq n} c(x,y)^2}.

as possible to the ground-truth one $c$. We evaluate this by the normalized MSE error \epsilon_{conf} = \frac{\sum_{1 \leq x \leq n, 1 \leq y \leq n} (c(x,y) - c_{fast}(x,y))^2}{\sum_{1 \leq x \leq n, 1 \leq y \leq n} c(x,y)^2}. Figure~\ref{fig:errors} shows our results for the MNIST dataset\,\citep{lecun2010MNISThandwritten}, classified with a simple deep learning network $f$ (flatten layer, dense 10-unit layer and softmax activation, 20 training epochs, 3.5K training samples, 1.5K test samples); t-SNE and NNInv used for $P$ and $P^{-1}$; DBM image size $n=256$ pixels, $B=8$ blocks. Image (a) shows the ground-truth DBM with labels and confidence color- respectively saturation-coded. Images (b-d) show the results of our binary split, confidence split, and confidence sampling heuristics, the latter using nearest neighbors, bilinear, and bicubic interpolation. Red points show pixels where ground-truth labels differ from our results. Our heuristics yield practically the same decision maps, with only a few different pixels. The binary split method is best -- only 8 pixels of the $256^2$ are different; the confidence sampling method is the worst; for the latter, errors appear \emph{strictly} on the decision boundaries. This is likely since confidence varies slowly inside decision zones but rapidly close to boundaries (see Fig.~\ref{fig:errors}a), so our interpolation has difficulties in the latter areas.

method is the worst; for the latter, errors appear \emph{strictly} on the decision boundaries. This is likely since confidence varies slowly inside decision zones but rapidly close to boundaries (see Fig.~\ref{fig:errors}a), so our interpolation has difficulties in the latter areas. \caption{Label errors $\epsilon_{label}$ (a), confidence errors $\epsilon_{conf}$ (b), and computation time (c) for our three acceleration heuristics, MNIST dataset.} Figure~\ref{fig:mnist_errors} shows the errors $\epsilon_{label}$ and $\epsilon_{conf}$ {and computing time} for the above experiment for different image resolutions $n$ (100 to 2000 pixels squared). For confidence sampling, we only use bicubic interpolation as this yields lower errors than nearest neighbor and bilinear (see Fig.~\ref{fig:errors}). Error-wise, the binary split and confidence split methods are very similar and consistently lower than confidence sampling {since the latter method uses a single \emph{fixed} block resolution which, if too low, is unable to capture complex signal variations over the map image}. {Also, the binary split and confidence split errors are virtually constant with $n$, while confidence sampling errors show a slight increase with $n$.}

block resolution which, if too low, is unable to capture complex signal variations over the map image}. {Also, the binary split and confidence split errors are virtually constant with $n$, while confidence sampling errors show a slight increase with $n$.} Speed-wise, the binary and confidence-sampling methods show near-linear behavior in $n$ {(with a very small slope)} as opposed to the quadratic behavior of ground-truth DBM, with the confidence-split method in between the two. The binary and confidence-sampling methods are over one order of magnitude faster than ground-truth DBMs. {The confidence split method's relative low speed can be explained by the fact that it can create up to 9 cells when splitting a single block as opposed to exactly four for the binary split (see Fig.~\ref{fig:splitting} and related text).} Note also that our maximal resolution $n=2000$ exceeds \emph{by far} all reported DBM results in the literature. {From the above, we conclude that the binary split method is the clear winner when considering computational speed and accuracy factors. As such, we focus only on this method in our further evaluations.} \subsection{Parameter setting for binary split heuristic}

literature. {From the above, we conclude that the binary split method is the clear winner when considering computational speed and accuracy factors. As such, we focus only on this method in our further evaluations.} \subsection{Parameter setting for binary split heuristic} \caption{{Speed (left) and label errors (right) of binary split method as function of initial block count $B$ for decision maps constructed by various $(P, P^{-1})$ methods.}} Binary split has one parameter -- the initial block count $B$ -- so how to set its value? A high $B$ will limit errors due to dense sampling of the image, but will be slow, since $f(P^{-1}$) must be evaluated on many blocks. A low $B$ will be fast, but as Sec.~\ref{sec:method_ch7} notes, decision map details under $\frac{n}{B}$ may be lost.

will limit errors due to dense sampling of the image, but will be slow, since $f(P^{-1}$) must be evaluated on many blocks. A low $B$ will be fast, but as Sec.~\ref{sec:method_ch7} notes, decision map details under $\frac{n}{B}$ may be lost. To find a good initial value for $B$, we measure both speed and label errors $\epsilon_{label}$ for various $B$ settings ranging from 8 to 96. To generalize our findings, we test several combinations of $P$ and $P^{-1}$ to compute our ground-truth decision maps, specifically autoencoders (AE, used for both $P$ and $P^{-1}$); SSNP (used for both $P$ and $P^{-1}$); and DBM (PCA, UMAP, and t-SNE used for $P$, NNInv used for $P^{-1}$). Figure~\ref{fig:hyperparams} shows the speed and label errors as function of $B$ for our maximally considered resolution $n=2000$. We see that, label-error-wise, all $B$ values above roughly 32 yield (very) low errors. Speed-wise, $B$ values in the interval 32-64 offer best results, which confirms our earlier observations that too low or too high $B$ will be slow. Also, we see that the overall speed trend as function of $B$ does not strongly depend on the choice of $(P, P^{-1})$, up to a constant bias factor. Hence, we conclude that a block size $B = 32$ is a good preset for FastDBM.

the overall speed trend as function of $B$ does not strongly depend on the choice of $(P, P^{-1})$, up to a constant bias factor. Hence, we conclude that a block size $B = 32$ is a good preset for FastDBM. Our FastDBM method is implemented in Python and runs fully on the CPU. The full source code, including datasets and experiments presented here, is publicly available\,\citep{softwareGfastDBM}. \section{In-depth evaluation of binary split acceleration} We found that binary split works the best among the three proposed heuristics (Sec.~\ref{sec:comp_accel}). We now further evaluate the binary split heuristic using more classifiers, an additional quality metric, and using decision maps constructed with all inverse projection techniques that we are aware of. \caption{Comparison between ground-truth DBM and our binary split method for three datasets, six classifiers, {t-SNE and UMAP projections}.}

using more classifiers, an additional quality metric, and using decision maps constructed with all inverse projection techniques that we are aware of. \caption{Comparison between ground-truth DBM and our binary split method for three datasets, six classifiers, {t-SNE and UMAP projections}.} We evaluate the binary split method with additional combinations of datasets, classifiers, and $(P, P^{-1})$ combinations used to compute the ground-truth DBM. The datasets include FashionMNIST\,\citep{xiao2017FashionMNISTnovel}, HAR\,\citep{anguita2012human}, and Iris\,\citep{fisher1988IrisPlants}. Classifiers included logistic regression (LR), support vector machines (SVM), k-nearest neighbors (kNN), decision trees (DT), random forests (RF), and the neural network (NN) we used earlier for MNIST. It is important to note that the accuracy of the trained classifiers is of no concern in this experiment. If FastDBM approximates well the ground-truth DBM, FastDBM can be next used next to assess how well (or poorly) the classifiers behave.

is important to note that the accuracy of the trained classifiers is of no concern in this experiment. If FastDBM approximates well the ground-truth DBM, FastDBM can be next used next to assess how well (or poorly) the classifiers behave. Ground-truth DBMs were created by the {DBM (t-SNE, NNInv)} and {DBM (UMAP, NNInv)} combinations at resolution $n=400$ pixels squared. Figure~\ref{fig:visual_results} shows the ground-truth DBMs; those created by our binary split method; 2D projections of training samples in green and the label difference encoded by red dots as in Fig.~\ref{fig:errors}, for the MNIST, FashionMNIST, and HAR datasets. We see that our method yields visually almost identical label results as the ground-truth -- there are only few red points in the `difference' images. This occurs consistently for quite different DBMs, \emph{e.g.}, the smooth decision-zone DBMs created for LR, NN, SVM, and KNN, but also the far noisier DBM created for DT, and the overall low-confidence DBM created for RF. Additional results for all other tested combinations, present in the supplementary material, confirm this observation.

DBMs created for LR, NN, SVM, and KNN, but also the far noisier DBM created for DT, and the overall low-confidence DBM created for RF. Additional results for all other tested combinations, present in the supplementary material, confirm this observation. To further confirm the visual similarity between the ground-truth decision maps and the FastDBM versions shown in Fig.~\ref{fig:visual_results}, we next compare the \emph{map consistency} metric $Cons_p$ (Eqn.~\ref{eq:cons_p}) computed for both cases. To recap, $Cons_p$ computes the fraction of `consistent' pixels in a decision map image, \emph{i.e.}, pixels whose corresponding data points (obtained by $P^{-1}$) have the same class label after a round-trip of projection and inverse projection (see \Cref{ch4:metrics_decision_map}).

both cases. To recap, $Cons_p$ computes the fraction of `consistent' pixels in a decision map image, \emph{i.e.}, pixels whose corresponding data points (obtained by $P^{-1}$) have the same class label after a round-trip of projection and inverse projection (see \Cref{ch4:metrics_decision_map}). If our acceleration technique works well, then the consistency $Cons_p^{fast}$ of the images it produces should be very close to the consistency $Cons_p$ of the ground-truth decision map images. Table~\ref{tab:cons_p_dbm} shows, for several datasets and classifiers, the values of $Cons_p^{fast}$ computed by binary split for DBM (UMAP+NNInv) and SDBM compared to the ground-truth $Cons_p$. We see that, although both $Cons_p^{fast}$ and $Cons_p$ are less than the ideal value of one (which would imply that $P^{-1}$ is an exact inverse of $P$), their values are very close to each other. That is, the quality of the images produced by FastDBM is very close to the ground-truth images. \caption{{$Cons_p$ of FastDBM \emph{vs} two ground truth methods for three datasets and six classifiers. $\Delta$ $Cons_p$ = $Cons_p^{fast}$ - $Cons_p$. See Sec.~\ref{sec:results_ch7}.}} \textbf{Model} & \textbf{Metric} & \textbf{Fashion} & \textbf{HAR} & \textbf{MNIST} \\ &    & \textbf{MNIST} &  &  \\ DT       & $Cons_p$          & 0.4033  & 0.3704  & 0.4718  \\

for three datasets and six classifiers. $\Delta$ $Cons_p$ = $Cons_p^{fast}$ - $Cons_p$. See Sec.~\ref{sec:results_ch7}.}} \textbf{Model} & \textbf{Metric} & \textbf{Fashion} & \textbf{HAR} & \textbf{MNIST} \\ & & \textbf{MNIST} & & \\ DT & $Cons_p$ & 0.4033 & 0.3704 & 0.4718 \\ & $Cons_p^{fast}$ & 0.4041 & 0.3659 & 0.4651 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$     & 0.0009  & -0.0045 & -0.0067 \\ KNN                 & $Cons_p$          & 0.2152  & 0.0816  & 0.1414  \\ & $Cons_p^{fast}$     & 0.2159  & 0.0735  & 0.1364  \\ \rowcolor{lightgray}  & $\Delta$ $Cons_p$     & 0.0007  & -0.0081 & -0.0049 \\ LR & $Cons_p$          & 0.2787  & 0.1776  & 0.2759  \\ & $Cons_p^{fast}$     & 0.2792  & 0.1745  & 0.2727  \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$     & 0.0005  & -0.0031 & -0.0032 \\ NN      & $Cons_p$          & 0.2959  & 0.1740  & 0.2810  \\ & $Cons_p^{fast}$    & 0.2969  & 0.1712  & 0.2785  \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$     & 0.0010  & -0.0028 & -0.0025 \\ RF      & $Cons_p$          & 0.2480  & 0.2477  & 0.3009  \\ & $Cons_p^{fast}$    & 0.2464  & 0.2400  & 0.2955  \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$     & -0.0016 & -0.0077 & -0.0053 \\ SVM                 & $Cons_p$          & 0.2153  & 0.1591  & 0.2470  \\ & $Cons_p^{fast}$     & 0.2149  & 0.1551  & 0.2458  \\

\\ & $Cons_p^{fast}$ & 0.2464 & 0.2400 & 0.2955 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ & -0.0016 & -0.0077 & -0.0053 \\ SVM & $Cons_p$ & 0.2153 & 0.1591 & 0.2470 \\ & $Cons_p^{fast}$ & 0.2149 & 0.1551 & 0.2458 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ & -0.0004 & -0.0039 & -0.0013 \\ } \textbf{Model} & \textbf{Metric} & \textbf{Fashion} & \textbf{HAR} & \textbf{MNIST} \\ &    & \textbf{MNIST} &  &  \\ DT       & $Cons_p$          & 0.2685 & 0.1515 & 0.3594 \\ & $Cons_p^{fast}$     & 0.2678 & 0.1510 & 0.3616 \\ \rowcolor{lightgray}  & $\Delta$ $Cons_p$ & -0.0007 & -0.0005 &  0.0022 \\ KNN                 & $Cons_p$          & 0.1145 & 0.0778 & 0.0950 \\ & $Cons_p^{fast}$     & 0.1149 & 0.0767 & 0.0956 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ &  0.0004 & -0.0011 &  0.0007 \\ LR & $Cons_p$          & 0.0589 & 0.0432 & 0.0909 \\ & $Cons_p^{fast}$  & 0.0591 & 0.0431 & 0.0910 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ &  0.0003 & -0.0001 &  0.0002 \\ NN      & $Cons_p$          & 0.0725 & 0.0320 & 0.0872 \\ & $Cons_p^{fast}$   & 0.0726 & 0.0321 & 0.0881 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ &  0.0001 &  0.0001 &  0.0009 \\ RF      & $Cons_p$          & 0.1455 & 0.0655 & 0.2174 \\

NN & $Cons_p$ & 0.0725 & 0.0320 & 0.0872 \\ & $Cons_p^{fast}$ & 0.0726 & 0.0321 & 0.0881 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ & 0.0001 & 0.0001 & 0.0009 \\ RF & $Cons_p$ & 0.1455 & 0.0655 & 0.2174 \\ & $Cons_p^{fast}$ & 0.1456 & 0.0658 & 0.2178 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ &  0.0001 &  0.0002 &  0.0004 \\ SVM                 & $Cons_p$          & 0.0597 & 0.0364 & 0.0893 \\ & $Cons_p^{fast}$   & 0.0603 & 0.0364 & 0.0898 \\ \rowcolor{lightgray} & $\Delta$ $Cons_p$ &  0.0006 & -0.0000 &  0.0006 \\ } \caption{{Maps computed using the iLAMP and RBF inverse projections in combination with the t-SNE and UMAP direct projections for the MNIST dataset. Resolution: $256^2$. See Sec.~\ref{sec:ilamp_rbf}.}} \subsection{Accelerating additional direct and inverse projection techniques for creating decision maps}

0.0006 \\ } \caption{{Maps computed using the iLAMP and RBF inverse projections in combination with the t-SNE and UMAP direct projections for the MNIST dataset. Resolution: $256^2$. See Sec.~\ref{sec:ilamp_rbf}.}} \subsection{Accelerating additional direct and inverse projection techniques for creating decision maps} So far, we computed our (accelerated) decision maps using NNInv and SSNP for the inverse projection $P^{-1}$ since, as mentioned in Sec.~\ref{sec:background_ch7}, earlier work showed that NNInv and SSNP are fast and accurate for this task. Yet, other inverse projection techniques do exist, most notably iLAMP\,\citep{Amorim2012ilamp} and the inverse projection using radial basis functions (RBF)\,\citep{amorim2015Facinghighdimensions}. Earlier work has shown that both these techniques are slower than NNInv\,\citep{espadoto2019NNinv}. However, it is interesting to see how these techniques fare given our acceleration. Separately, iLAMP and RBF have a quite different behavior from the already-tested NNInv and SSNP. Hence, if our acceleration technique can create \emph{accurate} approximations of decision maps using these inverse projections, this increases the claims of generality of our proposal.

Separately, iLAMP and RBF have a quite different behavior from the already-tested NNInv and SSNP. Hence, if our acceleration technique can create \emph{accurate} approximations of decision maps using these inverse projections, this increases the claims of generality of our proposal. Figure~\ref{fig:tsne_ilamp_mnist} shows, for the MNIST dataset, the decision maps computed by four ground-truth technique pairs (t-SNE and iLAMP, UMAP and iLAMP, t-SNE and RBF, and UMAP and RBF) and their counterparts produced by our binary split acceleration. The ground-truth maps are noisier than those we computed so far using NNInv and SSNP for $P^{-1}$, in line with earlier findings\,\citep{espadoto2019NNinv,rodrigues2018Imagebasedvisualization}. Our binary split method captures these ground truth images quite well -- the label difference images show only a few pixels where our results differ from the ground truth, much like in Fig.~\ref{fig:visual_results}. Our method speeds up the computation of most maps -- see timing figures in the lower-left corners of the images. Speed up is overall much smaller except for the t-SNE and iLAMP combination. This is due to the high irregularity of the decision boundaries in this case, which generates a very large amount of cell splits -- see the corresponding `binary split process' images in Fig.~\ref{fig:tsne_ilamp_mnist}.

except for the t-SNE and iLAMP combination. This is due to the high irregularity of the decision boundaries in this case, which generates a very large amount of cell splits -- see the corresponding `binary split process' images in Fig.~\ref{fig:tsne_ilamp_mnist}. Concluding, we claim that our binary split heuristic creates accurate decision maps for all existing inverse projections we are aware of; and, for most cases except very noisy decision maps (which are likely not useful in practice), it also accelerates the map computation by several factors. \section{Accelerating the computation of continuous maps} So far, our binary split method only works for maps with \emph{label} values like classification functions $f : \mathbb{R}^n \rightarrow C$. However, several maps used in classifier visualization have continuous values, \emph{i.e.}, are of the form $f : \mathbb{R}^n \rightarrow \mathbb{R}$. Examples are the gradient maps $G$ (Eqn.~\ref{eq:grad_map_ch4}), distance-to-closest-training sample $d_D$ (Eqn.~\ref{eqn:n}), and distance-to-decision boundary $d_B$ (Eqn.~\ref{eqn:b}). In general, one cannot reduce such continuous maps to the computation and comparison of purely categorical (label) values. Yet, we would like to accelerate their computation.

the gradient maps $G$ (Eqn.~\ref{eq:grad_map_ch4}), distance-to-closest-training sample $d_D$ (Eqn.~\ref{eqn:n}), and distance-to-decision boundary $d_B$ (Eqn.~\ref{eqn:b}). In general, one cannot reduce such continuous maps to the computation and comparison of purely categorical (label) values. Yet, we would like to accelerate their computation. To do this, we generalize the binary split idea by replacing the label comparison (see Sec.~\ref{sec:method_ch7}) with a threshold comparison. For a dataset $D$, we compute this threshold globally as T = \alpha \cdot \tau \cdot \left( \max_{\mathbf{p} \in \mathcal B} f (P^{-1}(\mathbf{p})) - \min_{\mathbf{p} \in \mathcal B} f ( P^{-1} (\mathbf{p})) \right).

comparison (see Sec.~\ref{sec:method_ch7}) with a threshold comparison. For a dataset $D$, we compute this threshold globally as T = \alpha \cdot \tau \cdot \left( \max_{\mathbf{p} \in \mathcal B} f (P^{-1}(\mathbf{p})) - \min_{\mathbf{p} \in \mathcal B} f ( P^{-1} (\mathbf{p})) \right). Simply put, $T$ is a fraction of the range of the function $f$ over the map. $\mathcal B$ denotes the set of center pixels of the initial $B^2$ blocks. $\tau = e ^ {-\frac{B \cdot d}{n}}$ is a decreasing function of the block size $d$, \emph{i.e.}, smaller blocks will use a higher threshold. The intuition behind this is that smaller blocks already capture $f$ at a higher resolution so we make them harder to further split to reduce over-refinement. Conversely, if $f$ exhibits even a small variation over large blocks, this is a reason to split these to capture further details. The last parameter $\alpha$ is a scaling factor. When the difference between the maximum and minimum values of the four neighbors of a block including the block itself exceeds $T$, we split the block.

split these to capture further details. The last parameter $\alpha$ is a scaling factor. When the difference between the maximum and minimum values of the four neighbors of a block including the block itself exceeds $T$, we split the block. Distance-to-boundary maps which compute $d_B$ (Eqn.~\ref{eqn:b}) are a first example of such continuous maps. Figure~\ref{fig:distance2boundary_all} shows the results of accelerating the computation of $d_B$ at resolution $512^2$ pixels for the three datasets as in Fig.~\ref{fig:visual_results},

$T$, we split the block. Distance-to-boundary maps which compute $d_B$ (Eqn.~\ref{eqn:b}) are a first example of such continuous maps. Figure~\ref{fig:distance2boundary_all} shows the results of accelerating the computation of $d_B$ at resolution $512^2$ pixels for the three datasets as in Fig.~\ref{fig:visual_results}, and for all four inverse projection techniques we are aware of (NNInv, SSNP, RBF, and iLAMP) with UMAP as the direct projection. For all inverse projection techniques, we show the ground-truth $d_B$ map, the map computed by our binary split acceleration, and the blocks created by the binary split process. Ground-truth maps are visually almost identical from those computed by our binary split heuristic. Speed-wise, our binary split heuristic is up to roughly ten times faster than computing the ground truth, see the figures in the bottom-left corners in the respective images in Fig.~\ref{fig:distance2boundary_all}. This speed-up is in line with what we visually see as amounts of cells being split in the rightmost columns in Fig.~\ref{fig:distance2boundary_all} -- the largest cells in those columns indicate the original block sizes, that is, using $B=32$ initial cells for the acceleration heuristic, as explained earlier in Sec.~\ref{sec:results_ch7}.

see as amounts of cells being split in the rightmost columns in Fig.~\ref{fig:distance2boundary_all} -- the largest cells in those columns indicate the original block sizes, that is, using $B=32$ initial cells for the acceleration heuristic, as explained earlier in Sec.~\ref{sec:results_ch7}. Gradient maps $G$ (Eqn.~\ref{eq:grad_map_ch4}) are a second example of continuous maps we can accelerate. Figure~\ref{fig:grad_map_all_fmnist} shows gradient maps computed by ground truth and our generalized binary split acceleration for the same dataset-projection-inverse projection combinations as shown in Fig.~\ref{fig:distance2boundary_all}. Our accelerated maps are very similar to the ground truth, while the computation time is up to 10 times lower -- see timing figures in the lower-left corners of the images.

the same dataset-projection-inverse projection combinations as shown in Fig.~\ref{fig:distance2boundary_all}. Our accelerated maps are very similar to the ground truth, while the computation time is up to 10 times lower -- see timing figures in the lower-left corners of the images. Figure~\ref{fig:distance2sample_all_fmnist} shows a final example of continuous maps, namely distance-to-closest sample maps $d_D$ (Eqn.~\ref{eqn:n}). As for $d_B$ and $G$, our acceleration yields practically the same images with high speed-ups \emph{vs} ground truth. Given these results and the fact that our binary split works entirely agnostically on the nature of the function $f$, we claim that similar results can be obtained for \emph{any} function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ that produces a real value from an inversely-projected 2D pixel. The only implicit assumption our acceleration method makes for $f$ is that it should be locally smooth, \emph{i.e.}, not have unbounded variations on a small spatial extent, so that we can use the threshold computed by Eqn.~\ref{eqn:threshold} to locate map areas needing subdivision. \caption{{Distance-to-decision-boundary maps $d_B$ for the generalized binary split method, three datasets (resolution: $512^2$). See Sec.~\ref{sec:general_fast_map}.}} \caption{{Gradient maps $G$ for the generalized binary split, three datasets (resolution: $512^2$). See Sec.~\ref{sec:general_fast_map}.}}

the threshold computed by Eqn.~\ref{eqn:threshold} to locate map areas needing subdivision. \caption{{Distance-to-decision-boundary maps $d_B$ for the generalized binary split method, three datasets (resolution: $512^2$). See Sec.~\ref{sec:general_fast_map}.}} \caption{{Gradient maps $G$ for the generalized binary split, three datasets (resolution: $512^2$). See Sec.~\ref{sec:general_fast_map}.}} \caption{{Distance to nearest sample maps $d_D$ for the generalized binary split method, three datasets (resolution: $512^2$). See Sec.~\ref{sec:general_fast_map}.}} To find a suitable choice for $\alpha$, we executed a grid search over the range $[0, 0.6]$ by evaluating both computation time and MSE error of our resulting map \emph{vs} the ground-truth maps $G$, $d_D$, and $d_B$ for the MNIST dataset. The MSE error is computed analogously to $\epsilon_{conf}$ (Eqn.~\ref{eqn:eps_conf}). Figure~\ref{fig:search_threshold} shows the search results. For larger $\alpha$ values, we get higher errors since the split threshold $T$ is larger, so fewer block refinements (splits) occur; for smaller $\alpha$ values, the error decreases but the computational time increases, since there are more splits. We found that $\alpha \in [0.1, 0.2]$ is a good choice balancing between speed and accuracy. Specifically, we set $\alpha = 0.125$ for $G$, $\alpha = 0.1$ for $d_D$, and $\alpha=0.15$ for $d_B$ consistently in all our following experiments.

splits. We found that $\alpha \in [0.1, 0.2]$ is a good choice balancing between speed and accuracy. Specifically, we set $\alpha = 0.125$ for $G$, $\alpha = 0.1$ for $d_D$, and $\alpha=0.15$ for $d_B$ consistently in all our following experiments. {Figure~\ref{fig:binary_split_general_vary_grid} shows the computation times and normalized MSE errors of our generalized binary split method for different image sizes and for all the three maps $d_D$, $G$, and $d_B$. The results are quite similar with the binary split used for label-based maps (Fig.~\ref{fig:mnist_errors}c): Our method is roughly linear in the map resolution (as compared to quadratic in resolution for the brute-force ground truth computation), while errors decrease inversely quadratically with resolution. All in all, the above results show us that the generalized binary split is a computationally effective and accurate way to accelerate the construction of continuous maps.} \caption{{Generalized binary split method: Search for the best threshold $\alpha$ for the MNIST dataset. Columns show different classifier maps ($d_D$, $G$, $d_B$). Top row shows computation time. Bottom row shows computation error.}}

way to accelerate the construction of continuous maps.} \caption{{Generalized binary split method: Search for the best threshold $\alpha$ for the MNIST dataset. Columns show different classifier maps ($d_D$, $G$, $d_B$). Top row shows computation time. Bottom row shows computation error.}} \caption{{Performance of the generalized binary split method with varying grid resolutions for NNInv and SSNP on the MNIST dataset. Columns show different classifier maps ($d_D$, $G$, $d_B$) with optimized thresholds $\alpha$. Top row shows computation time for binary split and ground truth methods. Bottom row shows the error as the image resolution increases.}} { We next discuss several aspects of our method. \textbf{Genericity:} Our acceleration method based on the binary split can accommodate the construction of classifier maps for \emph{any} function $f : \mathbb{R}^n \rightarrow \mathbb{R}$. This covers, but is not restricted to, the actual classification label $F$, gradient maps $G$, distance-to-boundary maps $d_B$, and distance-to-closest-training sample maps $d_D$. We can accelerate the computation of all such functions, while preserving their accuracy, in a black-box manner, \emph{i.e.}, without knowing anything additional about what the respective functions capture or how they are computed. The only constraint we have is that such functions are smooth.

computation of all such functions, while preserving their accuracy, in a black-box manner, \emph{i.e.}, without knowing anything additional about what the respective functions capture or how they are computed. The only constraint we have is that such functions are smooth. \textbf{Performance:} Our experiments showed a consistent performance gain of our binary split heuristic of roughly 5 times with respect to the brute-force computation of the decision maps at each pixel. This figure varies mainly as a function of the \emph{smoothness} of the inverse projection method $P^{-1}$ being used: NNInv, SSNP, and RBF are relatively speaking smooth mappings so they require less block splits to capture their variation, thus, yield higher speed-ups. In contrast, iLAMP is far less smooth so it requires many more block splits, thereby reducing our speed-ups to roughly 1 to 2 times. Practically, for the inverse projection methods NNInv and SSNP, which were earlier found to be the most reliable for computing decision maps, our acceleration yields roughly linear times \emph{vs} the map resolution as compared to quadratic time for the brute-force computation.

for the inverse projection methods NNInv and SSNP, which were earlier found to be the most reliable for computing decision maps, our acceleration yields roughly linear times \emph{vs} the map resolution as compared to quadratic time for the brute-force computation. \textbf{Quality:} All our experiments showed that we can obtain the maps virtually identical visually to the ground-truth ones no matter which type of function we visualize. Moreover, the quality, measured in terms of normalized MSE \emph{vs} ground truth, only increases with the image resolution. This means that our accelerated maps can be safely substituted for the brute-force-computed ones for all practical reasons. \textbf{Ease of use:} Our acceleration method is essentially parameter-free -- the only two parameters $B$ (initial block count, see Sec.~\ref{sec:binary_split}) and $\alpha$ (controlling the split threshold for continuous mappings, see Eqn.~\ref{eqn:threshold}) have well-tested presets which are independent on the classification model, choice of direct and inverse projections $P$ and $P^{-1}$, and type of map being computed.

block count, see Sec.~\ref{sec:binary_split}) and $\alpha$ (controlling the split threshold for continuous mappings, see Eqn.~\ref{eqn:threshold}) have well-tested presets which are independent on the classification model, choice of direct and inverse projections $P$ and $P^{-1}$, and type of map being computed. \textbf{Limitations:} The key assumption behind our acceleration is that the \emph{combination} of function $f$ we aim to visualize with the inverse projection function $P^{-1}$ is smooth and has bounded variation over $\mathbb{R}^2$. While this is true of all $f$ and $P^{-1}$ we know of, and is also in line with the well-known smoothness assumption underlying most machine learning methods for $f$, that cases could exist where smoothness would not hold. In such cases, it is possible that our acceleration does not yield worthwhile speed-ups and/or the accelerated maps have visible errors as compared to the ground truth ones. Separately, we believe that the confidence split method (Sec.~\ref{sec:conf_split}) has not yet reached its true potential. Better interpolation schemes than our current linear one should be able to decrease the number of generated cells and thereby achieve higher performance at the same quality level as compared to the so far currently best-ranked binary split.}

true potential. Better interpolation schemes than our current linear one should be able to decrease the number of generated cells and thereby achieve higher performance at the same quality level as compared to the so far currently best-ranked binary split.} Another open challenge lies in scaling decision map visualizations to a large number of classes. In that case, encoding class values in categorical colors will not work well. This limitation is broadly shared by many visualizations that use categorical color maps to encode class values. Potential solutions can group class values hierarchically to reduce the needed color count and offer detail-on-demand interactively. Note that this scalability problem only affects decision maps (which depict class value) and not the classifier maps (which depict real-valued quantities). {We have presented FastDBM, a technique for accelerating the computation of maps that describe the working of general-purpose classification models. Our technique is agnostic of the exact type of maps being computed as shown by its application to create maps of classification label, classification confidence, distance-to-classification-boundary, distance-to-closest-training sample, and gradient maps.

maps that describe the working of general-purpose classification models. Our technique is agnostic of the exact type of maps being computed as shown by its application to create maps of classification label, classification confidence, distance-to-classification-boundary, distance-to-closest-training sample, and gradient maps. We show that our technique can be applied to not only to decision maps but also real-valued maps; and also show high speed-ups and accuracy for more combinations of direct and inverse projection methods used to compute the maps. Practically, we show that our method can compute classifier maps that are visually almost identical to ground-truth ones with a speed-up of one order of magnitude on average. This allows the further deployment of such visualizations in interactive visual analytics workflows for classifier engineering. Our method depends on just two free parameters for which we provide good preset values. Our method can accelerate any current classifier map computation technique, and can be applied to any trained classifier model, as it only requires access to the inverse projection function this technique uses, respectively to the black-box execution of the trained model. }

can accelerate any current classifier map computation technique, and can be applied to any trained classifier model, as it only requires access to the inverse projection function this technique uses, respectively to the black-box execution of the trained model. } {Future work aims to explore our acceleration technique to compute additional classifier maps. Also, we consider speeding up our method by more advanced sampling and interpolation schemes, GPU execution, and evaluating it on novel direct and inverse projection methods which arrive in the infovis arena. In parallel, measuring the added value of computing near-real-time classifier maps for classifier engineering, \emph{e.g.}, in the context of visual active learning, is a key goal we aim at.} We close this thesis by revisiting our contributions and discussing future research directions. In \Cref{ch1:intro}, we introduced our key research question:\\ \emph{How can we improve decision map methods to understand and control which parts of the data space they sample, leading to desirable quality values, with high computational performance?}\\ This question was next refined into four more specific research questions that address \textbf{quality}, \textbf{coverage}, \textbf{control} and \textbf{interactivity} of decision maps, respectively:

control which parts of the data space they sample, leading to desirable quality values, with high computational performance?}\\ This question was next refined into four more specific research questions that address \textbf{quality}, \textbf{coverage}, \textbf{control} and \textbf{interactivity} of decision maps, respectively: \item [\textbf{RQ1:}] How can we define such quality metrics and how do current decision map methods fare with respect to each other from the perspective of such metrics? \item [\textbf{RQ2:}]  How do decision map methods differ in their sampling of the data space? Does this sampling depend on the data dimensionality or depicted classification model? \item [\textbf{RQ3:}] How can we enable users to control the parts of the data space depicted by a decision map in simple, interactive, ways? \item [\textbf{RQ4:}] Can we significantly improve this computation time for creating decision maps for any classification model and for any type and dimensionality of datasets?

the parts of the data space depicted by a decision map in simple, interactive, ways? \item [\textbf{RQ4:}] Can we significantly improve this computation time for creating decision maps for any classification model and for any type and dimensionality of datasets? Next, in \Cref{ch2:related}, we presented related work related to dimensionality reduction (direct and inverse projections) and decision maps. We highlighted how ML methods, in particular deep learning, can assist (inverse) projection and decision map computation; and also how projections and decision maps are key instruments for the exploration of ML models and related datasets. Apart from elaborating on the natural synergy between DL and MR in \Cref{ch2:related}, our main thesis contributions come in the next chapters, as follows. \section{Decision maps in practice} In \Cref{ch3:geo_application}, we presented an application of decision maps for understanding a mineral genesis classification problem in practical setting. Specifically, we created a decision map for a classifier trained on pyrite trace elements data to classify its formation environment. We demonstrated two use cases of the decision map: (1) providing extra evidence for a mineral deposit whose genetic environment is in debate; (2) visually identifying the most important trace elements for the classification.

data to classify its formation environment. We demonstrated two use cases of the decision map: (1) providing extra evidence for a mineral deposit whose genetic environment is in debate; (2) visually identifying the most important trace elements for the classification. Our work contributes to both the information visualization community and the mineralogy community by showing how decision maps, extended by additional features, can be efficient and effective instruments to interpret classification models and arrive at a better understanding of the involved data dimensions. While this work showed the added value of decision maps in research where the target users are not ML experts, it also highlighted the need for a comprehensive evaluation of existing decision maps, and the need for several enhancements of the by-then existing decision map techniques so as to make them more flexible and more informative. At a high level, our work here did not directly answer \textbf{RQ1} -- \textbf{RQ4} but demonstrated that, since decision maps are useful instruments in practice, answering these questions is next important to further increase the efficiency and effectiveness of decision maps. \section{Quality of decision maps}

work here did not directly answer \textbf{RQ1} -- \textbf{RQ4} but demonstrated that, since decision maps are useful instruments in practice, answering these questions is next important to further increase the efficiency and effectiveness of decision maps. \section{Quality of decision maps} In \Cref{ch4:metrics_decision_map}, we conducted a series of comprehensive assessments for the three state-of-the-art decision maps methods, namely DBM, SDBM, and DeepView, combined with several common classifiers, on a range of both synthetic and real-world datasets, which addresses \textbf{RQ1 (quality of decision maps)}. In this work, we proposed six global metrics and four local metrics to measure the global and local quality of decision maps, respectively. Our results showed that none of the evaluated decision map techniques consistently outperforms the others in all measured aspects. Separately, our analysis exposed several previously unknown properties and limitations of decision map techniques. In particular we saw that all the studied decision map methods have inherent limitations in various quality aspects; and that these limitations can fluctuate significantly depending on the dataset and/or classifier being explored. To support practitioners, we also propose a workflow for selecting the most appropriate decision map technique for given datasets, classifiers, and requirements of the application at hand.

these limitations can fluctuate significantly depending on the dataset and/or classifier being explored. To support practitioners, we also propose a workflow for selecting the most appropriate decision map technique for given datasets, classifiers, and requirements of the application at hand. \section{Coverage of decision maps} One of our most surprising discoveries in \Cref{ch4:metrics_decision_map} is that all the three examined decision map methods produce surface-like structures on a synthetic 3D dataset. At the time of the respective work, this indicated a potential limitation in the \emph{coverage} of the data space by inverse projections and decision maps. In \Cref{ch5:surface_like_behavior}, we elaborated to elucidate this potential limitation. For this, we further investigated the coverage of inverse projections and decision maps in more scenarios, \emph{i.e.}, additional datasets of different dimensionalities,  more classifiers for creating decision maps, and more direct and inverse projection method combinations to create the maps, thereby addressing \textbf{RQ2 (coverage of decision maps)}. We studied the said methods by using both visual assessment and quantitative assessment based on the intrinsic dimensionality of the backprojected surfaces.

more direct and inverse projection method combinations to create the maps, thereby addressing \textbf{RQ2 (coverage of decision maps)}. We studied the said methods by using both visual assessment and quantitative assessment based on the intrinsic dimensionality of the backprojected surfaces. Our results demonstrate that, despite differences in the behavior of various inverse projection methods, all the studied techniques essentially capture only two-dimensional structures embedded within the data space, regardless of the dimensionality of the data space or the classifier used. This work -- which we consider to be the first key result of this thesis -- highlights the fundamental limitations of all studied decision map techniques, particularly in terms of the extent to which they represent a classifier's behavior and the specific regions and ways in which they capture this behavior. Understanding these limitations is crucial both for selecting an appropriate technique to construct decision maps and for accurately interpreting the resulting maps in practice. Additionally, we also found the dimensionality of the projection space $q$ has an impact on the intrinsic dimensionality of inverse projections.

both for selecting an appropriate technique to construct decision maps and for accurately interpreting the resulting maps in practice. Additionally, we also found the dimensionality of the projection space $q$ has an impact on the intrinsic dimensionality of inverse projections. In \Cref{ch6:controllable_Pinv}, we proposed \ourname{}, to our knowledge the first controllable inverse projection method -- which we consider to be the second key result of this thesis. Our method overcomes the surface dimensionality of backprojections studied in \Cref{ch5:surface_like_behavior} and thereby addresses \textbf{RQ3 (control of decision maps)}. To achieve this control, we first found a latent space which is disentangled from the 2D projection space, and then designed a control mechanism to manipulate this latent space. The inverse projection, computed via deep learning, is then updated based on the manipulation of the latent space, as our proposed inverse projection takes both the latent space and the 2D projection space as input. Additionally, we managed to found a reasonable initial status for the latent space and thereby the inverse projection, named as `\ourname{} (fixed)' in \Cref{ch6:controllable_Pinv}.

our proposed inverse projection takes both the latent space and the 2D projection space as input. Additionally, we managed to found a reasonable initial status for the latent space and thereby the inverse projection, named as `\ourname{} (fixed)' in \Cref{ch6:controllable_Pinv}. Our evaluations showed that, without conducting the control, \ourname{} (fixed) are already comparable to existing inverse projection methods. When adding control, \ourname{} can hit higher intrinsic dimensionalities, and more importantly, can generate more diverse and meaningful inverse projections that no existing inverse projection methods can achieve. We also demonstrated the application of \ourname{} in style transfer application. \section{Fast decision map computation} In \Cref{ch7:fastDBM}, we presented FastDBM, a set of techniques for accelerating the computation of classifier maps (decision maps and other maps related to inverse projections)  thereby addressing \textbf{RQ4 (interactivity of decision maps)}. Our core idea is based on an initially sparse sampling of the image space, followed by local recursive refinement of areas marked as containing rapid changes of the depicted signal. Our evaluations showed that FastDBM can compute classifier maps that are visually almost identical to ground-truth ones with a speed-up of over one order of magnitude.

local recursive refinement of areas marked as containing rapid changes of the depicted signal. Our evaluations showed that FastDBM can compute classifier maps that are visually almost identical to ground-truth ones with a speed-up of over one order of magnitude. Importantly, our method can accelerate any current classifier map computation technique, \emph{e.g.} decision boundary maps, gradient maps, or distance to data maps; and can be applied to any trained classifier model, as it only requires access to the inverse projection function this technique uses, respectively to the black-box execution of the trained model (or property to be evaluated on that model). Importantly, FastDBM works with both discrete maps (\emph{e.g.} decision boundary maps) and continuous maps (all other aforementioned examples) in similar ways and with similar computational effort. FastDBM is, we believe, an important step towards the application of classifier maps in interactive visual analytics scenarios where users need to rapidly recompute and examine such maps. \section{Directions for future work} Based on our research, we foresee several new directions for future work.

an important step towards the application of classifier maps in interactive visual analytics scenarios where users need to rapidly recompute and examine such maps. \section{Directions for future work} Based on our research, we foresee several new directions for future work. In \Cref{ch6:controllable_Pinv}, we proposed \ourname{}, a controllable deep-learning-based inverse projection method. All the enhancements in \Cref{sec:dbm_enhancements}, such as gradient maps or distance to boundary maps, can benefit from the extra freedom that our user control offers. More interestingly, additional enhancements can be particularly designed for \ourname{} particularly, based on its dynamic control. For example, since we know the inverse projection of a pixel image is a surface-like structure, we can visualize the distance of each data sample to this surface. This enhancement would (1) provide clues to users about the selections of target points before the interaction; and (2) intuitively show how far the inversely projected `surface' moves when users conduct the interactions. Additionally, combining \ourname{} with FastDBM would significantly increase the interactivity rate which is key to \ourname{}.

the selections of target points before the interaction; and (2) intuitively show how far the inversely projected `surface' moves when users conduct the interactions. Additionally, combining \ourname{} with FastDBM would significantly increase the interactivity rate which is key to \ourname{}. In \Cref{ch6:controllable_Pinv}, we have only demonstrated one application of \ourname{} -- style transfer. We envision that \ourname{} can be applied to a wide range of applications in the context of VIS4ML, such as data augmentation, building controllable decision maps, and active learning. Future work could explore these applications in more details. One of the obvious low-hanging fruits is to apply \ourname{} to the data augmentation task. \citet{rodrigues2020VisualAnalytics} have shown that the original DBM method can be used to generate new samples for a training set and therefore improve the performance of a classifier -- an approach further verified in a different application context by \citet{benato2024HumanloopUsing}. As our work showed that all previous inverse projection methods can only cover a surface-like structure in the data space, the samples generated by these methods are limited. In contrast, we believe that \ourname{} can generate more diverse samples for the training set, and therefore improve the performance of classifier training more effectively.

a surface-like structure in the data space, the samples generated by these methods are limited. In contrast, we believe that \ourname{} can generate more diverse samples for the training set, and therefore improve the performance of classifier training more effectively. \textbf{More advanced architectures for neural network (inverse) projection methods}: So far, all the existing deep learning-based projection and inverse projection methods, including our proposed \ourname{}, use only simple fully connected neural networks. Future work could explore more advanced architectures, such as adding attention mechanisms to enhance the ability to focus on critical features for learning (inverse) projections, or leveraging diffusion models for more robust inverse projection processes. We foresee that using such advanced techniques can open new perspectives for decision maps and make them even more suitable and effective for assisting machine learning tasks.

learning (inverse) projections, or leveraging diffusion models for more robust inverse projection processes. We foresee that using such advanced techniques can open new perspectives for decision maps and make them even more suitable and effective for assisting machine learning tasks. \textbf{Handling modern AI models}: Our work showed that decision maps can be computed to bring useful insights for classification models. However, much of the current AI research moves into the direction of developing and deploying regression models such as the Large Language Models (LLMs) or image transformers well known via ChatGPT or DALL-E, to mention just a few. As such models start claiming an increasingly large share of the interest of both developers and end users of AI, a key question is whether decision maps can be adapted to convey insights in their working, much as we showed they can do for classification models. While we did not attack this question in this thesis, we believe that our contributions form a starting basis for this exploration: Our research questions \textbf{RQ1} -- \textbf{RQ4}, and our methodology to answer them, are directly applicable to the study of future decision-map-like methods designed for regressors. Additionally, the results we obtained for decision maps, most notably the limited coverage of control-free methods; the possibility for adding interactivity by capturing information lost by projections; and the possibility of accelerating map construction by hierarchical division of the image space, can be direct starting points for future researchers aiming to design regressor maps.

control-free methods; the possibility for adding interactivity by capturing information lost by projections; and the possibility of accelerating map construction by hierarchical division of the image space, can be direct starting points for future researchers aiming to design regressor maps. \markboth{\spacedlowsmallcaps{Acknowledgments}}{\spacedlowsmallcaps{Acknowledgments}} % work-around to have small caps also Yu Wang was born on May 9 1997 in Hebei, China. He received his Bachelor's degree in Product Design from the School of Gemmology at China University of Geosciences, Beijing (CUGB) in 2019. During his undergraduate studies, he also obtained the Gemmology Diploma (FGA) from the Gemmological Association of Great Britain (Gem-A). Following this, he began a six-year graduate program in Geology at CUGB. Yu's research in this direction focuses on the application of machine learning to classify mineral genetic types as well as using various explainable AI (XAI) techniques to improve the design and performance of such models.

program in Geology at CUGB. Yu's research in this direction focuses on the application of machine learning to classify mineral genetic types as well as using various explainable AI (XAI) techniques to improve the design and performance of such models. In 2022, Yu met prof. Alexandru Telea and began collaborating on research related to decision maps. In March 2023, Yu joined the Visualization and Graphics (VIG) group within the Department of Information and Computing Sciences at Utrecht University. While working with VIG, Yu expanded his research scope to include high-dimensional data visualization, inverse projection techniques, and decision maps for classifiers. Yu's research interests include interpretable machine learning, dimensionality reduction, deep-learning-based inverse projection methods, and generative AI. He is passionate about bridging algorithmic design and human understanding through visual analytics. First and foremost, I would like to express my heartfelt gratitude to my supervisor, prof. Alex Telea. Thank you for accepting me as his PhD student. Thank you for your patient guidance, strong support, and for helping shape the person I am today. You've given me a new direction in life, and I will always be grateful for that.

for accepting me as his PhD student. Thank you for your patient guidance, strong support, and for helping shape the person I am today. You've given me a new direction in life, and I will always be grateful for that. I also sincerely thank my second supervisor, dr. Michael Behrisch, for his support and for making my PhD journey smoother and more rewarding. My appreciation extends to all members of the Visualization and Graphics (VIG) group for the valuable feedback, engaging discussions, and the friendly and supportive atmosphere throughout my time here. I also want to thank my friend, Zhihai Zhu, who first guided me onto the path of programming and computer science, and who has patiently provided support along the way. To my climbing friends -- thank you for bringing fun, color, and adventure to my life in the Netherlands. Your friendship has meant so much to me. A special thanks to Lisa for designing the cover of this thesis, volunteering as the photographer for my defense, and offering many other forms of support along the way.

in the Netherlands. Your friendship has meant so much to me. A special thanks to Lisa for designing the cover of this thesis, volunteering as the photographer for my defense, and offering many other forms of support along the way. Last but certainly not least, I want to thank my family and friends for their constant love, encouragement, and understanding throughout this journey. Your support has been my foundation. This document was typeset using the typographical look-and-feel \texttt{classicthesis} developed by Andr\'e Miede. The style was inspired by Robert Bringhurst's seminal book on typography ``\emph{The Elements of Typographic Style}''. \texttt{classicthesis} is available for both \LaTeX\ and \mLyX:

